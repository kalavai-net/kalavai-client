{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kalavai","text":"<p>Kalavai is a self-hosted platform that turns everyday devices into your very own AI cluster. Do you have an old desktop or a gaming laptop gathering dust? Aggregate resources from multiple machines and say goodbye to CUDA out of memory errors. Deploy your favourite open source LLM, fine tune it with your own data, or simply run your distributed work, zero-devops. Simple. Private. Yours.</p> <p>Want to be notified of the latest features? </p> <p>Join our mailing list</p>"},{"location":"#what-problem-does-kalavai-address","title":"What problem does Kalavai address","text":"<p>Despite the hype of new models, AI remains hard to leverage in real world scenarios. We've been there, in startups, as AI developers. We have experienced how expensive the hardware is to procure; we've felt frustrated with cloud providers rationing GPUs; we've wasted countless hours in devops rabbit-holes configuring and managing multiple machines, distracted away from the actual fun of AI. All of this makes AI inaccessible for most.</p>"},{"location":"#who-is-kalavai-built-for","title":"Who is Kalavai built for","text":"<ul> <li>Research institutions such as Universities and research centres</li> <li>Enterprises looking to deploy AI in their workflows but without a big devops team</li> <li>The AI community looking to train and deploy Large Language Models -larger than any single device they own</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>Check out our quick start guide to get you going!</p>"},{"location":"bootstrap/","title":"Bootstrap a new AI cluster","text":"<p>Intro</p>","tags":["create cluster","bootstrap","master node"]},{"location":"bootstrap/#requirements","title":"Requirements","text":"<p>At least 1 computer (master node) Public IP (or all nodes within the same private network) Unique hostnames</p>","tags":["create cluster","bootstrap","master node"]},{"location":"bootstrap/#install-the-kalavai-client","title":"Install the Kalavai client","text":"<p>See how to install kalavai client</p>","tags":["create cluster","bootstrap","master node"]},{"location":"bootstrap/#bootstrap-your-master-node","title":"Bootstrap your master node","text":"<p>Once it's installed, run the CLI app with:</p> <pre><code>kalavai --help\n</code></pre> <pre><code>usage: kalavai [-h] command ...\n\npositional arguments:\n  command\n    login     Login with your Kalavai user email and password. Get an account from https://platform.kalavai.net\n    logout    Logout from your kalavai user account.\n    start     Join Kalavai cluster and start/resume sharing resources.\n    status    Check the current status of your device.\n    stop      Stop sharing your device and clean up. DO THIS ONLY IF YOU WANT TO REMOVE KALAVAI-CLIENT from your device.\n    pause     Pause sharing your device and make your device unavailable for kalavai scheduling.\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre> <p>Create cluster:</p> <pre><code>kalavai start ---\n</code></pre> <p>C&amp;P join token and use it on other nodes to join the cluster</p>","tags":["create cluster","bootstrap","master node"]},{"location":"bootstrap/#whats-next","title":"What's next","text":"<p>Install your app integrations and use your cluster!</p>","tags":["create cluster","bootstrap","master node"]},{"location":"concepts/","title":"Core concepts","text":"<p>Under construction. Come back soon!</p> <p>A platform to turn everyday devices into a powerful AI cloud</p>","tags":["concepts","architecture"]},{"location":"concepts/#how-it-works","title":"How it works?","text":"<p>To create an AI cluster, you need a seed node which acts as a control plane. It handles bookkeeping for the cluster. With a seed node, you can generate join tokens, which you can share with other machines --worker nodes.</p> <p>The more worker nodes you have in a cluster, the bigger workloads it can run. Note that the only requirement for a fully functioning cluster is a single seed node.</p> <p>Once you have a cluster running, you can easily deploy workloads using template jobs. These are community integrations that let users deploy jobs, such as LLM deployments or LLM fine tuning. A template makes using Kalavai really easy for end users, with a parameterised interface, and it also makes the platform infinitely expandable.</p>","tags":["concepts","architecture"]},{"location":"end2end/","title":"End to end walkthrough: how to boostrap a multi-node AI cluster","text":"","tags":["tutorial","end to end","VPN"]},{"location":"end2end/#setting-up-the-environment","title":"Setting up the environment","text":"","tags":["tutorial","end to end","VPN"]},{"location":"end2end/#optional-create-a-virtual-private-network-for-all-your-nodes","title":"(Optional) Create a Virtual Private Network for all your nodes","text":"","tags":["tutorial","end to end","VPN"]},{"location":"end2end/#optional-set-up-dns-redirection-on-your-domain","title":"(Optional) Set up DNS redirection on your domain","text":"","tags":["tutorial","end to end","VPN"]},{"location":"end2end/#master-node","title":"Master node","text":"","tags":["tutorial","end to end","VPN"]},{"location":"end2end/#worker-node-1","title":"Worker node 1","text":"","tags":["tutorial","end to end","VPN"]},{"location":"end2end/#worker-node-2","title":"Worker node 2","text":"","tags":["tutorial","end to end","VPN"]},{"location":"enterprise/","title":"Managed Kalavai","text":"<p>Coming soon...</p> <p>If you are interested on a fully managed, hosted seed node to make bootstrapping your AI clusters easy, please contact us:</p> <p>Contact us</p>","tags":["managed"]},{"location":"faqs/","title":"FAQs","text":"","tags":["FAQs"]},{"location":"faqs/#general","title":"General","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-clusters","title":"What are clusters?","text":"<p>In Kalavai parlor, a cluster refers to a group of resources. We go beyond machine procurement and include everything a team needs to work on AI; from the hardware devices (GPUs, CPUs and memory) to the setup of a distributed environment and the tech stack needed to make it useful.</p> <p>Kalavai aims to manage it all (procurement of additional cloud resources, installing and configuring open source and industry standard frameworks, configuration management, facilitate distributed computing) so teams can focus on AI innovation.</p>","tags":["FAQs"]},{"location":"faqs/#isnt-the-performance-of-distributed-training-much-slower","title":"Isn\u2019t the performance of distributed training much slower?","text":"<p>Distributed computing is not an option: due to the skyrocketing demand in computation from AI models, we are going to need to use multiple devices to do training and inference. </p> <p>NVIDIA cannot get devices larger fast enough, and cloud providers are busy counting the money they are going to make from all that computing to care.</p> <p>Kalavai has considerable tailwinds that will work to minimise the impact of distributed computing in the future:  - Consumer-grade GPU performance per dollar is improving at a faster rate than cloud GPUs - By 2030: Internet broadband speed will reach Gbit/s and 6G will reduce latency &lt; 1 microsecond</p>","tags":["FAQs"]},{"location":"faqs/#host-nodes","title":"Host nodes","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-the-minimum-specs-for-sharing","title":"What are the minimum specs for sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#is-my-device-safe","title":"Is my device safe?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-use-my-device-whilst-sharing","title":"Can I use my device whilst sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-limit-what-i-share-with-kalavai","title":"Can I limit what I share with Kalavai?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-run-the-kalavai-app-within-a-vm","title":"Can I run the kalavai app within a VM?","text":"","tags":["FAQs"]},{"location":"faqs/#why-does-it-require-sudo-privileges","title":"Why does it require sudo privileges?","text":"","tags":["FAQs"]},{"location":"faqs/#developers","title":"Developers","text":"","tags":["FAQs"]},{"location":"faqs/#there-are-plenty-of-mlops-platforms-out-there-why-would-organisations-turn-to-you-instead","title":"There are plenty of MLOps platforms out there, why would organisations turn to you instead?","text":"<p>MLOps solutions out there are great, and they continue to develop. But they all need hardware to run on; whether it is on premise servers, public cloud resources or managed services. </p> <p>We think of MLOps platforms as complementors, that\u2019s why we are building a marketplace for third parties to bring their solutions to our users. Since we manage the computing layer, we abstract away the complexity of integration for them, so they can also bring their tools without having to build multiple integrations.</p>","tags":["FAQs"]},{"location":"faqs/#enterprises","title":"Enterprises","text":"","tags":["FAQs"]},{"location":"faqs/#you-are-leveraging-the-organisations-existing-hardware-but-this-is-unlikely-to-meet-ai-demands-are-we-not-back-to-square-one-since-they-need-to-anyways-go-to-the-cloud","title":"You are leveraging the organisation's existing hardware, but this is unlikely to meet AI demands. Are we not back to square one since they need to anyways go to the cloud?","text":"<p>Our goal is not to narrow companies' choices but to manage the complexity of hybrid clouds. Organisations can bring hardware from anywhere (their own premises, their company devices, all the way to multi cloud on-demand resources) and Kalavai manages them equally. Developers then see a pool of resources that they treat the same.</p>","tags":["FAQs"]},{"location":"faqs/#organisations-with-on-premise-servers-already-have-systems-to-use-them-why-would-they-trust-you-to-manage-that-for-all-their-needs","title":"Organisations with on premise servers already have systems to use them. Why would they trust you to manage that for all their needs?","text":"<p>Kalavai works as an integration system, it does not force organisations to switch every workflow they have over to benefit from it. They can install the kalavai client in their existing on premise servers, which will automatically then connect them to the cluster and make them able to run workflows. The kalavai client is designed to co-exist with any application and can be limited to use only a portion of resources, so organisations can easily continue to use their on premise deployments.</p>","tags":["FAQs"]},{"location":"faqs/#ive-heard-of-a-bunch-of-service-providers-for-rent-a-gpu-on-demand-isnt-the-market-saturated-already","title":"I\u2019ve heard of a bunch of service providers for rent-a-GPU on demand. Isn\u2019t the market saturated already?","text":"<p>Kalavai does not have any hardware to lease. We believe there are enough providers out there to cover that. Where there\u2019s a gap is in managing the complexity of use cases that require distributed computing. When workflows require more than one computing device to run, organisations need to manage the orchestration, maintenance and coordination of devices.</p> <p>We have designed Kalavai to integrate nicely with almost any computing resource out there, from public cloud, serverless GPU providers and on premise devices.</p> <p>Got another question?</p>","tags":["FAQs"]},{"location":"getting_started/","title":"Quick start","text":"<p>Kalavai is free to use, no caps, for both commercial and non-commercial purposes. All you need to get started is one or more computers that can see each other (i.e. within the same network), and you are good to go. If you wish to join computers in different locations / networks, check our managed kalavai offering.</p>","tags":["kalavai-client","cli","install","requirements","quick start"]},{"location":"getting_started/#create-a-seed-node","title":"Create a seed node","text":"<p>Simply use the CLI to start your seed node:</p> <pre><code>kalavai start\n</code></pre> <p>Note that it will take a few minutes to setup and download all dependencies. Check the status of your cluster with:</p> <pre><code>kalavai diagnostics\n</code></pre>","tags":["kalavai-client","cli","install","requirements","quick start"]},{"location":"getting_started/#add-a-worker-node","title":"Add a worker node","text":"<p>In the seed node, generate a join token:</p> <pre><code>kalavai token\n</code></pre> <p>Copy the displayed token. On the worker node, run:</p> <pre><code>kalavai join &lt;token&gt;\n</code></pre> <p>After some time, you should be able to see the new node:</p> <pre><code>kalavai nodes\n</code></pre> <p>You can also see the total resources available:</p> <pre><code>kalavai resources\n</code></pre>","tags":["kalavai-client","cli","install","requirements","quick start"]},{"location":"getting_started/#enough-already-lets-run-stuff","title":"Enough already, let's run stuff!","text":"<p>In short, run a template job:</p> <pre><code>kalavai run &lt;template name&gt; --values-path &lt;values file&gt;\n</code></pre> <p>Each job requires two values: - Name of the template --get a list of available integrations with <code>kalavai templates</code> - Parameter values for the template.</p> <p>Here we will use the example of deploying a LLM (vllm template). To generate default values file:</p> <pre><code>kalavai job defaults vllm &gt; values.yaml\n</code></pre> <p>This will create a <code>values.yaml</code> file that contains the default values for a vllm job, such as the model id, the number of workers, etc.</p> <p>Then you can use the newly created values to run the job:</p> <pre><code>kalavai job run vllm --values-path values.yaml\n</code></pre> <p>In this case, the job also deploys a service that can be accessible via an endpoint. Find out the url with:</p> <pre><code>kalavai job list \n</code></pre> <p>Job monitoring and lifecycle:</p> <pre><code># provide the logs of a specific job\nkalavai job logs &lt;name of the job&gt; \n# delete a job\nkalavai job delete &lt;name of the job&gt;\n</code></pre>","tags":["kalavai-client","cli","install","requirements","quick start"]},{"location":"install_apps/","title":"Develop with Kalavai","text":"<p>[Coming soon...]</p>","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"install_apps/#install-applications","title":"Install applications","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"install_apps/#ray","title":"Ray","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"install_apps/#dask","title":"Dask","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"install_apps/#vcluster","title":"vCluster","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"install_apps/#why-is-insert-preferred-application-not-supported","title":"Why is [insert preferred application] not supported?","text":"<p>If your preferred distributed ML application is not yet supported, let us know!</p> <p>Contact us</p>","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"join/","title":"Join an existing AI cluster","text":"","tags":["join","worker node"]},{"location":"join/#requirements","title":"Requirements","text":"<ul> <li>Worker nodes should be able to connect directly to the master node (either via public IP or within the same private network)</li> </ul>","tags":["join","worker node"]},{"location":"join/#install-kalavai-client-app","title":"Install Kalavai client app","text":"<p>See how to install kalavai client app</p>","tags":["join","worker node"]},{"location":"join/#join-the-cluster","title":"Join the cluster","text":"<p>Fetch the joining token from the master node:</p> <pre><code>kalavai generate-token\n</code></pre> <p>And then join! Create cluster:</p> <pre><code>kalavai join ---\n</code></pre>","tags":["join","worker node"]},{"location":"join/#whats-next","title":"What's next?","text":"<p>Your cluster will now be able to schedule workloads on the new node. Check out the app integrations for more info.</p>","tags":["join","worker node"]}]}