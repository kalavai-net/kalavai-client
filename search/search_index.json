{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kalavai","text":"<p>Kalavai is an open source platform that turns everyday devices into your very own AI supercomputer. We help you aggregate resources from multiple machines: home desktops, gaming laptops, work computers, cloud VMs... When you need to go beyond, Kalavai facilitates matchmaking of resources so anyone in our community can tap into a larger pool of devices by inspiring others to join your cause.</p>"},{"location":"#what-can-you-do-with-kalavai","title":"What can you do with Kalavai","text":"<p>Kalavai helps manage the complexity of distributed computing. With Kalavai you can easily: - Deploy Large Language Models in a single machine, or seamlessly across multiple nodes - Run a Ray cluster for your AI training, fine tuning and serving needs</p>"},{"location":"#want-to-be-notified-of-the-latest-features","title":"Want to be notified of the latest features?","text":"<p>We regularly publish news, articles and updates on our Substack channel.</p> <p>Subscribe to our substack</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Check out our start guide for a step-by-step guide on how to get started. This is the recommended entry point for those that wish to explore the basics of Kalavai.</p> <p>For those that are more shoot first, ask later, we've prepared a quick start tutorial that goes straight to the point, from downloading Kalavai to running your own LLM across community machines.</p>"},{"location":"boinc/","title":"BOINC: Volunteer scientific computing","text":"<p>BOINC is an open source platform for volunteer computing, organised in scientific projects.</p> <p>Kalavai makes it easy to share your computing resources with Science United, a coordinated model for scientific computing where volunteers share their machines with a multitude of projects.</p> <p>What you get by sharing:</p> <ul> <li>Eternal kudos from the community!</li> <li>Kalavai credits that can be used in any of our public pools</li> </ul>","tags":["boinc","volunteer computing"]},{"location":"boinc/#requirements","title":"Requirements","text":"<ul> <li>A free Kalavai account. Create one here.</li> <li>A computer with the minimum requirements (see below)</li> </ul> <p>Hardware requirements</p> <ul> <li>4+ CPUs</li> <li>4GB+ RAM</li> <li>(optional) 1+ NVIDIA GPU</li> </ul>","tags":["boinc","volunteer computing"]},{"location":"boinc/#how-to-join","title":"How to join","text":"<ol> <li> <p>Create a free account with Kalavai.</p> </li> <li> <p>Install the kalavai client following the instructions here. Currently we support Linux distros and Windows.</p> </li> <li> <p>Get the joining token. Visit our platform and go to <code>Community pools</code>. Then click <code>Join</code> on the <code>BOINC</code> Pool to reveal the joining details. Copy the command (including the token).</p> </li> </ol> <p></p> <ol> <li>Authenticate the computer you want to use as worker:</li> </ol> <pre><code>$ kalavai login\n\n[10:33:16] Kalavai account details. If you don't have an account, create one at https://platform.kalavai.net                                                                 \nUser email: &lt;your email&gt;\nPassword: &lt;your password&gt;\n\n[10:33:25] &lt;email&gt; logged in successfully\n</code></pre> <ol> <li>Join the pool with the following command:</li> </ol> <pre><code>$ kalavai pool join &lt;token&gt;\n\n[16:28:14] Token format is correct\n           Joining private network\n\n[16:28:24] Scanning for valid IPs...\n           Using 100.10.0.8 address for worker\n           Connecting to BOINC @ 100.10.0.9 (this may take a few minutes)...\n[16:29:41] Worskpace created\n           You are connected to BOINC\n</code></pre> <p>That's it, your machine is now contributing to scientific discovery!</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#stop-sharing","title":"Stop sharing","text":"<p>You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). </p> <p>To pause sharing (but remain on the pool), run the following command:</p> <pre><code>kalavai pool pause\n</code></pre> <p>When you are ready to resume sharing, run:</p> <pre><code>kalavai pool resume\n</code></pre> <p>To stop and leave the pool, run the following:</p> <pre><code>kalavai pool stop\n</code></pre>","tags":["boinc","volunteer computing"]},{"location":"boinc/#faqs","title":"FAQs","text":"","tags":["boinc","volunteer computing"]},{"location":"boinc/#something-isnt-right","title":"Something isn't right","text":"<p>Growing pains! Please report any issues in our github repository.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#can-i-join-and-leave-whenever-i-want","title":"Can I join (and leave) whenever I want?","text":"<p>Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#what-is-in-it-for-me","title":"What is in it for me?","text":"<p>If you decide to share your compute with BOINC, you will gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon).</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#is-my-gpu-constantly-being-used","title":"Is my GPU constantly being used?","text":"<p>No. BOINC projects upload tasks to be completed to a queue, which volunteers computers poll for work. If there is no suitable work to be done by the worker, the machine will remain idle and no resources are spent.</p> <p>If at any point you need your machine back, pause or stop sharing and come back when you are free.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#how-do-i-check-how-much-have-i-contributed","title":"How do I check how much have I contributed?","text":"<p>Kalavai pools all the machines together and contributes to BOINC as a single entity. You can check how much the pool has shared overtime through the Science United leaderboard page -look out for <code>Kalavai.net</code> entry.</p> <p></p> <p>Individual users can also check how much compute have they contributed via their home page in our platform. Once you are logged in, click on the button displaying your user name on the left panel. This view will show how much of each key resource you have contributed thus far (CPUs, RAM, GPU).</p> <p></p>","tags":["boinc","volunteer computing"]},{"location":"concepts/","title":"Concepts","text":"<p>Work in progress</p>","tags":["concepts","architecture","pool"]},{"location":"concepts/#core-concepts","title":"Core concepts","text":"<p>Under construction. Come back soon!</p> <p>A platform to turn everyday devices into a powerful AI cloud</p>","tags":["concepts","architecture","pool"]},{"location":"concepts/#how-it-works","title":"How it works?","text":"<p>To create an AI pool, you need a seed node which acts as a control plane. It handles bookkeeping for the pool. With a seed node, you can generate join tokens, which you can share with other machines --worker nodes.</p> <p>The more worker nodes you have in a pool, the bigger workloads it can run. Note that the only requirement for a fully functioning pool is a single seed node.</p> <p>Once you have a pool running, you can easily deploy workloads using template jobs. These are community integrations that let users deploy jobs, such as LLM deployments or LLM fine tuning. A template makes using Kalavai really easy for end users, with a parameterised interface, and it also makes the platform infinitely expandable.</p>","tags":["concepts","architecture","pool"]},{"location":"faqs/","title":"FAQs","text":"<p>Work in progress</p>","tags":["FAQs"]},{"location":"faqs/#general","title":"General","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-ai-pools","title":"What are AI pools?","text":"<p>In Kalavai parlor, a pool refers to a group of resources. We go beyond machine procurement and include everything a team needs to work on AI; from the hardware devices (GPUs, CPUs and memory) to the setup of a distributed environment and the tech stack needed to make it useful.</p> <p>Kalavai aims to manage it all (procurement of additional cloud resources, installing and configuring open source and industry standard frameworks, configuration management, facilitate distributed computing) so teams can focus on AI innovation.</p>","tags":["FAQs"]},{"location":"faqs/#isnt-the-performance-of-distributed-training-much-slower","title":"Isn\u2019t the performance of distributed training much slower?","text":"<p>Distributed computing is not an option: due to the skyrocketing demand in computation from AI models, we are going to need to use multiple devices to do training and inference. </p> <p>NVIDIA cannot get devices larger fast enough, and cloud providers are busy counting the money they are going to make from all that computing to care.</p> <p>Kalavai has considerable tailwinds that will work to minimise the impact of distributed computing in the future:  - Consumer-grade GPU performance per dollar is improving at a faster rate than cloud GPUs - By 2030: Internet broadband speed will reach Gbit/s and 6G will reduce latency &lt; 1 microsecond</p>","tags":["FAQs"]},{"location":"faqs/#host-nodes","title":"Host nodes","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-the-minimum-specs-for-sharing","title":"What are the minimum specs for sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#is-my-device-safe","title":"Is my device safe?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-use-my-device-whilst-sharing","title":"Can I use my device whilst sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-limit-what-i-share-with-kalavai","title":"Can I limit what I share with Kalavai?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-run-the-kalavai-app-within-a-vm","title":"Can I run the kalavai app within a VM?","text":"","tags":["FAQs"]},{"location":"faqs/#why-does-it-require-sudo-privileges","title":"Why does it require sudo privileges?","text":"","tags":["FAQs"]},{"location":"faqs/#developers","title":"Developers","text":"","tags":["FAQs"]},{"location":"faqs/#there-are-plenty-of-mlops-platforms-out-there-why-would-organisations-turn-to-you-instead","title":"There are plenty of MLOps platforms out there, why would organisations turn to you instead?","text":"<p>MLOps solutions out there are great, and they continue to develop. But they all need hardware to run on; whether it is on premise servers, public cloud resources or managed services. </p> <p>We think of MLOps platforms as complementors, that\u2019s why we are building a marketplace for third parties to bring their solutions to our users. Since we manage the computing layer, we abstract away the complexity of integration for them, so they can also bring their tools without having to build multiple integrations.</p>","tags":["FAQs"]},{"location":"faqs/#enterprises","title":"Enterprises","text":"","tags":["FAQs"]},{"location":"faqs/#you-are-leveraging-the-organisations-existing-hardware-but-this-is-unlikely-to-meet-ai-demands-are-we-not-back-to-square-one-since-they-need-to-anyways-go-to-the-cloud","title":"You are leveraging the organisation's existing hardware, but this is unlikely to meet AI demands. Are we not back to square one since they need to anyways go to the cloud?","text":"<p>Our goal is not to narrow companies' choices but to manage the complexity of hybrid clouds. Organisations can bring hardware from anywhere (their own premises, their company devices, all the way to multi cloud on-demand resources) and Kalavai manages them equally. Developers then see a pool of resources that they treat the same.</p>","tags":["FAQs"]},{"location":"faqs/#organisations-with-on-premise-servers-already-have-systems-to-use-them-why-would-they-trust-you-to-manage-that-for-all-their-needs","title":"Organisations with on premise servers already have systems to use them. Why would they trust you to manage that for all their needs?","text":"<p>Kalavai works as an integration system, it does not force organisations to switch every workflow they have over to benefit from it. They can install the kalavai client in their existing on premise servers, which will automatically then connect them to the pool and make them able to run workflows. The kalavai client is designed to co-exist with any application and can be limited to use only a portion of resources, so organisations can easily continue to use their on premise deployments.</p>","tags":["FAQs"]},{"location":"faqs/#ive-heard-of-a-bunch-of-service-providers-for-rent-a-gpu-on-demand-isnt-the-market-saturated-already","title":"I\u2019ve heard of a bunch of service providers for rent-a-GPU on demand. Isn\u2019t the market saturated already?","text":"<p>Kalavai does not have any hardware to lease. We believe there are enough providers out there to cover that. Where there\u2019s a gap is in managing the complexity of use cases that require distributed computing. When workflows require more than one computing device to run, organisations need to manage the orchestration, maintenance and coordination of devices.</p> <p>We have designed Kalavai to integrate nicely with almost any computing resource out there, from public cloud, serverless GPU providers and on premise devices.</p> <p>Got another question?</p>","tags":["FAQs"]},{"location":"getting_started/","title":"Getting started","text":"<p>Kalavai is free to use, no caps, for both commercial and non-commercial purposes. All you need to get started is one or more computers that can see each other (i.e. within the same network), and you are good to go. If you wish to join computers in different locations / networks, check our managed kalavai offering.</p> <p>The <code>kalavai</code> CLI is the main tool to interact with the Kalavai platform, to create and manage both local and public pools. Let's go over its installation</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#requirements","title":"Requirements","text":"<ul> <li>A laptop, desktop or Virtual Machine</li> <li>Admin / privileged access (eg. <code>sudo</code> access in linux or Administrator in Windows)</li> <li>Running Windows or Linux (see more details in our compatibility matrix)</li> </ul>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#linux","title":"Linux","text":"<p>Run the following command on your terminal:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kalavai-net/kalavai-client/main/assets/install_client.sh | bash -\n</code></pre>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#windows","title":"Windows","text":"<p>For Windows machines complete WSL configuration first before continuing. You must be running Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11 to use the commands below. If you are on earlier versions please see the manual install page.</p> <ol> <li> <p>Open a PowerShell with administrative permissions (Run as Administrator)</p> </li> <li> <p>Install WSL2:</p> </li> </ol> <pre><code>wsl --install -d Ubuntu-24.04\n</code></pre> <ol> <li>Make sure to enable <code>systemd</code> by editing (or creating if required) a file <code>/etc/wsl.conf</code></li> </ol> <pre><code>[boot]\nsystemd=true\n</code></pre> <ol> <li>Restart the WSL instance by exiting and logging back in:</li> </ol> <pre><code>exit\nwsl --shutdown\nwsl -d Ubuntu-24.04\n</code></pre> <ol> <li>Inside WSL, install Kalavai:</li> </ol> <pre><code>curl -sfL https://raw.githubusercontent.com/kalavai-net/kalavai-client/main/assets/install_client.sh | bash -\n</code></pre> <p>Note: you must keep the WSL console window open to continue to share resources with an AI pool. If you restart your machine or close the console, you will need to resume kalavai as follows:</p> <pre><code>kalavai pool resume\n</code></pre> <p>Known issue: if the above resume command hangs or fails, try to run the pause command before and then reattempt resuming:</p> <pre><code>kalavai pool pause\nkalavai pool resume\n</code></pre>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#whats-next","title":"What's next","text":"<p>Now that you have your client up and running, you are ready to create and join computing pools. The easiest way to get started is by joining a public pool, so we'll look at that next. </p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"local_pool/","title":"Local pools","text":"","tags":["create local pool","bootstrap","seed node"]},{"location":"local_pool/#createa-a-local-pool","title":"Createa a local pool","text":"<p>Kalavai is free to use, no caps, for both commercial and non-commercial purposes. All you need to get started is one or more computers that can see each other (i.e. within the same network), and you are good to go. If you wish to join computers in different locations / networks, check managed kalavai.</p>","tags":["create local pool","bootstrap","seed node"]},{"location":"local_pool/#1-start-a-seed-node","title":"1. Start a seed node","text":"<p>Simply use the CLI to start your seed node:</p> <pre><code>kalavai pool start &lt;pool-name&gt;\n</code></pre> <p>Now you are ready to add worker nodes to this seed. To do so, generate a joining token:</p> <pre><code>$ kalavai pool token\n\nJoin token: &lt;token&gt;\n</code></pre>","tags":["create local pool","bootstrap","seed node"]},{"location":"local_pool/#2-add-worker-nodes","title":"2. Add worker nodes","text":"<p>Increase the power of your AI pool by inviting others to join.</p> <p>Copy the joining token. On the worker node, run:</p> <pre><code>kalavai pool join &lt;token&gt;\n</code></pre>","tags":["create local pool","bootstrap","seed node"]},{"location":"petals/","title":"Public Petals Swarm: BitTorrent-style LLMs","text":"<p>Contribute to the public Petals swarm and help deploy and fine tune Large Language Models across consumer-grade devices. See more about the Petals project here. You'll get:</p> <ul> <li>Eternal kudos from the community!</li> <li>Access to all the models in the server</li> <li>Easy access for inference (via Petals SDK and installation-free Kalavai endpoint).</li> </ul>","tags":["petals","share"]},{"location":"petals/#requirements","title":"Requirements","text":"<ul> <li>A free Kalavai account. Create one here.</li> <li>A computer with the minimum requirements (see below)</li> </ul> <p>Hardware requirements</p> <ul> <li>1+ NVIDIA GPU</li> <li>2+ CPUs</li> <li>4GB+ RAM</li> <li>Free space 4x available VRAM (for an 8GB VRAM GPU, you'll need ~32GB free space in your disk)</li> </ul>","tags":["petals","share"]},{"location":"petals/#how-to-join","title":"How to join","text":"<ol> <li> <p>Create a free account with Kalavai.</p> </li> <li> <p>Install the kalavai client following the instructions here. Currently we support Linux distros and Windows.</p> </li> <li> <p>Get the joining token. Visit our platform and go to <code>Community pools</code>. Then click <code>Join</code> on the <code>Petals</code> Pool to reveal the joining details. Copy the command (including the token).</p> </li> </ol> <p></p> <ol> <li>Authenticate the computer you want to use as worker:</li> </ol> <pre><code>$ kalavai login\n\n[10:33:16] Kalavai account details. If you don't have an account, create one at https://platform.kalavai.net                                                                 \nUser email: &lt;your email&gt;\nPassword: &lt;your password&gt;\n\n[10:33:25] &lt;email&gt; logged in successfully\n</code></pre> <ol> <li>Join the pool with the following command:</li> </ol> <pre><code>$ kalavai pool join &lt;token&gt;\n\n[16:28:14] Token format is correct\n           Joining private network\n\n[16:28:24] Scanning for valid IPs...\n           Using 100.10.0.8 address for worker\n           Connecting to PETALS @ 100.10.0.9 (this may take a few minutes)...\n[16:29:41] Worskpace created\n           You are connected to PETALS\n</code></pre>","tags":["petals","share"]},{"location":"petals/#check-petals-health","title":"Check Petals health","text":"<p>Kalavai's pool connects directly to the public swarm on Petals, which means we can use their public health check UI to see how much we are contributing and what models are ready to use.</p> <p></p> <p>Models with at least one copy of each shard (a green dot in each column) are ready to be used. If not, wait for more workers to join in.</p> <p>Using the kalavai client you can monitor the state of the pool and all of the connected nodes:</p> <pre><code>$ kalavai pool status\n\n# Displays the status of the pool\n\n$ kalavai node list\n\n# Displays the list of connected nodes, and their current status\n</code></pre> <p>The command <code>kalavai node list</code> is useful to see if our node has any issues and whether it's currently online.</p>","tags":["petals","share"]},{"location":"petals/#how-to-use-the-models","title":"How to use the models","text":"<p>For all public swarms you can use the Petals SDK in the usual way. Here is an example:</p> <pre><code>from transformers import AutoTokenizer\nfrom petals import AutoDistributedModelForCausalLM\n\n# Choose any model available at https://health.petals.dev\nmodel_name = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n\n# Connect to a distributed network hosting model layers\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n\n# Run the model as if it were on your computer\ninputs = tokenizer(\"A cat sat\", return_tensors=\"pt\")[\"input_ids\"]\noutputs = model.generate(inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))  # A cat sat on a mat...\n</code></pre> <p>This path is great if you are a dev with python installed, and don't mind installing the Petals SDK. If you want an install-free path, Kalavai deploys a single endpoint for models, which allows you to do inference via gRPC and HTTP requests. Substitute KALAVAI_ENDPOINT with the endpoint displayed under the <code>Community Pools</code> page. Here is a request example:</p> <pre><code>\"\"\"\nMore info: https://github.com/petals-infra/chat.petals.dev\n\nRequired: pip install websockets\n\"\"\"\nimport time\nimport json\nimport websockets\nimport asyncio\n\n\nKALAVAI_ENDPOINT = \"192.168.68.67:31220\" # &lt;-- change for the kalavai endpoint\nMODEL_NAME = \"mistralai/Mixtral-8x22B-Instruct-v0.1\" # &lt;-- change for the models available in Kalavai PETALS pool.\n\n\nasync def ws_generate(text, max_length=100, temperature=0.1):\n    async with websockets.connect(f\"ws://{KALAVAI_ENDPOINT}/api/v2/generate\") as websocket:\n        try:\n            await websocket.send(\n                json.dumps({\"model\": MODEL_NAME, \"type\": \"open_inference_session\", \"max_length\": max_length})\n            )\n            response = await websocket.recv()\n            result = json.loads(response)\n\n            if result[\"ok\"]:\n                await websocket.send(\n                    json.dumps({\n                        \"type\": \"generate\",\n                        \"model\": MODEL_NAME,\n                        \"inputs\": text,\n                        \"max_length\": max_length,\n                        \"temperature\": temperature\n                    })\n                )\n                response = await websocket.recv()\n                return json.loads(response)\n            else:\n                return response\n        except Exception as e:\n            return {\"error\": str(e)}\n\n\nif __name__ == \"__main__\":\n    t = time.time()\n    output = asyncio.get_event_loop().run_until_complete(\n        ws_generate(text=\"Tell me a story: \")\n    )\n    final_time = time.time() - t\n    print(f\"[{final_time:.2f} secs]\", output)\n    print(f\"{output['token_count'] / final_time:.2f}\", \"tokens/s\")\n</code></pre> <p>NOTE: the endpoints are only available within worker nodes, not from any other computer.</p>","tags":["petals","share"]},{"location":"petals/#stop-sharing","title":"Stop sharing","text":"<p>You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). </p> <p>To pause sharing (but remain on the pool), run the following command:</p> <pre><code>kalavai pool pause\n</code></pre> <p>When you are ready to resume sharing, run:</p> <pre><code>kalavai pool resume\n</code></pre> <p>To stop and leave the pool, run the following:</p> <pre><code>kalavai pool stop\n</code></pre>","tags":["petals","share"]},{"location":"petals/#faqs","title":"FAQs","text":"","tags":["petals","share"]},{"location":"petals/#something-isnt-right","title":"Something isn't right","text":"<p>Growing pains! Please report any issues in our github repository.</p>","tags":["petals","share"]},{"location":"petals/#can-i-join-and-leave-whenever-i-want","title":"Can I join (and leave) whenever I want?","text":"<p>Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here.</p>","tags":["petals","share"]},{"location":"petals/#what-is-in-it-for-me","title":"What is in it for me?","text":"<p>If you decide to share your compute with the community, not only you'll get access to all the models we deploy in it, but you will also gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon).</p>","tags":["petals","share"]},{"location":"petals/#is-my-data-secured-private","title":"Is my data secured / private?","text":"<p>The public pool in Kalavai has the same level of privacy and security than the general Petals public swarm. See their privacy details here. In the future we will improve support for private swarms; at the moment private swarms are a beta feature for all kalavai pools that can be used via the petals template.</p>","tags":["petals","share"]},{"location":"petals/#is-my-gpu-constantly-being-used","title":"Is my GPU constantly being used?","text":"<p>Yes and no. The model weights for the shard you are responsible for are loaded in GPU memory for as long as your machine is sharing. However, this does not mean the GPU is active (doing computing) constantly; computation (and hence the vast majority of energy comsumption) only happens when your shard is summoned to process inference requests.</p> <p>If at any point you need your GPU memory back, pause or stop sharing and come back when you are free.</p>","tags":["petals","share"]},{"location":"public_pool/","title":"Public pools: crowdsource community resources","text":"<p>Our public platform expands local pools in two key aspects: - Worker nodes no longer have to be in the same local network - Users can tap into community resources: inspire others in the community to join their projects with their resources</p> <p>To get started, you need is a free account on our platform.</p>","tags":["crowdsource","public pool"]},{"location":"public_pool/#a-tap-into-community-resources","title":"A) Tap into community resources","text":"<p>Create a new pool, using a public location provided by Kalavai:</p> <pre><code># Authenticate with your kalavai account\nkalavai login\n\n# Get available public locations\nkalavai location list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \n\u2503 VPN \u2503 location    \u2503 subnet        \u2503          \n\u2521\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0   \u2502 uk_london_1 \u2502 100.10.0.0/16 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Create and publish your pool\nkalavai pool start &lt;pool-name&gt; --location uk_london_1\n</code></pre> <p>If all goes well, your pool will be created and published on the <code>Public Seeds</code> section of our platform</p> <p></p> <p>Note: to be able to publish pools your account needs to have sufficient karma points. Earn karma by sharing your resources with others.</p>","tags":["crowdsource","public pool"]},{"location":"public_pool/#b-share-resources-with-inspiring-community-projects","title":"B) Share resources with inspiring community projects","text":"<p>Have idle computing resources? Wish to be part of exciting public projects? Want to give back to the community? Earn social credit (both literally and metaphorically) by sharing your computer with others within the community.</p> <p>All you need is a public joining key. Get them in our platform, on the list of published pools. Press <code>Join</code> and follow the instructions</p> <p></p>","tags":["crowdsource","public pool"]},{"location":"quickstart/","title":"Quickstart","text":"<p>For this guide we assume you have either a compatible linux machine or WSL running on windows. For more info, check our getting started guide.</p> <p>Goal: to get you up and running as fast and painlessly as possible, from zero to distributed LLM deployment across community devices.</p>","tags":["quickstart"]},{"location":"quickstart/#1-download-kalavai","title":"1. Download Kalavai","text":"<p>Download and install the latest version of <code>kalavai</code>:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kalavai-net/kalavai-client/main/assets/install_client.sh | bash -\n</code></pre>","tags":["quickstart"]},{"location":"quickstart/#2-create-a-free-account","title":"2. Create a free account","text":"<p>Go to our platform and register to get an account. You'll need the credentials later.</p>","tags":["quickstart"]},{"location":"quickstart/#3-join-a-public-computing-pool","title":"3. Join a public computing pool","text":"<p>Computing pools are the heart and soul of Kalavai. It's a shared space where developers, researchers and enthusiasts join in with their computing power for the benefit of the community, so each user can go beyond the hardware they own.</p> <p>In the platform, go to <code>Computing Pools</code> and click <code>JOIN</code> on any of the pools shown. This will display the details on how to join using the <code>kalavai</code> client.</p> <p></p> <p>Paste the command and run it in your computer. After a few minutes you should be connected and ready to deploy!</p> <pre><code>$ kalavai pool join &lt;join token&gt;\n\n[01:24:45] Token format is correct                                                                                                                               \n[sudo] password for carlosfm: \n[01:24:48] Joining private network                                                                                                                               \n[KalavaiAuthClient]Logged in as carlosfm\n[01:25:01] Scanning for valid IPs...  \n           Using 100.10.0.7 address for worker    \n            Connecting to publicllm @ 100.10.0.2 (this may take a few minutes)...\n            You are connected to publicllm\n</code></pre>","tags":["quickstart"]},{"location":"quickstart/#4-deploy-an-llm","title":"4. Deploy an LLM","text":"<p>We are now ready to deploy an LLM across the available resources. First, let's check what resources are available:</p> <pre><code>$ kalavai pool resources\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503           \u2503 n_nodes \u2503 cpu   \u2503 memory      \u2503 nvidia.com/gpu \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Available \u2502 4       \u2502 38.08 \u2502 70096719872 \u2502 3              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Total     \u2502 4       \u2502 42    \u2502 70895734784 \u2502 3              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n$ kalavai pool gpus\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Node               \u2503 Ready \u2503 GPU(s)                                               \u2503 Available \u2503 Total \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 carlosfm-desktop-1 \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 2070 (8 GBs)               \u2502 1         \u2502 1     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 carlosfm-desktop-2 \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 3060 (12 GBs)              \u2502 1         \u2502 1     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 pop-os             \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4 GBs) \u2502 1         \u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>OK, let's deploy! We are going to deploy Qwen2.5-1.5B-Instruct model across 2 machines. We have already prepared a config file for it here. </p> <p>With <code>kalavai</code>, you can deploy most LLMs using vLLM or Aphrodite-Engine, with one command, passing all the parameter details within the config file. The <code>run</code> command asks you whether you want to target or avoid certain GPU models; for now, we'll just select <code>0</code>, which will use any GPU available.</p> <pre><code>$ kalavai job run aphrodite --values qwen2.5-1.5B.yaml\n\n[01:42:07] SELECT Target GPUs for the job          \n[KalavaiAuthClient]Logged in as carlosfm\n\n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\n[01:42:40] AVOID Target GPUs for the job\n\n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\n[01:43:13] Template /home/carlosfm/.cache/kalavai/templates/aphrodite/template.yaml successfully deployed!\n[01:43:15] Service deployed   \n</code></pre>","tags":["quickstart"]},{"location":"quickstart/#5-calling-the-model","title":"5. Calling the model","text":"<p>Deploying a model could take several minutes, since we are provisioning the machines, downloading the model and loading it in memory. To check the progress, run the following:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Deployment \u2503 Status                         \u2503 Workers    \u2503 Endpoint                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 qwen-1     \u2502 [2024-11-27T02:17:35Z] Pending \u2502 Pending: 1 \u2502 http://100.10.0.2:30271 \u2502\n\u2502            \u2502                                \u2502 Ready: 1   \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[01:48:23] Check detailed status with kalavai job status &lt;name of deployment&gt; \n           Get logs with kalavai job logs &lt;name of deployment&gt; (note it only works when the deployment is complete) \n</code></pre> <p>Once all workers are in the <code>Running</code> state, check the model is ready for inference:</p> <pre><code>$ kalavai job logs qwen-1 qwen-1-ps-0\n\n\n</code></pre> <p>There are various ways to do so, but for this tutorial we'll use the KoboldAI launched with Aphrodite-Engine. You can see the endpoint by running <code>kalavai job list</code>. In our example: http://100.10.0.2:32136</p>","tags":["quickstart"]},{"location":"ray/","title":"Ray","text":"<p>Work in progress</p>","tags":["ray"]},{"location":"ray/#ray-clusters-for-distributed-computing","title":"Ray clusters for distributed computing","text":"<p>From Ray's documentation:</p> <p>Ray is an open-source unified framework for scaling AI and Python applications like machine learning.</p> <p>Kalavai and Ray work perfectly together. Ray is a great framework to deal with distributed computation on top of an existing hardware pool. Kalavai acts as a unifying layer that brings that required hardware together for Ray to do its magic.</p> <p>To get started, check out our example to get a Ray cluster going. </p>","tags":["ray"]},{"location":"ray/#create-a-cluster","title":"Create a cluster","text":"<ul> <li>Specs how to define specs: kalavai pool resources (cpu, memory and nvidia.com/gpu)</li> </ul> <pre><code>$ kalavai pool resources\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \n\u2503           \u2503 n_nodes \u2503 cpu                \u2503 memory      \u2503 nvidia.com/gpu \u2503 \n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \n\u2502 Available \u2502 2       \u2502 10.684999999999999 \u2502 16537780224 \u2502 1              \u2502 \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \n\u2502 Total     \u2502 4       \u2502 42                 \u2502 70895030272 \u2502 3              \u2502 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n</code></pre> <pre><code>spec:\n  ...\n  headGroupSpec:\n    ...\n    template:\n      spec:\n        ...\n        containers:\n        ...\n          resources:\n            limits:\n              cpu: 2\n              memory: 4Gi\n            requests:\n              cpu: 2\n              memory: 4Gi\n  workerGroupSpecs:\n  ...\n    template:\n      spec:\n        containers:\n        ...\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n              cpu: 2\n              memory: 4Gi\n            requests:\n              nvidia.com/gpu: 1\n              cpu: 2\n              memory: 4Gi\n</code></pre> <p>Interact with Ray - Interactive mode - Endpoint - RayJobs</p>","tags":["ray"]},{"location":"ray/#advanced-topics","title":"Advanced topics","text":"<p>Autoscaling</p> <p>Node hardware requirements (limits vs requests)</p>","tags":["ray"]},{"location":"templates/","title":"Develop with Kalavai","text":"<p>Work in progress</p>","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"templates/#install-applications","title":"Install applications","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"templates/#ray","title":"Ray","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"templates/#dask","title":"Dask","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"templates/#vcluster","title":"vCluster","text":"","tags":["integrations","install apps","ray","dask","vcluster"]},{"location":"templates/#why-is-insert-preferred-application-not-supported","title":"Why is [insert preferred application] not supported?","text":"<p>If your preferred distributed ML application is not yet supported, let us know!</p> <p>Contact us</p>","tags":["integrations","install apps","ray","dask","vcluster"]}]}