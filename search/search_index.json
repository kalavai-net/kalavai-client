{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kalavai is an open source platform that unlocks computing from spare capacity . It aggregates resources from multiple sources to increase your computing budget and run large AI workloads. Core features Kalavai helps teams use GPU resources more efficiently. It acts as a control plane for all your GPUs , wherever they are: local, on prem and multi-cloud. Increase GPU utilisation from your devices (fractional GPU). Multi-node, multi-GPU and multi-architecture support (AMD and NVIDIA). Aggregate computing resources from multiple sources : home desktops, on premise servers<0>, multi cloud VMs, raspberry pi's, etc. Including our own GPU fleets . Ready-made templates to deploy common AI building blocks : model inference (vLLM, llama.cpp, SGLang), GPU clusters (Ray, GPUStack), automation workflows (n8n and Flowise), evaluation and monitoring tools (Langfuse), production dev tools (LiteLLM, OpenWebUI) and more. Easy to expand to custom workloads Getting started The best way to getting started is to check out our start guide for a step-by-step guide on how to get started. This is the recommended entry point for those that wish to explore the basics of Kalavai. Core components Kalavai turns devices into a scalable LLM platform . It connects multiple machines together and manages the distribution AI workloads on them. There are three core components: Kalavai client : python CLI program that lets users create and interact with GPU pools distributed across multiple machines. Seed node : master / server machine that initialises and manages a GPU pool. This is the node where the client runs the start command ( kalavai pool start <pool> ). Note that seed nodes must remain on and available for the platform to remain operational. Worker node : any machine that joins a pool, where the AI workload will be deployed to. This are nodes that run the join command ( kalavai pool join <token> ) Typically a client will be installed in both the seed and worker nodes, but since v0.5.0, clients can also be installed on external machines. This is useful to be able to connect and send work to your pool from any machine. How it works? To create a GPU pool, you need a seed node which acts as a control plane. It handles bookkeeping for the pool. With a seed node, you can generate join tokens, which you can share with other machines -- worker nodes . The more worker nodes you have in a pool, the bigger workloads it can run. Note that the only requirement for a fully functioning pool is a single seed node. Once you have a pool running, you can deploy workloads using template jobs . These are community integrations that let users deploy AI using multiple frameworks (such as vLLM, SGLang, llama.cpp and n8n). A template makes using Kalavai really easy for end users, with a parameterised interface, and it also makes the platform infinitely expandable . Kalavai platform For a fully managed computing pool, consider our managed service . Managed seed instance for over-the-internet pools Encrypted communication with VPN Access to flexible and scalable GPUs fleet from Kalavai Check out our documentation for more details. Want to be notified of the latest features? Subscribe to our substack channel , where we regularly publish news, articles and updates. Join our discord community","title":"Overview"},{"location":"#core-features","text":"Kalavai helps teams use GPU resources more efficiently. It acts as a control plane for all your GPUs , wherever they are: local, on prem and multi-cloud. Increase GPU utilisation from your devices (fractional GPU). Multi-node, multi-GPU and multi-architecture support (AMD and NVIDIA). Aggregate computing resources from multiple sources : home desktops, on premise servers<0>, multi cloud VMs, raspberry pi's, etc. Including our own GPU fleets . Ready-made templates to deploy common AI building blocks : model inference (vLLM, llama.cpp, SGLang), GPU clusters (Ray, GPUStack), automation workflows (n8n and Flowise), evaluation and monitoring tools (Langfuse), production dev tools (LiteLLM, OpenWebUI) and more. Easy to expand to custom workloads","title":"Core features"},{"location":"#getting-started","text":"The best way to getting started is to check out our start guide for a step-by-step guide on how to get started. This is the recommended entry point for those that wish to explore the basics of Kalavai.","title":"Getting started"},{"location":"#core-components","text":"Kalavai turns devices into a scalable LLM platform . It connects multiple machines together and manages the distribution AI workloads on them. There are three core components: Kalavai client : python CLI program that lets users create and interact with GPU pools distributed across multiple machines. Seed node : master / server machine that initialises and manages a GPU pool. This is the node where the client runs the start command ( kalavai pool start <pool> ). Note that seed nodes must remain on and available for the platform to remain operational. Worker node : any machine that joins a pool, where the AI workload will be deployed to. This are nodes that run the join command ( kalavai pool join <token> ) Typically a client will be installed in both the seed and worker nodes, but since v0.5.0, clients can also be installed on external machines. This is useful to be able to connect and send work to your pool from any machine.","title":"Core components"},{"location":"#how-it-works","text":"To create a GPU pool, you need a seed node which acts as a control plane. It handles bookkeeping for the pool. With a seed node, you can generate join tokens, which you can share with other machines -- worker nodes . The more worker nodes you have in a pool, the bigger workloads it can run. Note that the only requirement for a fully functioning pool is a single seed node. Once you have a pool running, you can deploy workloads using template jobs . These are community integrations that let users deploy AI using multiple frameworks (such as vLLM, SGLang, llama.cpp and n8n). A template makes using Kalavai really easy for end users, with a parameterised interface, and it also makes the platform infinitely expandable .","title":"How it works?"},{"location":"#kalavai-platform","text":"For a fully managed computing pool, consider our managed service . Managed seed instance for over-the-internet pools Encrypted communication with VPN Access to flexible and scalable GPUs fleet from Kalavai Check out our documentation for more details.","title":"Kalavai platform"},{"location":"#want-to-be-notified-of-the-latest-features","text":"Subscribe to our substack channel , where we regularly publish news, articles and updates. Join our discord community","title":"Want to be notified of the latest features?"},{"location":"amd_node/","text":"Kalavai platform supports the use of both NVIDIA and AMD cards. To ensure full compatibility, the worker node must have the following requirements met: OS: Linux; kernel <= 6.11 Python: 3.10 - 3.12 GPU: MI200s (gfx90a), MI300 (gfx942), Radeon RX 7900 series (gfx1100) ROCm 6.4.2 Installing dependencies In this example we are using Ubuntu 24.04 LTS as a base OS, but this will work with any debian-based distribution too. # install docker sudo apt-get update sudo apt-get install ca-certificates curl gcc-14 -y sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc sudo tee /etc/apt/sources.list.d/docker.sources <<EOF Types: deb URIs: https://download.docker.com/linux/ubuntu Suites: $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") Components: stable Signed-By: /etc/apt/keyrings/docker.asc EOF sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y # optional sudo usermod -aG docker $USER # install python3.12+ (dev and venv) sudo apt install python3.12-dev python3.12-venv -y # install ROCm 6.4.2 wget https://repo.radeon.com/amdgpu-install/6.4.2/ubuntu/noble/amdgpu-install_6.4.60402-1_all.deb sudo dpkg -i amdgpu-install_6.4.60402-1_all.deb sudo apt update && sudo apt install python3-setuptools python3-wheel -y sudo usermod -a -G render,video $LOGNAME sudo apt install rocm -y # install AMD-GPU driver sudo apt install \"linux-headers- $( uname -r ) \" -y sudo apt update && sudo apt install amdgpu-dkms -y # Reboot machine sudo reboot # (optional) add rocm-smi and amd-smi to path (add to ~/.bashrc) export PATH = $PATH :/opt/rocm-6.4.2/bin:/opt/rocm-6.4.2/libexec/rocm_smi Once the node is configured, it can join the Kalavai pool as usual: # install kalavai-client python3 -m venv myenv source myenv/bin/activate pip install kalavai-client # join the network kalavai auth <user_id> kalavai pool join <token> See our getting started guide for more information about the joining process.","title":"AMD"},{"location":"amd_node/#installing-dependencies","text":"In this example we are using Ubuntu 24.04 LTS as a base OS, but this will work with any debian-based distribution too. # install docker sudo apt-get update sudo apt-get install ca-certificates curl gcc-14 -y sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc sudo tee /etc/apt/sources.list.d/docker.sources <<EOF Types: deb URIs: https://download.docker.com/linux/ubuntu Suites: $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") Components: stable Signed-By: /etc/apt/keyrings/docker.asc EOF sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y # optional sudo usermod -aG docker $USER # install python3.12+ (dev and venv) sudo apt install python3.12-dev python3.12-venv -y # install ROCm 6.4.2 wget https://repo.radeon.com/amdgpu-install/6.4.2/ubuntu/noble/amdgpu-install_6.4.60402-1_all.deb sudo dpkg -i amdgpu-install_6.4.60402-1_all.deb sudo apt update && sudo apt install python3-setuptools python3-wheel -y sudo usermod -a -G render,video $LOGNAME sudo apt install rocm -y # install AMD-GPU driver sudo apt install \"linux-headers- $( uname -r ) \" -y sudo apt update && sudo apt install amdgpu-dkms -y # Reboot machine sudo reboot # (optional) add rocm-smi and amd-smi to path (add to ~/.bashrc) export PATH = $PATH :/opt/rocm-6.4.2/bin:/opt/rocm-6.4.2/libexec/rocm_smi Once the node is configured, it can join the Kalavai pool as usual: # install kalavai-client python3 -m venv myenv source myenv/bin/activate pip install kalavai-client # join the network kalavai auth <user_id> kalavai pool join <token> See our getting started guide for more information about the joining process.","title":"Installing dependencies"},{"location":"boinc/","text":"BOINC is an open source platform for volunteer computing, organised in scientific projects. Kalavai makes it easy to share your computing resources with Science United , a coordinated model for scientific computing where volunteers share their machines with a multitude of projects. What you get by sharing: Eternal kudos from the community! Kalavai credits that can be used in any of our public pools Requirements A free Kalavai account. Create one here . A computer with the minimum requirements (see below) Hardware requirements 4+ CPUs 4GB+ RAM (optional) 1+ NVIDIA GPU How to join Create a free account with Kalavai. Install the kalavai client following the instructions here . Currently we support Linux distros and Windows. Get the joining token. Visit our platform and go to Community pools . Then click Join on the BOINC Pool to reveal the joining details. Copy the command (including the token). Authenticate the computer you want to use as worker: $ kalavai login [ 10 :33:16 ] Kalavai account details. If you don ' t have an account, create one at https://platform.kalavai.net User email: <your email> Password: <your password> [ 10 :33:25 ] <email> logged in successfully Join the pool with the following command: $ kalavai pool join <token> [ 16 :28:14 ] Token format is correct Joining private network [ 16 :28:24 ] Scanning for valid IPs... Using 100 .10.0.8 address for worker Connecting to BOINC @ 100 .10.0.9 ( this may take a few minutes ) ... [ 16 :29:41 ] Worskpace created You are connected to BOINC That's it, your machine is now contributing to scientific discovery! Stop sharing You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). To pause sharing (but remain on the pool), run the following command: kalavai pool pause When you are ready to resume sharing, run: kalavai pool resume To stop and leave the pool, run the following: kalavai pool stop FAQs Something isn't right Growing pains! Please report any issues in our github repository . Can I join (and leave) whenever I want? Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here . What is in it for me? If you decide to share your compute with BOINC, you will gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon). Is my GPU constantly being used? No. BOINC projects upload tasks to be completed to a queue, which volunteers computers poll for work. If there is no suitable work to be done by the worker, the machine will remain idle and no resources are spent. If at any point you need your machine back, pause or stop sharing and come back when you are free. How do I check how much have I contributed? Kalavai pools all the machines together and contributes to BOINC as a single entity. You can check how much the pool has shared overtime through the Science United leaderboard page -look out for Kalavai.net entry. Individual users can also check how much compute have they contributed via their home page in our platform . Once you are logged in, click on the button displaying your user name on the left panel. This view will show how much of each key resource you have contributed thus far (CPUs, RAM, GPU).","title":"Boinc"},{"location":"boinc/#requirements","text":"A free Kalavai account. Create one here . A computer with the minimum requirements (see below) Hardware requirements 4+ CPUs 4GB+ RAM (optional) 1+ NVIDIA GPU","title":"Requirements"},{"location":"boinc/#how-to-join","text":"Create a free account with Kalavai. Install the kalavai client following the instructions here . Currently we support Linux distros and Windows. Get the joining token. Visit our platform and go to Community pools . Then click Join on the BOINC Pool to reveal the joining details. Copy the command (including the token). Authenticate the computer you want to use as worker: $ kalavai login [ 10 :33:16 ] Kalavai account details. If you don ' t have an account, create one at https://platform.kalavai.net User email: <your email> Password: <your password> [ 10 :33:25 ] <email> logged in successfully Join the pool with the following command: $ kalavai pool join <token> [ 16 :28:14 ] Token format is correct Joining private network [ 16 :28:24 ] Scanning for valid IPs... Using 100 .10.0.8 address for worker Connecting to BOINC @ 100 .10.0.9 ( this may take a few minutes ) ... [ 16 :29:41 ] Worskpace created You are connected to BOINC That's it, your machine is now contributing to scientific discovery!","title":"How to join"},{"location":"boinc/#stop-sharing","text":"You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). To pause sharing (but remain on the pool), run the following command: kalavai pool pause When you are ready to resume sharing, run: kalavai pool resume To stop and leave the pool, run the following: kalavai pool stop","title":"Stop sharing"},{"location":"boinc/#faqs","text":"","title":"FAQs"},{"location":"boinc/#something-isnt-right","text":"Growing pains! Please report any issues in our github repository .","title":"Something isn't right"},{"location":"boinc/#can-i-join-and-leave-whenever-i-want","text":"Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here .","title":"Can I join (and leave) whenever I want?"},{"location":"boinc/#what-is-in-it-for-me","text":"If you decide to share your compute with BOINC, you will gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon).","title":"What is in it for me?"},{"location":"boinc/#is-my-gpu-constantly-being-used","text":"No. BOINC projects upload tasks to be completed to a queue, which volunteers computers poll for work. If there is no suitable work to be done by the worker, the machine will remain idle and no resources are spent. If at any point you need your machine back, pause or stop sharing and come back when you are free.","title":"Is my GPU constantly being used?"},{"location":"boinc/#how-do-i-check-how-much-have-i-contributed","text":"Kalavai pools all the machines together and contributes to BOINC as a single entity. You can check how much the pool has shared overtime through the Science United leaderboard page -look out for Kalavai.net entry. Individual users can also check how much compute have they contributed via their home page in our platform . Once you are logged in, click on the button displaying your user name on the left panel. This view will show how much of each key resource you have contributed thus far (CPUs, RAM, GPU).","title":"How do I check how much have I contributed?"},{"location":"cli/","text":"The full functionality set of Kalavai LLM Pools can be accessed via the command line. This is ideal when working with Virtual Machines in the cloud or in automating workflows where GUI access is not possible or not required. Requirements Make sure you have the kalavai-client installed in your machine before continuing. Overview $ kalavai --help usage: kalavai [ -h ] command ... positional arguments: command login [ AUTH ] ( For public clusters only ) Log in to Kalavai server. logout [ AUTH ] ( For public clusters only ) Log out of Kalavai server. gui location pool storage node job ray options: -h, --help show this help message and exit For help on a specific command, or group of commands, you can use the --help flag: $ kalavai pool --help usage: kalavai pool [ -h ] command ... positional arguments: command publish [ AUTH ] Publish pool to Kalavai platform, where other users may be able to join unpublish [ AUTH ] Unpublish pool to Kalavai platform. Cluster and all its workers will still work list [ AUTH ] List public pools in to Kalavai platform. start Start Kalavai pool and start/resume sharing resources. token Generate a join token for others to connect to your pool check-token Utility to check the validity of a join token join Join Kalavai pool and start/resume sharing resources. stop Stop sharing your device and clean up. DO THIS ONLY IF YOU WANT TO REMOVE KALAVAI-CLIENT from your device. pause Pause sharing your device and make your device unavailable for kalavai scheduling. resume Resume sharing your device and make device available for kalavai scheduling. gpus Display GPU information from all connected nodes resources Display information about resources on the pool update Update kalavai pool status Run diagnostics on a local installation of kalavai attach Set creds in token on the local instance options: -h, --help show this help message and exit Examples Start a seed node and get token kalavai pool start <pool-name> Now you are ready to add worker nodes to this seed. To do so, generate a joining token: $ kalavai pool token --user Join token: <token> Add worker nodes kalavai pool join <token> Attach more clients You can now connect to an existing pool from any computer -not just from worker nodes. To connect to a pool, run: kalavai pool attach <token> Check resources in the pool List resources are available: $ kalavai pool resources \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 \u2503 n_nodes \u2503 cpu \u2503 memory \u2503 nvidia.com/gpu \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 Available \u2502 4 \u2502 38 .08 \u2502 70096719872 \u2502 3 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Total \u2502 4 \u2502 42 \u2502 70895734784 \u2502 3 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 $ kalavai pool gpus \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Node \u2503 Ready \u2503 GPU ( s ) \u2503 Available \u2503 Total \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 carlosfm-desktop-1 \u2502 True \u2502 NVIDIA-NVIDIA GeForce RTX 2070 ( 8 GBs ) \u2502 1 \u2502 1 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 carlosfm-desktop-2 \u2502 True \u2502 NVIDIA-NVIDIA GeForce RTX 3060 ( 12 GBs ) \u2502 1 \u2502 1 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 pop-os \u2502 True \u2502 NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU ( 4 GBs ) \u2502 1 \u2502 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Deploy jobs Deploy a job using a template: $ kalavai job run aphrodite --values qwen2.5-1.5B.yaml [ 01 :42:07 ] SELECT Target GPUs for the job [ KalavaiAuthClient ] Logged in as carlosfm 0 ) Any/None 1 ) NVIDIA-NVIDIA GeForce RTX 2070 ( 8GB ) ( in use: False ) 2 ) NVIDIA-NVIDIA GeForce RTX 3060 ( 12GB ) ( in use: False ) 3 ) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU ( 4GB ) ( in use: False ) --> : 0 [ 01 :42:40 ] AVOID Target GPUs for the job 0 ) Any/None 1 ) NVIDIA-NVIDIA GeForce RTX 2070 ( 8GB ) ( in use: False ) 2 ) NVIDIA-NVIDIA GeForce RTX 3060 ( 12GB ) ( in use: False ) 3 ) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU ( 4GB ) ( in use: False ) --> : 0 [ 01 :43:13 ] Template /home/carlosfm/.cache/kalavai/templates/aphrodite/template.yaml successfully deployed! [ 01 :43:15 ] Service deployed List available jobs: $ kalavai job list \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Deployment \u2503 Status \u2503 Workers \u2503 Endpoint \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 qwen-1 \u2502 [ 2024 -11-27T02:17:35Z ] Pending \u2502 Pending: 1 \u2502 http://100.10.0.2:30271 \u2502 \u2502 \u2502 \u2502 Ready: 1 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ 01 :48:23 ] Check detailed status with kalavai job status <name of deployment> Get logs with kalavai job logs <name of deployment> ( note it only works when the deployment is complete )","title":"CLI"},{"location":"cli/#requirements","text":"Make sure you have the kalavai-client installed in your machine before continuing.","title":"Requirements"},{"location":"cli/#overview","text":"$ kalavai --help usage: kalavai [ -h ] command ... positional arguments: command login [ AUTH ] ( For public clusters only ) Log in to Kalavai server. logout [ AUTH ] ( For public clusters only ) Log out of Kalavai server. gui location pool storage node job ray options: -h, --help show this help message and exit For help on a specific command, or group of commands, you can use the --help flag: $ kalavai pool --help usage: kalavai pool [ -h ] command ... positional arguments: command publish [ AUTH ] Publish pool to Kalavai platform, where other users may be able to join unpublish [ AUTH ] Unpublish pool to Kalavai platform. Cluster and all its workers will still work list [ AUTH ] List public pools in to Kalavai platform. start Start Kalavai pool and start/resume sharing resources. token Generate a join token for others to connect to your pool check-token Utility to check the validity of a join token join Join Kalavai pool and start/resume sharing resources. stop Stop sharing your device and clean up. DO THIS ONLY IF YOU WANT TO REMOVE KALAVAI-CLIENT from your device. pause Pause sharing your device and make your device unavailable for kalavai scheduling. resume Resume sharing your device and make device available for kalavai scheduling. gpus Display GPU information from all connected nodes resources Display information about resources on the pool update Update kalavai pool status Run diagnostics on a local installation of kalavai attach Set creds in token on the local instance options: -h, --help show this help message and exit","title":"Overview"},{"location":"cli/#examples","text":"","title":"Examples"},{"location":"cli/#start-a-seed-node-and-get-token","text":"kalavai pool start <pool-name> Now you are ready to add worker nodes to this seed. To do so, generate a joining token: $ kalavai pool token --user Join token: <token>","title":"Start a seed node and get token"},{"location":"cli/#add-worker-nodes","text":"kalavai pool join <token>","title":"Add worker nodes"},{"location":"cli/#attach-more-clients","text":"You can now connect to an existing pool from any computer -not just from worker nodes. To connect to a pool, run: kalavai pool attach <token>","title":"Attach more clients"},{"location":"cli/#check-resources-in-the-pool","text":"List resources are available: $ kalavai pool resources \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 \u2503 n_nodes \u2503 cpu \u2503 memory \u2503 nvidia.com/gpu \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 Available \u2502 4 \u2502 38 .08 \u2502 70096719872 \u2502 3 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Total \u2502 4 \u2502 42 \u2502 70895734784 \u2502 3 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 $ kalavai pool gpus \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Node \u2503 Ready \u2503 GPU ( s ) \u2503 Available \u2503 Total \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 carlosfm-desktop-1 \u2502 True \u2502 NVIDIA-NVIDIA GeForce RTX 2070 ( 8 GBs ) \u2502 1 \u2502 1 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 carlosfm-desktop-2 \u2502 True \u2502 NVIDIA-NVIDIA GeForce RTX 3060 ( 12 GBs ) \u2502 1 \u2502 1 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 pop-os \u2502 True \u2502 NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU ( 4 GBs ) \u2502 1 \u2502 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Check resources in the pool"},{"location":"cli/#deploy-jobs","text":"Deploy a job using a template: $ kalavai job run aphrodite --values qwen2.5-1.5B.yaml [ 01 :42:07 ] SELECT Target GPUs for the job [ KalavaiAuthClient ] Logged in as carlosfm 0 ) Any/None 1 ) NVIDIA-NVIDIA GeForce RTX 2070 ( 8GB ) ( in use: False ) 2 ) NVIDIA-NVIDIA GeForce RTX 3060 ( 12GB ) ( in use: False ) 3 ) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU ( 4GB ) ( in use: False ) --> : 0 [ 01 :42:40 ] AVOID Target GPUs for the job 0 ) Any/None 1 ) NVIDIA-NVIDIA GeForce RTX 2070 ( 8GB ) ( in use: False ) 2 ) NVIDIA-NVIDIA GeForce RTX 3060 ( 12GB ) ( in use: False ) 3 ) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU ( 4GB ) ( in use: False ) --> : 0 [ 01 :43:13 ] Template /home/carlosfm/.cache/kalavai/templates/aphrodite/template.yaml successfully deployed! [ 01 :43:15 ] Service deployed List available jobs: $ kalavai job list \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Deployment \u2503 Status \u2503 Workers \u2503 Endpoint \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 qwen-1 \u2502 [ 2024 -11-27T02:17:35Z ] Pending \u2502 Pending: 1 \u2502 http://100.10.0.2:30271 \u2502 \u2502 \u2502 \u2502 Ready: 1 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ 01 :48:23 ] Check detailed status with kalavai job status <name of deployment> Get logs with kalavai job logs <name of deployment> ( note it only works when the deployment is complete )","title":"Deploy jobs"},{"location":"compatibility/","text":"If your system is not currently supported, open an issue and request it. We are expanding this list constantly. OS compatibility Currently seed nodes are supported exclusively on linux machines (x86_64 platform). However Kalavai supports mixed pools, i.e. having Windows and MacOS computers as workers. Since worker nodes run inside docker, any machine that can run docker should be compatible with Kalavai. Here are instructions for linux , Windows and MacOS . The kalavai client, which controls and access pools, can be installed on any machine that has python 3.10+. Hardware compatibility: amd64 or x86_64 CPU architecture for seed and worker nodes. arm64 CPU architecture for worker nodes. Note: not all workloads support arm64 workers. NVIDIA GPU or AMD GPUs Mac M series and Intel GPUs are currently not supported ( interested in helping us test it? ) Help testing new systems If you want to help testing Kalavai in new Windows / Linux based systems (thank you!), follow the next steps: Follow the instructions to install the kalavai client. Save the entire install logs (printed out in the console) to a file (install.log) If the installation went through, run kalavai commands to test the output: kalavai pool status > status.log kalavai pool start test > test_pool.log kalavai pool resources > resources.log Create an issue on our repo and share the results. Include the four log files (status.log, test_pool.log, resources.log and install.log) as well as a description of the system you are testing. If the system ends up being supported, you'll be invited to create a PR to add support to the compatibility matrix.","title":"Compatibility"},{"location":"compatibility/#os-compatibility","text":"Currently seed nodes are supported exclusively on linux machines (x86_64 platform). However Kalavai supports mixed pools, i.e. having Windows and MacOS computers as workers. Since worker nodes run inside docker, any machine that can run docker should be compatible with Kalavai. Here are instructions for linux , Windows and MacOS . The kalavai client, which controls and access pools, can be installed on any machine that has python 3.10+.","title":"OS compatibility"},{"location":"compatibility/#hardware-compatibility","text":"amd64 or x86_64 CPU architecture for seed and worker nodes. arm64 CPU architecture for worker nodes. Note: not all workloads support arm64 workers. NVIDIA GPU or AMD GPUs Mac M series and Intel GPUs are currently not supported ( interested in helping us test it? )","title":"Hardware compatibility:"},{"location":"compatibility/#help-testing-new-systems","text":"If you want to help testing Kalavai in new Windows / Linux based systems (thank you!), follow the next steps: Follow the instructions to install the kalavai client. Save the entire install logs (printed out in the console) to a file (install.log) If the installation went through, run kalavai commands to test the output: kalavai pool status > status.log kalavai pool start test > test_pool.log kalavai pool resources > resources.log Create an issue on our repo and share the results. Include the four log files (status.log, test_pool.log, resources.log and install.log) as well as a description of the system you are testing. If the system ends up being supported, you'll be invited to create a PR to add support to the compatibility matrix.","title":"Help testing new systems"},{"location":"faqs/","text":"Work in progress General What are AI pools? In Kalavai parlor, a pool refers to a group of resources. We go beyond machine procurement and include everything a team needs to work on AI; from the hardware devices (GPUs, CPUs and memory) to the setup of a distributed environment and the tech stack needed to make it useful. Kalavai aims to manage it all (procurement of additional cloud resources, installing and configuring open source and industry standard frameworks, configuration management, facilitate distributed computing) so teams can focus on AI innovation. Isn\u2019t the performance of distributed training much slower? Distributed computing is not an option: due to the skyrocketing demand in computation from AI models, we are going to need to use multiple devices to do training and inference. NVIDIA cannot get devices larger fast enough, and cloud providers are busy counting the money they are going to make from all that computing to care. Kalavai has considerable tailwinds that will work to minimise the impact of distributed computing in the future: - Consumer-grade GPU performance per dollar is improving at a faster rate than cloud GPUs - By 2030: Internet broadband speed will reach Gbit/s and 6G will reduce latency < 1 microsecond Host nodes What are the minimum specs for sharing? Is my device safe? Can I use my device whilst sharing? Can I limit what I share with Kalavai? Can I run the kalavai app within a VM? Why does it require sudo privileges? Developers There are plenty of MLOps platforms out there, why would organisations turn to you instead? MLOps solutions out there are great, and they continue to develop. But they all need hardware to run on; whether it is on premise servers, public cloud resources or managed services. We think of MLOps platforms as complementors, that\u2019s why we are building a marketplace for third parties to bring their solutions to our users. Since we manage the computing layer, we abstract away the complexity of integration for them, so they can also bring their tools without having to build multiple integrations. Enterprises You are leveraging the organisation's existing hardware, but this is unlikely to meet AI demands. Are we not back to square one since they need to anyways go to the cloud? Our goal is not to narrow companies' choices but to manage the complexity of hybrid clouds. Organisations can bring hardware from anywhere (their own premises, their company devices, all the way to multi cloud on-demand resources) and Kalavai manages them equally. Developers then see a pool of resources that they treat the same. Organisations with on premise servers already have systems to use them. Why would they trust you to manage that for all their needs? Kalavai works as an integration system, it does not force organisations to switch every workflow they have over to benefit from it. They can install the kalavai client in their existing on premise servers, which will automatically then connect them to the pool and make them able to run workflows. The kalavai client is designed to co-exist with any application and can be limited to use only a portion of resources, so organisations can easily continue to use their on premise deployments. I\u2019ve heard of a bunch of service providers for rent-a-GPU on demand. Isn\u2019t the market saturated already? Kalavai does not have any hardware to lease. We believe there are enough providers out there to cover that. Where there\u2019s a gap is in managing the complexity of use cases that require distributed computing. When workflows require more than one computing device to run, organisations need to manage the orchestration, maintenance and coordination of devices. We have designed Kalavai to integrate nicely with almost any computing resource out there, from public cloud, serverless GPU providers and on premise devices. Got another question?","title":"Faqs"},{"location":"faqs/#general","text":"","title":"General"},{"location":"faqs/#what-are-ai-pools","text":"In Kalavai parlor, a pool refers to a group of resources. We go beyond machine procurement and include everything a team needs to work on AI; from the hardware devices (GPUs, CPUs and memory) to the setup of a distributed environment and the tech stack needed to make it useful. Kalavai aims to manage it all (procurement of additional cloud resources, installing and configuring open source and industry standard frameworks, configuration management, facilitate distributed computing) so teams can focus on AI innovation.","title":"What are AI pools?"},{"location":"faqs/#isnt-the-performance-of-distributed-training-much-slower","text":"Distributed computing is not an option: due to the skyrocketing demand in computation from AI models, we are going to need to use multiple devices to do training and inference. NVIDIA cannot get devices larger fast enough, and cloud providers are busy counting the money they are going to make from all that computing to care. Kalavai has considerable tailwinds that will work to minimise the impact of distributed computing in the future: - Consumer-grade GPU performance per dollar is improving at a faster rate than cloud GPUs - By 2030: Internet broadband speed will reach Gbit/s and 6G will reduce latency < 1 microsecond","title":"Isn\u2019t the performance of distributed training much slower?"},{"location":"faqs/#host-nodes","text":"","title":"Host nodes"},{"location":"faqs/#what-are-the-minimum-specs-for-sharing","text":"","title":"What are the minimum specs for sharing?"},{"location":"faqs/#is-my-device-safe","text":"","title":"Is my device safe?"},{"location":"faqs/#can-i-use-my-device-whilst-sharing","text":"","title":"Can I use my device whilst sharing?"},{"location":"faqs/#can-i-limit-what-i-share-with-kalavai","text":"","title":"Can I limit what I share with Kalavai?"},{"location":"faqs/#can-i-run-the-kalavai-app-within-a-vm","text":"","title":"Can I run the kalavai app within a VM?"},{"location":"faqs/#why-does-it-require-sudo-privileges","text":"","title":"Why does it require sudo privileges?"},{"location":"faqs/#developers","text":"","title":"Developers"},{"location":"faqs/#there-are-plenty-of-mlops-platforms-out-there-why-would-organisations-turn-to-you-instead","text":"MLOps solutions out there are great, and they continue to develop. But they all need hardware to run on; whether it is on premise servers, public cloud resources or managed services. We think of MLOps platforms as complementors, that\u2019s why we are building a marketplace for third parties to bring their solutions to our users. Since we manage the computing layer, we abstract away the complexity of integration for them, so they can also bring their tools without having to build multiple integrations.","title":"There are plenty of MLOps platforms out there, why would organisations turn to you instead?"},{"location":"faqs/#enterprises","text":"","title":"Enterprises"},{"location":"faqs/#you-are-leveraging-the-organisations-existing-hardware-but-this-is-unlikely-to-meet-ai-demands-are-we-not-back-to-square-one-since-they-need-to-anyways-go-to-the-cloud","text":"Our goal is not to narrow companies' choices but to manage the complexity of hybrid clouds. Organisations can bring hardware from anywhere (their own premises, their company devices, all the way to multi cloud on-demand resources) and Kalavai manages them equally. Developers then see a pool of resources that they treat the same.","title":"You are leveraging the organisation's existing hardware, but this is unlikely to meet AI demands. Are we not back to square one since they need to anyways go to the cloud?"},{"location":"faqs/#organisations-with-on-premise-servers-already-have-systems-to-use-them-why-would-they-trust-you-to-manage-that-for-all-their-needs","text":"Kalavai works as an integration system, it does not force organisations to switch every workflow they have over to benefit from it. They can install the kalavai client in their existing on premise servers, which will automatically then connect them to the pool and make them able to run workflows. The kalavai client is designed to co-exist with any application and can be limited to use only a portion of resources, so organisations can easily continue to use their on premise deployments.","title":"Organisations with on premise servers already have systems to use them. Why would they trust you to manage that for all their needs?"},{"location":"faqs/#ive-heard-of-a-bunch-of-service-providers-for-rent-a-gpu-on-demand-isnt-the-market-saturated-already","text":"Kalavai does not have any hardware to lease. We believe there are enough providers out there to cover that. Where there\u2019s a gap is in managing the complexity of use cases that require distributed computing. When workflows require more than one computing device to run, organisations need to manage the orchestration, maintenance and coordination of devices. We have designed Kalavai to integrate nicely with almost any computing resource out there, from public cloud, serverless GPU providers and on premise devices. Got another question?","title":"I\u2019ve heard of a bunch of service providers for rent-a-GPU on demand. Isn\u2019t the market saturated already?"},{"location":"getting_started/","text":"The kalavai client is the main tool to interact with the Kalavai platform, to create and manage pools and also to interact with them (e.g. deploy models). Let's go over its installation. From release v0.5.0, you can now install kalavai client in non-worker computers . You can run a pool on a set of machines and have the client on a remote computer from which you access the LLM pool. Because the client only requires having python installed, this means more computers are now supported to run it. Requirements to run the client For seed nodes: A 64 bits x86 based Linux machine (laptop, desktop or VM) Docker engine installed with privilege access . Python 3.12+ For workers sharing resources with the pool: A laptop, desktop or Virtual Machine (MacOS, Linux or Windows; ARM or x86) If self-hosting, workers should be on the same network as the seed node. Looking for over-the-internet connectivity? Check out our managed service Docker engine installed (for linux , Windows and MacOS ) with privilege access . Python 3.12+ Ports Once a machine is part of a pool, the following ports must be enabled and open to accept and process workloads: Seed nodes : 2379-2380 TCP inbound/outbound 6443 TCP inbound 8472 UDP inbound/outbound 10250 TCP inbound/outbound 51820-51821 UDP inbound/outbound Worker nodes : 6443 TCP outbound 8472 UDP inbound/outbound 10250 TCP inbound/outbound 51820-51821 UDP inbound/outbound 5121 TCP inbound/outbound Install the client The client is a python package and can be installed with one command: pip install kalavai-client Create a local, private pool To create your own GPU pool, you will need at least one machine (the seed) and (optionally) one or more workers. See Kalavai concepts for an overview of AI pool architecture. Note that seed machines should always be available for the platform to remain operational . You can create a seed by self-hosting the open source platform (limited to same network machines only) or using our managed pools service (pre-configured, hosted seed with over-the-internet workers from everywhere). 1a. [Self hosted] Create a seed Note: Currently seed nodes are only supported in Linux x86_64 machines. In any machine with the kalavai client installed, execute the following to start a seed node: kalavai pool start <name> Where is the name of the pool. This will deploy a series of docker containers to manage and interact with the platform. Once the seed is up and running, you can start the GUI manually to manage devices and workloads: $ kalavai gui start [ 10 :11:13 ] Using ports: [ 49152 , 49153 , 49154 ] [ + ] Running 2 /2 \u2714 Network kalavai_kalavai-net Created0.1s \u2714 Container kalavai_gui Started0.4s Loading GUI, may take a few minutes. It will be available at http://localhost:49153 By default, the GUI is available via your browser at http://localhost:49153 (but note the port may change depending on port availability). 1b. [Managed pools] Create a seed We offer a service to host and manage seed nodes with the following advantages: - Connect worker nodes from anywhere (over-the-internet) - Always on to keep your AI pool operational. - Great if you don't have a linux x86_64 machine to use as a seed. Create a free account on our platform . Then, navigate to My Pools to manage and create seed nodes for your pools: Once your seed is up and running and the status is Healthy , follow the on-screen instructions to access it via remote GUI. 2. Add worker nodes Important: if you are self hosting seed nodes, only nodes within the same network as the seed node can be added successfully. This limitation does not apply to our managed seeds Increase the power of your GPU pool by adding resources from other devices. For that, you need to generate a joining token. You can do this by using the seed GUI or the CLI. [On the seed node] Using the GUI Use the navigation panel to go to Devices , and then click the circle-plus button to add new devices. You can select the Access mode , which determine the level of access new nodes will have over the pool: - admin : Same level of access than the seed node, including generating new joining tokens and deleting nodes. - user : Can deploy jobs, but lacks admin access over nodes. - worker : Workers carry on jobs, but cannot deploy their own jobs or manage devices. [On the seed node] Using the CLI Alternatively , if you do not want to use the GUI, you can join from the command-line. Run the following to obtain your joining token: kalavai pool token --worker [On the worker node] Join the pool Once you have the joining token, use it on the machines you want to add to the pool. Workers can use the GUI interface to make this step easier too: From the command line, join with: kalavai pool join <TOKEN> 3. Explore resources For both seed and worker nodes, the dashboard shows a high level view of the LLM pool: resources available, current utilisation and active devices and deployments. Use the navigation bar to see more details on key resources: Resources : every machine and GPU devices connected to the pool and its current status Jobs : all models and deployments active in the pool 4. Leave the pool Any device can leave the pool at any point and its workload will get reassigned. To leave the pool, use the command line CLI on the worker you wish to disconnect: kalavai pool stop What's next Now that you know how to get a pool up and running, check our end to end tutorial on how to self-host an LLM Pool with OpenAI compatible API and a ChatGPT-like interface for all your LLM models.","title":"Quick Start"},{"location":"getting_started/#requirements-to-run-the-client","text":"For seed nodes: A 64 bits x86 based Linux machine (laptop, desktop or VM) Docker engine installed with privilege access . Python 3.12+ For workers sharing resources with the pool: A laptop, desktop or Virtual Machine (MacOS, Linux or Windows; ARM or x86) If self-hosting, workers should be on the same network as the seed node. Looking for over-the-internet connectivity? Check out our managed service Docker engine installed (for linux , Windows and MacOS ) with privilege access . Python 3.12+","title":"Requirements to run the client"},{"location":"getting_started/#ports","text":"Once a machine is part of a pool, the following ports must be enabled and open to accept and process workloads: Seed nodes : 2379-2380 TCP inbound/outbound 6443 TCP inbound 8472 UDP inbound/outbound 10250 TCP inbound/outbound 51820-51821 UDP inbound/outbound Worker nodes : 6443 TCP outbound 8472 UDP inbound/outbound 10250 TCP inbound/outbound 51820-51821 UDP inbound/outbound 5121 TCP inbound/outbound","title":"Ports"},{"location":"getting_started/#install-the-client","text":"The client is a python package and can be installed with one command: pip install kalavai-client","title":"Install the client"},{"location":"getting_started/#create-a-local-private-pool","text":"To create your own GPU pool, you will need at least one machine (the seed) and (optionally) one or more workers. See Kalavai concepts for an overview of AI pool architecture. Note that seed machines should always be available for the platform to remain operational . You can create a seed by self-hosting the open source platform (limited to same network machines only) or using our managed pools service (pre-configured, hosted seed with over-the-internet workers from everywhere).","title":"Create a local, private pool"},{"location":"getting_started/#1a-self-hosted-create-a-seed","text":"Note: Currently seed nodes are only supported in Linux x86_64 machines. In any machine with the kalavai client installed, execute the following to start a seed node: kalavai pool start <name> Where is the name of the pool. This will deploy a series of docker containers to manage and interact with the platform. Once the seed is up and running, you can start the GUI manually to manage devices and workloads: $ kalavai gui start [ 10 :11:13 ] Using ports: [ 49152 , 49153 , 49154 ] [ + ] Running 2 /2 \u2714 Network kalavai_kalavai-net Created0.1s \u2714 Container kalavai_gui Started0.4s Loading GUI, may take a few minutes. It will be available at http://localhost:49153 By default, the GUI is available via your browser at http://localhost:49153 (but note the port may change depending on port availability).","title":"1a. [Self hosted] Create a seed"},{"location":"getting_started/#1b-managed-pools-create-a-seed","text":"We offer a service to host and manage seed nodes with the following advantages: - Connect worker nodes from anywhere (over-the-internet) - Always on to keep your AI pool operational. - Great if you don't have a linux x86_64 machine to use as a seed. Create a free account on our platform . Then, navigate to My Pools to manage and create seed nodes for your pools: Once your seed is up and running and the status is Healthy , follow the on-screen instructions to access it via remote GUI.","title":"1b. [Managed pools] Create a seed"},{"location":"getting_started/#2-add-worker-nodes","text":"Important: if you are self hosting seed nodes, only nodes within the same network as the seed node can be added successfully. This limitation does not apply to our managed seeds Increase the power of your GPU pool by adding resources from other devices. For that, you need to generate a joining token. You can do this by using the seed GUI or the CLI. [On the seed node] Using the GUI Use the navigation panel to go to Devices , and then click the circle-plus button to add new devices. You can select the Access mode , which determine the level of access new nodes will have over the pool: - admin : Same level of access than the seed node, including generating new joining tokens and deleting nodes. - user : Can deploy jobs, but lacks admin access over nodes. - worker : Workers carry on jobs, but cannot deploy their own jobs or manage devices. [On the seed node] Using the CLI Alternatively , if you do not want to use the GUI, you can join from the command-line. Run the following to obtain your joining token: kalavai pool token --worker [On the worker node] Join the pool Once you have the joining token, use it on the machines you want to add to the pool. Workers can use the GUI interface to make this step easier too: From the command line, join with: kalavai pool join <TOKEN>","title":"2. Add worker nodes"},{"location":"getting_started/#3-explore-resources","text":"For both seed and worker nodes, the dashboard shows a high level view of the LLM pool: resources available, current utilisation and active devices and deployments. Use the navigation bar to see more details on key resources: Resources : every machine and GPU devices connected to the pool and its current status Jobs : all models and deployments active in the pool","title":"3. Explore resources"},{"location":"getting_started/#4-leave-the-pool","text":"Any device can leave the pool at any point and its workload will get reassigned. To leave the pool, use the command line CLI on the worker you wish to disconnect: kalavai pool stop","title":"4. Leave the pool"},{"location":"getting_started/#whats-next","text":"Now that you know how to get a pool up and running, check our end to end tutorial on how to self-host an LLM Pool with OpenAI compatible API and a ChatGPT-like interface for all your LLM models.","title":"What's next"},{"location":"gui/","text":"The recommended interface to use for easy control of your Kalavai pools. Requirements Make sure you have the kalavai-client installed in your machine before continuing. Start You can spin up the browser GUI on any machine connected to the pool --seed, worker or connected. To do so, run the following command: $ kalavai gui start TODO Then open the browser and visit http://localhost:49153 .","title":"Browser GUI"},{"location":"gui/#requirements","text":"Make sure you have the kalavai-client installed in your machine before continuing.","title":"Requirements"},{"location":"gui/#start","text":"You can spin up the browser GUI on any machine connected to the pool --seed, worker or connected. To do so, run the following command: $ kalavai gui start TODO Then open the browser and visit http://localhost:49153 .","title":"Start"},{"location":"nvidia_node/","text":"Kalavai platform supports the use of both NVIDIA and AMD cards. To ensure full compatibility, the worker node must have the following requirements met: OS: Linux Python: 3.10+ GPU: Any modern architecture after Pascal (2016) should work (Pascal, Volta, Turing, Ampere, Hopper, Ada Lovelace and Blackwell). Older architectures have not been tested but they may still work (Maxwell and Kepler) CUDA 11.6+ Installing dependencies In this example we are using Ubuntu 24.04 LTS as a base OS, but this will work with any debian-based distribution too. sudo apt update # install nvidia drivers (if not present) sudo apt install nvidia-driver-570 -y # install python dependencies (if not present) sudo apt install python3 python3-pip python3-venv python3-dev -y # install docker sudo apt-get update sudo apt-get install ca-certificates curl gcc-14 -y sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc sudo tee /etc/apt/sources.list.d/docker.sources <<EOF Types: deb URIs: https://download.docker.com/linux/ubuntu Suites: $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") Components: stable Signed-By: /etc/apt/keyrings/docker.asc EOF sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y # install nvidia container runtime curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update export NVIDIA_CONTAINER_TOOLKIT_VERSION = 1 .17.8-1 sudo apt-get install -y \\ nvidia-container-toolkit = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } \\ nvidia-container-toolkit-base = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } \\ libnvidia-container-tools = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } \\ libnvidia-container1 = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } echo \"REBOOT REQUIRED! Run: sudo shutdown -r now\" Once the node is configured, it can join the Kalavai pool as usual: # install kalavai-client python3 -m venv myenv source myenv/bin/activate pip install kalavai-client # join the network kalavai auth <user_id> kalavai pool join <token> See our getting started guide for more information about the joining process.","title":"NVIDIA"},{"location":"nvidia_node/#installing-dependencies","text":"In this example we are using Ubuntu 24.04 LTS as a base OS, but this will work with any debian-based distribution too. sudo apt update # install nvidia drivers (if not present) sudo apt install nvidia-driver-570 -y # install python dependencies (if not present) sudo apt install python3 python3-pip python3-venv python3-dev -y # install docker sudo apt-get update sudo apt-get install ca-certificates curl gcc-14 -y sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc sudo tee /etc/apt/sources.list.d/docker.sources <<EOF Types: deb URIs: https://download.docker.com/linux/ubuntu Suites: $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") Components: stable Signed-By: /etc/apt/keyrings/docker.asc EOF sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y # install nvidia container runtime curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update export NVIDIA_CONTAINER_TOOLKIT_VERSION = 1 .17.8-1 sudo apt-get install -y \\ nvidia-container-toolkit = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } \\ nvidia-container-toolkit-base = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } \\ libnvidia-container-tools = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } \\ libnvidia-container1 = ${ NVIDIA_CONTAINER_TOOLKIT_VERSION } echo \"REBOOT REQUIRED! Run: sudo shutdown -r now\" Once the node is configured, it can join the Kalavai pool as usual: # install kalavai-client python3 -m venv myenv source myenv/bin/activate pip install kalavai-client # join the network kalavai auth <user_id> kalavai pool join <token> See our getting started guide for more information about the joining process.","title":"Installing dependencies"},{"location":"remote_pool/","text":"You can connect to a remote pool too from any machine that has the kalavai-client installed. The only requirement is to have a compatible version of python installed. First, get the connection credentials from the seed node: # Run from the seed node kalavai pool credentials This command shows how to connect a remote machine to the local pool. Kalavai API URL: http://<ip address>:49152 Kalavai API Key: be416a3e-5aa3-47e3-8398-0f993518f3dc Run the following command from a remote machine to connect to this pool: kalavai pool connect http://<ip address>:49152 be416a3e-5aa3-47e3-8398-0f993518f3dc Then, in the remote machine, use those credentials to connect: kalavai pool connect <KALAVAI_API_URL> <API_KEY> Now the local machine can use the CLI and the GUI to send commands to the remote pool","title":"Connect to remote pool"},{"location":"templates/","text":"Work in progress Template jobs built by Kalavai and the community make deploying distributed LLMs easy for end users. Templates are like recipes, where developers describe what worker nodes should run, and users customise the behaviour via pre-defined parameters. Kalavai handles the heavy lifting: workload distribution, communication between nodes and monitoring the state of the deployment to restart the job if required. Using the client, you can list what templates your LLM pool supports: $ kalavai job templates [ 10 :51:29 ] Templates available in the pool [ 'vllm' , 'aphrodite' , 'llamacpp' , 'petals' , 'litellm' , 'playground' , 'boinc' , 'gpustack' ] Deploying a template is easy: kalavai job run <template name> --values <template values> Where <template name> refers to one of the supported templates above, and <template values> is a local yaml file containing the parameters of the job. See examples for more information. List of available templates We currently support out of the box the following AI engines: vLLM : most popular GPU-based model inference. Ray Clusters inference. GPUstack (experimental) Coming soon: llama.cpp : CPU-based GGUF model inference. SGLang : Super fast GPU-based model inference. n8n (experimental): no-code workload automation framework. Flowise (experimental): no-code agentic AI workload framework. Speaches : audio (speech-to-text and text-to-speech) model inference. Langfuse (experimental): open source evaluation and monitoring GenAI framework. OpenWebUI : ChatGPT-like UI playground to interface with any models. diffusers (experimental) RayServe inference. GPUstack (experimental) How to contribute Do you want to develop your own template and share it with the community? We are working on a path to make it easier for developers to do so. Hang on tight! But for now, head over to our repository and check the examples in there. Why is [insert preferred application] not supported? If your preferred distributed ML application is not yet supported, let us know ! Or better yet, add it and contribute to community integrations .","title":"Templates"},{"location":"templates/#list-of-available-templates","text":"We currently support out of the box the following AI engines: vLLM : most popular GPU-based model inference. Ray Clusters inference. GPUstack (experimental) Coming soon: llama.cpp : CPU-based GGUF model inference. SGLang : Super fast GPU-based model inference. n8n (experimental): no-code workload automation framework. Flowise (experimental): no-code agentic AI workload framework. Speaches : audio (speech-to-text and text-to-speech) model inference. Langfuse (experimental): open source evaluation and monitoring GenAI framework. OpenWebUI : ChatGPT-like UI playground to interface with any models. diffusers (experimental) RayServe inference. GPUstack (experimental)","title":"List of available templates"},{"location":"templates/#how-to-contribute","text":"Do you want to develop your own template and share it with the community? We are working on a path to make it easier for developers to do so. Hang on tight! But for now, head over to our repository and check the examples in there.","title":"How to contribute"},{"location":"templates/#why-is-insert-preferred-application-not-supported","text":"If your preferred distributed ML application is not yet supported, let us know ! Or better yet, add it and contribute to community integrations .","title":"Why is [insert preferred application] not supported?"},{"location":"windows_node/","text":"Work in progress. @echo off setlocal :: Replace this with your real Netmaker token set NETMAKER_TOKEN = XX :: Step 1 : Download netclientbundle.exe echo Downloading netclientbundle.exe... curl -L -o netclientbundle.exe https://fileserver.netmaker.io/releases/download/v0.90.0/netclientbundle.exe if %errorlevel% neq 0 ( echo Failed to download netclientbundle.exe exit /b 1 ) :: Step 2 : Install netclientbundle.exe silently echo Installing Netclient... netclientbundle.exe /S if %errorlevel% neq 0 ( echo Netclient installation failed exit /b 1 ) :: Step 3 : Wait a bit in case the installer needs time timeout /t 5 > nul :: Step 4 : Run netclient join echo Joining Netmaker network... netclient join -t %NETMAKER_TOKEN% --static-port -p 51821 if %errorlevel% neq 0 ( echo Netclient join command failed exit /b 1 ) :: Step 5 : Start Docker Compose echo Starting worker with Docker Compose... docker compose -f worker.yaml up -d if %errorlevel% neq 0 ( echo Docker Compose failed exit /b 1 ) echo All steps completed successfully! endlocal pause","title":"Windows"},{"location":"managed/overview/","text":"Welcome to the Kalavai Developer Docs \u2014 your guide to building, training, and deploying AI workloads on Kalavai\u2019s distributed compute platform. Kalavai leverages spare data center capacity to deliver flexible, cost-effective compute for machine learning, AI inference, and large model hosting. What is the Kalavai Platform Kalavai Platform is a managed computing platform that simplifies access to GPU compute and LLM hosting. It builds on our open-source orchestration library , integrating directly with tools you already use \u2014 like Ray and GPUStack \u2014 to provide on-demand distributed compute for AI workloads. Efficient cost and low infrastructure overhead Our platform abstracts the complexity of provisioning and managing GPU clusters, while optimizing performance and cost through dynamic utilization of spare capacity. When using the Kalavai Platform, users have direct access to a large fleet of data centre level GPUs at the lowest price in the market. Product Description Managed GPU Clusters Spin up distributed Ray and GPUStack clusters for training, hyperparameter tuning, reinforcement learning and custom workloads. Async Inference & LLM queue Affordable LLM inference for large scale intelligence projects Model fine tuning Easily customise LLM to your data Beta Tester Program \ud83d\ude80 We\u2019re currently in Beta, and inviting developers and research teams to get early access to Kalavai. We're seeking developers who have hands-on experience with one or more of the following frameworks to participate in our beta testing program: Ray , Unsloth , Axolotl or GPUStack . \ud83d\udc49 Join the Beta Tester Program to get started. Join our exclusive Discord community for beta testers. Get started for free Get a free account here to access the Kalavai Platform. All accounts come with access to free resources, like CPUs, GPUs and memory. During the Beta Testing phase, you will be asked to join the Beta Program the first time you login. This will grant you free resources to test the platform. If you need more resources, you can request more in the Clusters page, but note that capacity is limited.","title":"Overview"},{"location":"managed/overview/#what-is-the-kalavai-platform","text":"Kalavai Platform is a managed computing platform that simplifies access to GPU compute and LLM hosting. It builds on our open-source orchestration library , integrating directly with tools you already use \u2014 like Ray and GPUStack \u2014 to provide on-demand distributed compute for AI workloads.","title":"What is the Kalavai Platform"},{"location":"managed/overview/#efficient-cost-and-low-infrastructure-overhead","text":"Our platform abstracts the complexity of provisioning and managing GPU clusters, while optimizing performance and cost through dynamic utilization of spare capacity. When using the Kalavai Platform, users have direct access to a large fleet of data centre level GPUs at the lowest price in the market. Product Description Managed GPU Clusters Spin up distributed Ray and GPUStack clusters for training, hyperparameter tuning, reinforcement learning and custom workloads. Async Inference & LLM queue Affordable LLM inference for large scale intelligence projects Model fine tuning Easily customise LLM to your data","title":"Efficient cost and low infrastructure overhead"},{"location":"managed/overview/#beta-tester-program","text":"We\u2019re currently in Beta, and inviting developers and research teams to get early access to Kalavai. We're seeking developers who have hands-on experience with one or more of the following frameworks to participate in our beta testing program: Ray , Unsloth , Axolotl or GPUStack . \ud83d\udc49 Join the Beta Tester Program to get started. Join our exclusive Discord community for beta testers.","title":"Beta Tester Program \ud83d\ude80"},{"location":"managed/overview/#get-started-for-free","text":"Get a free account here to access the Kalavai Platform. All accounts come with access to free resources, like CPUs, GPUs and memory. During the Beta Testing phase, you will be asked to join the Beta Program the first time you login. This will grant you free resources to test the platform. If you need more resources, you can request more in the Clusters page, but note that capacity is limited.","title":"Get started for free"},{"location":"use_cases/fine_tuning/","text":"Beta access only. Register your interest here Fine tune models with Axolotl and Ray Axolotl is popular A Free and Open Source LLM Fine-tuning Framework that supports distributed, multi-node, multi-GPU training. It supports running on Ray resources, which means it can easily be run on Kalavai Ray clusters too. For more information on Axolotl , check out their awesome documentation . In this guide we show how easy it is to run Axolotl training runs at any scale to help customise LLMs to your needs. You'll be training a lora adaptor for the model NousResearch/Llama-3.2-1B , and upload it to your HuggingFace account. You'll need at least 8 GB of vRAM for this job. Pre-requisites First, you'll need to create a Ray cluster to run the Axolotl training run on. Check out our guide on how to set up a Ray cluster . Once you have an active Ray cluster, make a note of the Dashboard endpoint as we'll use that to deploy our training jobs. In this guide I'll use the following dashboard address, but you should use the one available in your cluster: Dashboard address : ` http : //127.0.0.1:8265` In your local machine, install ray to match your cluster ray version. Include [default] for Ray Jobs. pip install ray [ default ]== 2 .49.0 Now you are ready to submit an Axolotl job to your cluster. Submit your job Place the axolotl config in your local machine, in a subfolder assets/ , and submit a Ray job in either of the three accepted interfaces below (Ray CLI, python SDK or HTTP requests). Note that the example config provided sets the resources to be used by Ray, i.e. 1 node with 1 GPU: # Ray parameters use_ray : true ray_num_workers : 1 resources_per_worker : GPU : 1 For this example, you'll need at least 8 GB of vRAM for this job. 1. Ray job CLI Set your own HuggingFace token, HF organisation and model name. These details are used to download pretrained weights and to correctly authenticate your account to upload trained weights. export RAY_DASHBOARD_URL = \"http://51.159.173.70:30570\" export HF_TOKEN = \"your token\" export HUB_MODEL_ID = organisation/model_name With those values set, you can submit your job via the CLI: ray job submit --address $RAY_DASHBOARD_URL --entrypoint-num-cpus 1 --working-dir assets/ --runtime-env-json = '{\"env_vars\": {\"HF_TOKEN\": \"' $HF_TOKEN '\", \"HUB_MODEL_ID\": \"' $HUB_MODEL_ID '\"}, \"pip\": [\"torch==2.6.0\", \"axolotl\"], \"config\": {\"setup_timeout_seconds\": 1800}}' -- axolotl train lora-1b-ray.yaml --output-dir output/ --hub-model-id $HUB_MODEL_ID Ensure you set the flat --entrypoint-num-cpus 1 so the job is ran on an active Ray worker. 2. Python API You can also use the python SDK to connect to your cluster. # Filename: run.py from ray.job_submission import JobSubmissionClient # Replace with your own values RAY_DASHBOARD_URL = \"http://127.0.0.1:8265\" HF_TOKEN = \"your token\" HUB_MODEL_ID = \"org/model\" client = JobSubmissionClient ( RAY_DASHBOARD_URL ) job_id = client . submit_job ( entrypoint = \"axolotl train lora-1b-ray.yaml --output-dir output/ --hub_model_id $HUB_MODEL_ID\" , submission_id = \"my_training_1\" , runtime_env = { \"working_dir\" : \"assets/\" , \"env_vars\" : { \"HF_TOKEN\" : HF_TOKEN , \"HUB_MODEL_ID\" : HUB_MODEL_ID }, \"config\" : { \"setup_timeout_seconds\" : 1800 }, \"pip\" : [ \"torch==2.6.0\" , \"axolotl\" ] }, entrypoint_num_cpus = 1 # ensure this is on so the workload is assigned to active Ray workers ) print ( job_id ) Then run: python run.py 3. HTTP request Or use the REST API: https://docs.ray.io/en/latest/cluster/running-applications/job-submission/rest.html Monitor progress Once the job is running, you can access the Ray dashboard in the browser to check the progress of the fine tunning process. Note that it may take a few minutes to create the runtime environment, particularly if you have heavy dependencies. Once the job completes, the model weights are uploaded to your HuggingFace account: Stop a job Get the submission ID of the job you want to stop / delete from the dashboard, or running the CLI: ray job list --address = $RAY_DASHBOARD_URL Then: ray job stop --address = $RAY_DASHBOARD_URL <submission_id> FAQs Handling extra python dependencies If your training run requires further libraries to be installed, you can list them as the pip parameter of your job submission, for example, we can add the requests library: job_id = client . submit_job ( entrypoint = \"...\" , submission_id = \"my_training_1\" , runtime_env = { ... \"pip\" : [ \"torch==2.6.0\" , \"axolotl\" , \"requests\" ] # add as many as you need! }, ... ) Failed to build environment. Timeout Use a large value for setup_timeout_seconds for large environments. ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2. If using flash attention ( flash_attention: true in your axolotl config yaml), make sure you install FlashAttention dependencies in your environment: # flash-attention dependencies pip install packaging ninja wheel pip install torch == 2 .6.0 pip install flash-attn pip install axolotl [ flash-attn ] import flash_attn_2_cuda as flash_attn_gpu ImportError: /home/carlosfm/kalavai/kube-watcher/templates/axolotl/env/lib/python3.12/site-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so: undefined symbol Known issue , try to downgrade flash attention to <2.8.0 version: pip install \"flash-attn<2.8.0\" 403 Forbidden: You don't have the rights to create a model under the namespace \" \". Cannot access content at: https://huggingface.co/api/repos/create. Make sure your token has the correct permissions. Make sure you are setting the HuggingFace token as environment variable for your Ray job, and that the token has write permissions on your username space. export HF_TOKEN = hf_xxxxxxxxxxxxxxxxxxx ray job submit --runtime-env-json = '{\"env_vars\": {\"HF_TOKEN\": \"' $HF_TOKEN '\"}}' -- <entrypoint comand> torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 3.71 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 2.65 GiB memory in use. Of the allocated memory 2.46 GiB is allocated by PyTorch, and 77.64 MiB is reserved by PyTorch but unallocated. This message suggests your cluster does not have enough vRAM memory to run the training job. Try increasing the resources available in your cluster and run again.","title":"Fine tune"},{"location":"use_cases/fine_tuning/#fine-tune-models-with-axolotl-and-ray","text":"Axolotl is popular A Free and Open Source LLM Fine-tuning Framework that supports distributed, multi-node, multi-GPU training. It supports running on Ray resources, which means it can easily be run on Kalavai Ray clusters too. For more information on Axolotl , check out their awesome documentation . In this guide we show how easy it is to run Axolotl training runs at any scale to help customise LLMs to your needs. You'll be training a lora adaptor for the model NousResearch/Llama-3.2-1B , and upload it to your HuggingFace account. You'll need at least 8 GB of vRAM for this job.","title":"Fine tune models with Axolotl and Ray"},{"location":"use_cases/fine_tuning/#pre-requisites","text":"First, you'll need to create a Ray cluster to run the Axolotl training run on. Check out our guide on how to set up a Ray cluster . Once you have an active Ray cluster, make a note of the Dashboard endpoint as we'll use that to deploy our training jobs. In this guide I'll use the following dashboard address, but you should use the one available in your cluster: Dashboard address : ` http : //127.0.0.1:8265` In your local machine, install ray to match your cluster ray version. Include [default] for Ray Jobs. pip install ray [ default ]== 2 .49.0 Now you are ready to submit an Axolotl job to your cluster.","title":"Pre-requisites"},{"location":"use_cases/fine_tuning/#submit-your-job","text":"Place the axolotl config in your local machine, in a subfolder assets/ , and submit a Ray job in either of the three accepted interfaces below (Ray CLI, python SDK or HTTP requests). Note that the example config provided sets the resources to be used by Ray, i.e. 1 node with 1 GPU: # Ray parameters use_ray : true ray_num_workers : 1 resources_per_worker : GPU : 1 For this example, you'll need at least 8 GB of vRAM for this job.","title":"Submit your job"},{"location":"use_cases/fine_tuning/#1-ray-job-cli","text":"Set your own HuggingFace token, HF organisation and model name. These details are used to download pretrained weights and to correctly authenticate your account to upload trained weights. export RAY_DASHBOARD_URL = \"http://51.159.173.70:30570\" export HF_TOKEN = \"your token\" export HUB_MODEL_ID = organisation/model_name With those values set, you can submit your job via the CLI: ray job submit --address $RAY_DASHBOARD_URL --entrypoint-num-cpus 1 --working-dir assets/ --runtime-env-json = '{\"env_vars\": {\"HF_TOKEN\": \"' $HF_TOKEN '\", \"HUB_MODEL_ID\": \"' $HUB_MODEL_ID '\"}, \"pip\": [\"torch==2.6.0\", \"axolotl\"], \"config\": {\"setup_timeout_seconds\": 1800}}' -- axolotl train lora-1b-ray.yaml --output-dir output/ --hub-model-id $HUB_MODEL_ID Ensure you set the flat --entrypoint-num-cpus 1 so the job is ran on an active Ray worker.","title":"1. Ray job CLI"},{"location":"use_cases/fine_tuning/#2-python-api","text":"You can also use the python SDK to connect to your cluster. # Filename: run.py from ray.job_submission import JobSubmissionClient # Replace with your own values RAY_DASHBOARD_URL = \"http://127.0.0.1:8265\" HF_TOKEN = \"your token\" HUB_MODEL_ID = \"org/model\" client = JobSubmissionClient ( RAY_DASHBOARD_URL ) job_id = client . submit_job ( entrypoint = \"axolotl train lora-1b-ray.yaml --output-dir output/ --hub_model_id $HUB_MODEL_ID\" , submission_id = \"my_training_1\" , runtime_env = { \"working_dir\" : \"assets/\" , \"env_vars\" : { \"HF_TOKEN\" : HF_TOKEN , \"HUB_MODEL_ID\" : HUB_MODEL_ID }, \"config\" : { \"setup_timeout_seconds\" : 1800 }, \"pip\" : [ \"torch==2.6.0\" , \"axolotl\" ] }, entrypoint_num_cpus = 1 # ensure this is on so the workload is assigned to active Ray workers ) print ( job_id ) Then run: python run.py","title":"2. Python API"},{"location":"use_cases/fine_tuning/#3-http-request","text":"Or use the REST API: https://docs.ray.io/en/latest/cluster/running-applications/job-submission/rest.html","title":"3. HTTP request"},{"location":"use_cases/fine_tuning/#monitor-progress","text":"Once the job is running, you can access the Ray dashboard in the browser to check the progress of the fine tunning process. Note that it may take a few minutes to create the runtime environment, particularly if you have heavy dependencies. Once the job completes, the model weights are uploaded to your HuggingFace account:","title":"Monitor progress"},{"location":"use_cases/fine_tuning/#stop-a-job","text":"Get the submission ID of the job you want to stop / delete from the dashboard, or running the CLI: ray job list --address = $RAY_DASHBOARD_URL Then: ray job stop --address = $RAY_DASHBOARD_URL <submission_id>","title":"Stop a job"},{"location":"use_cases/fine_tuning/#faqs","text":"Handling extra python dependencies If your training run requires further libraries to be installed, you can list them as the pip parameter of your job submission, for example, we can add the requests library: job_id = client . submit_job ( entrypoint = \"...\" , submission_id = \"my_training_1\" , runtime_env = { ... \"pip\" : [ \"torch==2.6.0\" , \"axolotl\" , \"requests\" ] # add as many as you need! }, ... ) Failed to build environment. Timeout Use a large value for setup_timeout_seconds for large environments. ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2. If using flash attention ( flash_attention: true in your axolotl config yaml), make sure you install FlashAttention dependencies in your environment: # flash-attention dependencies pip install packaging ninja wheel pip install torch == 2 .6.0 pip install flash-attn pip install axolotl [ flash-attn ] import flash_attn_2_cuda as flash_attn_gpu ImportError: /home/carlosfm/kalavai/kube-watcher/templates/axolotl/env/lib/python3.12/site-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so: undefined symbol Known issue , try to downgrade flash attention to <2.8.0 version: pip install \"flash-attn<2.8.0\" 403 Forbidden: You don't have the rights to create a model under the namespace \" \". Cannot access content at: https://huggingface.co/api/repos/create. Make sure your token has the correct permissions. Make sure you are setting the HuggingFace token as environment variable for your Ray job, and that the token has write permissions on your username space. export HF_TOKEN = hf_xxxxxxxxxxxxxxxxxxx ray job submit --runtime-env-json = '{\"env_vars\": {\"HF_TOKEN\": \"' $HF_TOKEN '\"}}' -- <entrypoint comand> torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 3.71 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 2.65 GiB memory in use. Of the allocated memory 2.46 GiB is allocated by PyTorch, and 77.64 MiB is reserved by PyTorch but unallocated. This message suggests your cluster does not have enough vRAM memory to run the training job. Try increasing the resources available in your cluster and run again.","title":"FAQs"},{"location":"use_cases/gpustack/","text":"Beta access only. Register your interest here GPUStack in Kalavai Create, manage and deploy LLMs across multiple devices with GPUStack in Kalavai. What is GPUStack ? GPUStack is an open-source system for managing GPU scheduling and sharing across workloads. Kalavai offers managed GPUStack clusters that automatically allocate resources to the GPUStack pool. Platform features : Create GPUStack clusters with mix hardware (NVIDIA, AMD) without dealing with infrastructure Highly configurable (number of GPUs, node capabilities) Flexible and affordable access to thousands of data centre-level GPUs Getting Started Log in to your Kalavai account and navigate to the Clusters page. This section allows you to create, manage and connect to your GPU clusters. As long as you are within your resource quota (as indicated under Available Resources ) you can create as many clusters as you need --even multiple of the same type. You can create a cluster by selecting any of the supported templates (growing!) under the Create new Cluster section. Create a GPUStack cluster Select `GPUStack` Cluster on the list of cluster templates to configure your GPUStack cluster. Configuring your GPUStack cluster The GPUStack template allows you to configure your cluster instance to your needs. Here's a list of key parameters: admin_password (default: \"password\" , required): Default password for the admin user (required to login to the UI) working_memory (default: 30 , optional): Temporary storage (in GB) to cache model weights. nvidia_gpus (default: 1 , required): Number of NVIDIA GPUs. Each GPU is a separate worker machine. amd_gpus (default: 1 , required): Number of AMD GPUs. Each GPU is a separate worker machine. token (default: \"sometoken\" , required): Token used to load the cluster or authenticate access. hf_token (default: null , required): Huggingface token, required to load model weights. cpus (default: 2 , optional): CPUs per worker (final count = cpus * workers). memory (default: 8 , optional): RAM memory per single worker (final count = memory * workers) When you are ready, click on Deploy Cluster . The GPUStack instance may take a few minutes to spin up. Check the status of the pool under Your clusters . Note that the cluster will be ready for access as soon as the head node is ready. Workers may be queued up to join based on the number and types requested. Example: Hybrid GPU cluster GPUStack lets you connect heterogeneous GPU devices to a single instance. For instance, you may connect 4 NVIDIA GPUs and 8 AMD GPUs with the following settings: nvidia_gpus : 4 amd_gpus : 2 When you deploy models in the GPUStack interface you can target either of the GPUs or do distributed deployments across multiple devices. Connecting to your cluster Once the status of the cluster is Ready you are ready to put the instance to work. Each GPUStack cluster exposes a single endpoint for you to connect to the UI interface. Click on the UI endpoint of your GPUStack cluster. You will be presented with the login screen. Use the following default credentials: Username: admin Password: <password set during cluster config> You can monitor the status of the workers and GPUs under the Resources section of your GPUStack cluster. Deploy a model Follow the official examples from GPUStack to deploy models and more. One of the most interesting features of GPUStack is it's ability to deploy models that don't fit in a single GPU. In this example, I have a GPUStack with 2 GPUs (RTX A4500 with 20GB vRAM each), and I want to deploy Qwen3 8B , which requires approximately 32GB of vRAM. Note: you will need at least ~40GB of working memory in the cluster to accommodate for the model weights Navigate to Deployments, then Deploy Model, and select Catalog. This is the easiest way to select models and quantizations. Search the catalog for Qwen3 8B , and choose the following options: Backend: llama-box Size: 8B Quantization: Q8_0 Click Save to deploy. The model will take a few minutes to be able to serve inference requests. You can follow the progress in the Deployments page: Once it is ready, you can interact with it in the Chat page. You can also check out a code snippet example in the same page by clicking on the View Code button at the top. Known issue: there seems to be an issue with using vLLM backend on distributed (multiple GPUs) deployments that leads to instances hanging after the weights have been downloaded. Please use llama-box backend instead. What next GPUStack documentation","title":"Easy LLMs with GPUstack"},{"location":"use_cases/gpustack/#gpustack-in-kalavai","text":"Create, manage and deploy LLMs across multiple devices with GPUStack in Kalavai.","title":"GPUStack in Kalavai"},{"location":"use_cases/gpustack/#what-is-gpustack","text":"GPUStack is an open-source system for managing GPU scheduling and sharing across workloads. Kalavai offers managed GPUStack clusters that automatically allocate resources to the GPUStack pool. Platform features : Create GPUStack clusters with mix hardware (NVIDIA, AMD) without dealing with infrastructure Highly configurable (number of GPUs, node capabilities) Flexible and affordable access to thousands of data centre-level GPUs","title":"What is GPUStack?"},{"location":"use_cases/gpustack/#getting-started","text":"Log in to your Kalavai account and navigate to the Clusters page. This section allows you to create, manage and connect to your GPU clusters. As long as you are within your resource quota (as indicated under Available Resources ) you can create as many clusters as you need --even multiple of the same type. You can create a cluster by selecting any of the supported templates (growing!) under the Create new Cluster section.","title":"Getting Started"},{"location":"use_cases/gpustack/#create-a-gpustack-cluster","text":"Select `GPUStack` Cluster on the list of cluster templates to configure your GPUStack cluster.","title":"Create a GPUStack cluster"},{"location":"use_cases/gpustack/#configuring-your-gpustack-cluster","text":"The GPUStack template allows you to configure your cluster instance to your needs. Here's a list of key parameters: admin_password (default: \"password\" , required): Default password for the admin user (required to login to the UI) working_memory (default: 30 , optional): Temporary storage (in GB) to cache model weights. nvidia_gpus (default: 1 , required): Number of NVIDIA GPUs. Each GPU is a separate worker machine. amd_gpus (default: 1 , required): Number of AMD GPUs. Each GPU is a separate worker machine. token (default: \"sometoken\" , required): Token used to load the cluster or authenticate access. hf_token (default: null , required): Huggingface token, required to load model weights. cpus (default: 2 , optional): CPUs per worker (final count = cpus * workers). memory (default: 8 , optional): RAM memory per single worker (final count = memory * workers) When you are ready, click on Deploy Cluster . The GPUStack instance may take a few minutes to spin up. Check the status of the pool under Your clusters . Note that the cluster will be ready for access as soon as the head node is ready. Workers may be queued up to join based on the number and types requested.","title":"Configuring your GPUStack cluster"},{"location":"use_cases/gpustack/#example-hybrid-gpu-cluster","text":"GPUStack lets you connect heterogeneous GPU devices to a single instance. For instance, you may connect 4 NVIDIA GPUs and 8 AMD GPUs with the following settings: nvidia_gpus : 4 amd_gpus : 2 When you deploy models in the GPUStack interface you can target either of the GPUs or do distributed deployments across multiple devices.","title":"Example: Hybrid GPU cluster"},{"location":"use_cases/gpustack/#connecting-to-your-cluster","text":"Once the status of the cluster is Ready you are ready to put the instance to work. Each GPUStack cluster exposes a single endpoint for you to connect to the UI interface. Click on the UI endpoint of your GPUStack cluster. You will be presented with the login screen. Use the following default credentials: Username: admin Password: <password set during cluster config> You can monitor the status of the workers and GPUs under the Resources section of your GPUStack cluster.","title":"Connecting to your cluster"},{"location":"use_cases/gpustack/#deploy-a-model","text":"Follow the official examples from GPUStack to deploy models and more. One of the most interesting features of GPUStack is it's ability to deploy models that don't fit in a single GPU. In this example, I have a GPUStack with 2 GPUs (RTX A4500 with 20GB vRAM each), and I want to deploy Qwen3 8B , which requires approximately 32GB of vRAM. Note: you will need at least ~40GB of working memory in the cluster to accommodate for the model weights Navigate to Deployments, then Deploy Model, and select Catalog. This is the easiest way to select models and quantizations. Search the catalog for Qwen3 8B , and choose the following options: Backend: llama-box Size: 8B Quantization: Q8_0 Click Save to deploy. The model will take a few minutes to be able to serve inference requests. You can follow the progress in the Deployments page: Once it is ready, you can interact with it in the Chat page. You can also check out a code snippet example in the same page by clicking on the View Code button at the top. Known issue: there seems to be an issue with using vLLM backend on distributed (multiple GPUs) deployments that leads to instances hanging after the weights have been downloaded. Please use llama-box backend instead.","title":"Deploy a model"},{"location":"use_cases/gpustack/#what-next","text":"GPUStack documentation","title":"What next"},{"location":"use_cases/multi_gpu_inference/","text":"| WIP: coming soon Meanwhile, visit our managed hosting platform CoGenAI for shared and dedicated model hosting and inference, the easiest way to deploy and use LLMs.","title":"Multi-GPU LLM"},{"location":"use_cases/ray/","text":"Beta access only. Register your interest here Ray in Kalavai Create, manage and deploy Ray workloads in Kalavai without touching infrastructure. What is Ray? Ray is a distributed framework for scaling Python and ML workloads. Kalavai\u2019s managed Ray clusters let you launch distributed training or inference tasks without setting up or managing nodes. Platform features : Create Ray clusters that autoscale to your needs without dealing with infrastructure Highly configurable (python version, CUDA kernels, node capabilities) Flexible and affordable access to thousands of data centre-level GPUs Getting Started Log in to your Kalavai account and navigate to the Clusters page. This section allows you to create, manage and connect to your GPU clusters. As long as you are within your resource quota (as indicated under Available Resources ) you can create as many clusters as you need --even multiple of the same type. You can create a cluster by selecting any of the supported templates (growing!) under the Create new Cluster section. Create a Ray cluster Select Ray Cluster on the list of cluster templates to configure your Ray cluster. Configuring your Ray cluster The Ray template allows you to configure your cluster instance to your needs. Here's a list of key parameters: workers (default: 1 , required): Number of desired starting workers min_workers (default: 1 , required): Minimum desired workers for autoscaling max_workers (default: 1 , required): Maximum workers for autoscaling cpus (default: 2 , optional): CPUs to be used per single worker (final one = cpus * workers). Workers should have these many CPUs available. gpus (default: 1 , optional): GPUs to be used per single worker (final one = gpus * workers). Workers should have these many GPUs available. memory (default: 8 , optional): RAM memory to be used per single worker (final one = memory * workers). Workers should have this much RAM available. cuda_gpu_mem_percentage (default: 100 , optional): Maximum memory fraction allowed to be used from the GPU vRAM. ray_version (default: \"2.49.0\" , optional): Ray version to use in the cluster python_version (default: \"312\" , optional): Python version to use in the cluster (39, 310, 311, 312) cuda_version (default: \"cu124\" , optional): CUDA version to use in the cluster (cu117 to cu128) upscaling_mode (default: \"Default\" , optional): Defines autoscale mode. One of: Conservative, Default or Aggressive. More info idle_timeout_seconds (default: 60 , optional): Defines the waiting time in seconds before scaling down an idle worker pod. More info When you are ready, click on Deploy Cluster . The Ray instance may take a few minutes to spin up. Check the status of the pool under Your clusters . Example: Autoscalable cluster Autoscalable clusters are ideal to keep cost under control. You can set up a no-GPU cluster that autoscales on demand to up to 10 GPUs based on your workloads. To do so, here are the parameters you can use: workers : 0 min_workers : 0 max_workers : 10 idle_time_out : 120 With this configuration, you get a 0 GPUs cluster that scales up to 10 GPUs when you send demand to it. Once the demand ceases, each idle worker scales down itself after 120 seconds. Connecting to your cluster Once the status of the cluster is Ready you are ready to put the instance to work. Each Ray cluster exposes a list of endpoints: GCS : Global Control Store, Coordinates communication between the head node and worker nodes. Dashboard : Provides a web-based monitoring interface for the Ray cluster. Client : Allows external Python clients (e.g., from your laptop) to connect remotely to a running Ray cluster. Used to connect remotely to your cluster with ray.init(address=ray://<address>) . Make sure you do not use http:// within the address and use the ray:// protocol. Serve : The HTTP entrypoint for Ray Serve, which is Ray\u2019s model serving layer. Python example To run Ray in python locally and connect to your cluster, first install a matching version of ray library: pip install ray [ default ]== 2 .49.0 # <-- should match the version on your cluster Connect directly from python # Name: test.py import ray ray . init ( \"<client endpoint>\" ) @ray . remote def f ( x ): return x * x futures = [ f . remote ( i ) for i in range ( 2 )] print ( ray . get ( futures )) # [0, 1] And execute it locally: python test.py Note that your local python version must match that of the cluster. If you want to wave this restriction, use the submission route below. Submit script to your cluster We need to create a python script, put it in its own working folder, then submit it to our cluster. Create a raytest.py script and place it under raytest/ folder: import ray ray . init () @ray . remote def f ( x ): return x * x futures = [ f . remote ( i ) for i in range ( 2 )] print ( ray . get ( futures )) # [0, 1] The folder structure should look as follows: raytest/ | |---raytest.py Now submit your job using the dashboard endpoint in your Ray cluster as address . ray job submit --working-dir ./raytest --address <dashboard endpoint> -- python raytest.py You should see the output in the console, and can also inspect the job progress by visiting the dashboard endpoint in your browser, under Jobs What next Ray official documentation and examples . FAQs ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set When using PyTorch Distributed (torch.distributed) with backend env:// , MASTER_ADDR and MASTER_PORT must be set manually. You can set both environment variables by passing them with your Ray Job: ray job submit --runtime-env-json = '{\"env_vars\": {\"MASTER_ADDR\": \"127.0.0.1\", \"MASTER_PORT\": \"29500\"}}' -- <endpoint command> or if using Python: job_id = client . submit_job ( entrypoint = \"<endpoint command>\" , submission_id = \"my_training_1\" , runtime_env = { \"env_vars\" : { \"MASTER_ADDR\" : \"127.0.0.1\" , \"MASTER_PORT\" : \"29500\" } } ) PyTorch distributed is used in plain PyTorch DDP, HuggingFace accelerate, DeepSpeed and any code that calls: torch . distributed . init_process_group ( \"nccl\" , init_method = \"env://\" )","title":"Production GPU fleets"},{"location":"use_cases/ray/#ray-in-kalavai","text":"Create, manage and deploy Ray workloads in Kalavai without touching infrastructure.","title":"Ray in Kalavai"},{"location":"use_cases/ray/#what-is-ray","text":"Ray is a distributed framework for scaling Python and ML workloads. Kalavai\u2019s managed Ray clusters let you launch distributed training or inference tasks without setting up or managing nodes. Platform features : Create Ray clusters that autoscale to your needs without dealing with infrastructure Highly configurable (python version, CUDA kernels, node capabilities) Flexible and affordable access to thousands of data centre-level GPUs","title":"What is Ray?"},{"location":"use_cases/ray/#getting-started","text":"Log in to your Kalavai account and navigate to the Clusters page. This section allows you to create, manage and connect to your GPU clusters. As long as you are within your resource quota (as indicated under Available Resources ) you can create as many clusters as you need --even multiple of the same type. You can create a cluster by selecting any of the supported templates (growing!) under the Create new Cluster section.","title":"Getting Started"},{"location":"use_cases/ray/#create-a-ray-cluster","text":"Select Ray Cluster on the list of cluster templates to configure your Ray cluster.","title":"Create a Ray cluster"},{"location":"use_cases/ray/#configuring-your-ray-cluster","text":"The Ray template allows you to configure your cluster instance to your needs. Here's a list of key parameters: workers (default: 1 , required): Number of desired starting workers min_workers (default: 1 , required): Minimum desired workers for autoscaling max_workers (default: 1 , required): Maximum workers for autoscaling cpus (default: 2 , optional): CPUs to be used per single worker (final one = cpus * workers). Workers should have these many CPUs available. gpus (default: 1 , optional): GPUs to be used per single worker (final one = gpus * workers). Workers should have these many GPUs available. memory (default: 8 , optional): RAM memory to be used per single worker (final one = memory * workers). Workers should have this much RAM available. cuda_gpu_mem_percentage (default: 100 , optional): Maximum memory fraction allowed to be used from the GPU vRAM. ray_version (default: \"2.49.0\" , optional): Ray version to use in the cluster python_version (default: \"312\" , optional): Python version to use in the cluster (39, 310, 311, 312) cuda_version (default: \"cu124\" , optional): CUDA version to use in the cluster (cu117 to cu128) upscaling_mode (default: \"Default\" , optional): Defines autoscale mode. One of: Conservative, Default or Aggressive. More info idle_timeout_seconds (default: 60 , optional): Defines the waiting time in seconds before scaling down an idle worker pod. More info When you are ready, click on Deploy Cluster . The Ray instance may take a few minutes to spin up. Check the status of the pool under Your clusters .","title":"Configuring your Ray cluster"},{"location":"use_cases/ray/#example-autoscalable-cluster","text":"Autoscalable clusters are ideal to keep cost under control. You can set up a no-GPU cluster that autoscales on demand to up to 10 GPUs based on your workloads. To do so, here are the parameters you can use: workers : 0 min_workers : 0 max_workers : 10 idle_time_out : 120 With this configuration, you get a 0 GPUs cluster that scales up to 10 GPUs when you send demand to it. Once the demand ceases, each idle worker scales down itself after 120 seconds.","title":"Example: Autoscalable cluster"},{"location":"use_cases/ray/#connecting-to-your-cluster","text":"Once the status of the cluster is Ready you are ready to put the instance to work. Each Ray cluster exposes a list of endpoints: GCS : Global Control Store, Coordinates communication between the head node and worker nodes. Dashboard : Provides a web-based monitoring interface for the Ray cluster. Client : Allows external Python clients (e.g., from your laptop) to connect remotely to a running Ray cluster. Used to connect remotely to your cluster with ray.init(address=ray://<address>) . Make sure you do not use http:// within the address and use the ray:// protocol. Serve : The HTTP entrypoint for Ray Serve, which is Ray\u2019s model serving layer.","title":"Connecting to your cluster"},{"location":"use_cases/ray/#python-example","text":"To run Ray in python locally and connect to your cluster, first install a matching version of ray library: pip install ray [ default ]== 2 .49.0 # <-- should match the version on your cluster","title":"Python example"},{"location":"use_cases/ray/#connect-directly-from-python","text":"# Name: test.py import ray ray . init ( \"<client endpoint>\" ) @ray . remote def f ( x ): return x * x futures = [ f . remote ( i ) for i in range ( 2 )] print ( ray . get ( futures )) # [0, 1] And execute it locally: python test.py Note that your local python version must match that of the cluster. If you want to wave this restriction, use the submission route below.","title":"Connect directly from python"},{"location":"use_cases/ray/#submit-script-to-your-cluster","text":"We need to create a python script, put it in its own working folder, then submit it to our cluster. Create a raytest.py script and place it under raytest/ folder: import ray ray . init () @ray . remote def f ( x ): return x * x futures = [ f . remote ( i ) for i in range ( 2 )] print ( ray . get ( futures )) # [0, 1] The folder structure should look as follows: raytest/ | |---raytest.py Now submit your job using the dashboard endpoint in your Ray cluster as address . ray job submit --working-dir ./raytest --address <dashboard endpoint> -- python raytest.py You should see the output in the console, and can also inspect the job progress by visiting the dashboard endpoint in your browser, under Jobs","title":"Submit script to your cluster"},{"location":"use_cases/ray/#what-next","text":"Ray official documentation and examples .","title":"What next"},{"location":"use_cases/ray/#faqs","text":"ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set When using PyTorch Distributed (torch.distributed) with backend env:// , MASTER_ADDR and MASTER_PORT must be set manually. You can set both environment variables by passing them with your Ray Job: ray job submit --runtime-env-json = '{\"env_vars\": {\"MASTER_ADDR\": \"127.0.0.1\", \"MASTER_PORT\": \"29500\"}}' -- <endpoint command> or if using Python: job_id = client . submit_job ( entrypoint = \"<endpoint command>\" , submission_id = \"my_training_1\" , runtime_env = { \"env_vars\" : { \"MASTER_ADDR\" : \"127.0.0.1\" , \"MASTER_PORT\" : \"29500\" } } ) PyTorch distributed is used in plain PyTorch DDP, HuggingFace accelerate, DeepSpeed and any code that calls: torch . distributed . init_process_group ( \"nccl\" , init_method = \"env://\" )","title":"FAQs"},{"location":"use_cases/self_hosted_llm_pool/","text":"\u2b50\u2b50\u2b50 Kalavai and our LLM pools are open source and free to use in both commercial and non-commercial purposes. If you find it useful, consider supporting us by giving a star to our GitHub project , joining our discord channel and follow our Substack . This guide will show you how to start a self-hosted LLM pool with your own hardware, configure it with a single API and UI Playground for all your models and deploy and access a Qwen3 4B instance. What you'll achieve Configure unified LLM interface Deploy a llamacpp model Access model via code and UI 1. Pre-requisites Install kalavai CLI on each machine Set up a 2 machine LLM pool , i.e. a seed node and one worker Note: the following commands can be executed on any machine that is part of the pool, provided you have used admin or user access modes to generate the token. If you have used worker , deployments are only allowed in the seed node. Unified OpenAI-like API Model templates deployed in LLM pools have an optional key parameter to register themselves with a LiteLLM instance. LiteLLM is a powerful API that unifies all of your models into a single API, making developing apps with LLMs easier and more flexible. Our LiteLLM template automates the deployment of the API across a pool, database included. To deploy it using the Kalavai GUI, navigate to Jobs , then click on the circle-plus button, in which you can select a litellm template. Once the deployment is complete, you can check the LiteLLM endpoint by navigating to Jobs and seeing the corresponding endpoint for the litellm job. You will need a virtual key to register models with LiteLLM. For testing you can use the master key defined in your values.yaml under master_key , but it is recommended to generate a virtual one that does not have privilege access. The easiest way of doing so is via the admin UI, under http://192.168.68.67:30535/ui (see more details here ). Example virtual key: sk-rDCm0Vd5hDOigaNbQSSsEQ Unified UI Playground OpenWebUI is a great ChatGPT-like app that helps testing LLMs. Our WebUI template manages the deployment of an OpenWebUI instance in your LLM pool, and links it to your LiteLLM instance, so any models deployed and registered with LiteLLM automatically appear in the playground. To deploy, navigate back to Jobs and click the circle-plus button, this time selecting the playground template. Set the litellm_key to match your virtual key. Once it's ready, you can access the UI via its advertised endpoint (under Jobs ), directly on your browser. The first time you login you'll be able to create an admin user. Check the official documentation for more details on the app. Check deployment progress Jobs may take a while to deploy. Check the progress in the Jobs page, or using the CLI: $ kalavai job list \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Owner \u2503 Deployment \u2503 Workers \u2503 Endpoint \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 default \u2502 litellm \u2502 Ready: 2 \u2502 http://192.168.68.67:30535 ( mapped to 4000 ) \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 default \u2502 webui-1 \u2502 Pending: 1 \u2502 http://192.168.68.67:31141 ( mapped to 8080 ) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 In this case, litellm has been deployed but webui-1 is still pending schedule. If a job cannot be scheduled due to lack of resources, consider adding more nodes or reducing the requested resources via the values.yaml files. 3. Deploy models with compatible frameworks In this section, we'll look into how to deploy a model with another of our supported model engines: llama.cpp . You can use the kalavai CLI to deploy jobs (via kalavai job deploy) but here we'll use the much simpler GUI route. Just like we did for LiteLLM and Playground, you can deploy a model by navigating to the Jobs page and clicking the circle-plus button. Select llamacpp as model template, and populate the following values: working_memory : 10 (enough free space GBs to fit the model weights) workers : 2 (this will distribute the model onto our 2 machines) repo_id : Qwen/Qwen3-4B-GGUF (the repo id from Huggingface) model_filename : Qwen3-4B-Q4_K_M.gguf (the filename of the quantized version we want) hf_token : if using a gated model (in this case it's not needed) litellm_key : sk-qoQC5lijoaBwXoyi_YP1xA (Advanced parameter; the virtual key generated above for LiteLLM. This is key to make sure models are self registering to both LiteLLM and the playground. ) 4. Access your models Once they are donwloaded and loaded into memory, your models will be readily available both via the LiteLLM API as well as through the UI Playground. UI Playground The pool comes with an OpenWebUI deployment ( playground job) to make it easy to test model inference with LLMs via the browser. Within the UI you can select the model you wish to test and have a chat. Note: the playground is a shared instance to help users test models without code and should not be used in production. You need to create a playground account to access it. This can be different to your Kalavai account details. The creation of a new user is necessary to keep things like user chat history and preferences. Single API endpoint All interactions to models in the pool are brokered by a LiteLLM endpoint that is installed in the system. To interact with it you need the following: The LITELLM_URL is the endpoint displayed in the Jobs page for the litellm job. The LITELLM_KEY is the one you have generated above. The MODEL_NAME you want to use (the job name displayed in the Jobs page) In this example: LITELLM_URL=http://192.168.68.67:30535 LITELLM_KEY=sk-qoQC5lijoaBwXoyi_YP1xA MODEL_NAME=qwen3_qwen3_4b_gguf_qwen3_4b_q4_k_m_gguf Check available LLMs Using cURL: curl -X GET \"<LITELLM_URL>/v1/models\" \\ -H 'Authorization: Bearer <LITELLM_KEY>' \\ -H \"accept: application/json\" \\ -H \"Content-Type: application/json\" Using python: import requests LITELLM_URL = \"http://192.168.68.67:30535\" LITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\" def list_models (): response = requests . get ( f \" { LITELLM_URL } /v1/models\" , headers = { \"Authorization\" : f \"Bearer { LITELLM_KEY } \" } ) return response . json () if __name__ == \"__main__\" : print ( list_models () ) Use models Using cURL: curl --location '<LITELLM_URL>/chat/completions' \\ --header 'Authorization: Bearer <LITELLM_KEY>' \\ --header 'Content-Type: application/json' \\ --data '{ \"model\": \"<MODEL_NAME>\", \"messages\": [ { \"role\": \"user\", \"content\": \"what llm are you\" } ] }' Using python: import requests LITELLM_URL = \"http://192.168.68.67:30535\" LITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\" def model_inference (): response = requests . post ( f \" { LITELLM_URL } /chat/completions\" , headers = { \"Authorization\" : f \"Bearer { LITELLM_KEY } \" }, json = { \"model\" : \"<MODEL_NAME>\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"what llm are you\" }] } ) return response . json () if __name__ == \"__main__\" : print ( model_inference () ) For more details on the endpoint(s) parameters, check out LiteLLM documentation and the Swagger API 5. Clean up To remove any model deployment, navigate to the Jobs page, select the job (checkbox next to its name) and click the bin icon on top of the table. This will remove the deployment from any worker involved and free its resources. 6. What's next? Enjoy your new supercomputer, check out our templates and examples for more model engines and keep us posted on what you achieve!","title":"BYO Model Gateway"},{"location":"use_cases/self_hosted_llm_pool/#what-youll-achieve","text":"Configure unified LLM interface Deploy a llamacpp model Access model via code and UI","title":"What you'll achieve"},{"location":"use_cases/self_hosted_llm_pool/#1-pre-requisites","text":"Install kalavai CLI on each machine Set up a 2 machine LLM pool , i.e. a seed node and one worker Note: the following commands can be executed on any machine that is part of the pool, provided you have used admin or user access modes to generate the token. If you have used worker , deployments are only allowed in the seed node.","title":"1. Pre-requisites"},{"location":"use_cases/self_hosted_llm_pool/#unified-openai-like-api","text":"Model templates deployed in LLM pools have an optional key parameter to register themselves with a LiteLLM instance. LiteLLM is a powerful API that unifies all of your models into a single API, making developing apps with LLMs easier and more flexible. Our LiteLLM template automates the deployment of the API across a pool, database included. To deploy it using the Kalavai GUI, navigate to Jobs , then click on the circle-plus button, in which you can select a litellm template. Once the deployment is complete, you can check the LiteLLM endpoint by navigating to Jobs and seeing the corresponding endpoint for the litellm job. You will need a virtual key to register models with LiteLLM. For testing you can use the master key defined in your values.yaml under master_key , but it is recommended to generate a virtual one that does not have privilege access. The easiest way of doing so is via the admin UI, under http://192.168.68.67:30535/ui (see more details here ). Example virtual key: sk-rDCm0Vd5hDOigaNbQSSsEQ","title":"Unified OpenAI-like API"},{"location":"use_cases/self_hosted_llm_pool/#unified-ui-playground","text":"OpenWebUI is a great ChatGPT-like app that helps testing LLMs. Our WebUI template manages the deployment of an OpenWebUI instance in your LLM pool, and links it to your LiteLLM instance, so any models deployed and registered with LiteLLM automatically appear in the playground. To deploy, navigate back to Jobs and click the circle-plus button, this time selecting the playground template. Set the litellm_key to match your virtual key. Once it's ready, you can access the UI via its advertised endpoint (under Jobs ), directly on your browser. The first time you login you'll be able to create an admin user. Check the official documentation for more details on the app.","title":"Unified UI Playground"},{"location":"use_cases/self_hosted_llm_pool/#check-deployment-progress","text":"Jobs may take a while to deploy. Check the progress in the Jobs page, or using the CLI: $ kalavai job list \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Owner \u2503 Deployment \u2503 Workers \u2503 Endpoint \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 default \u2502 litellm \u2502 Ready: 2 \u2502 http://192.168.68.67:30535 ( mapped to 4000 ) \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 default \u2502 webui-1 \u2502 Pending: 1 \u2502 http://192.168.68.67:31141 ( mapped to 8080 ) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 In this case, litellm has been deployed but webui-1 is still pending schedule. If a job cannot be scheduled due to lack of resources, consider adding more nodes or reducing the requested resources via the values.yaml files.","title":"Check deployment progress"},{"location":"use_cases/self_hosted_llm_pool/#3-deploy-models-with-compatible-frameworks","text":"In this section, we'll look into how to deploy a model with another of our supported model engines: llama.cpp . You can use the kalavai CLI to deploy jobs (via kalavai job deploy) but here we'll use the much simpler GUI route. Just like we did for LiteLLM and Playground, you can deploy a model by navigating to the Jobs page and clicking the circle-plus button. Select llamacpp as model template, and populate the following values: working_memory : 10 (enough free space GBs to fit the model weights) workers : 2 (this will distribute the model onto our 2 machines) repo_id : Qwen/Qwen3-4B-GGUF (the repo id from Huggingface) model_filename : Qwen3-4B-Q4_K_M.gguf (the filename of the quantized version we want) hf_token : if using a gated model (in this case it's not needed) litellm_key : sk-qoQC5lijoaBwXoyi_YP1xA (Advanced parameter; the virtual key generated above for LiteLLM. This is key to make sure models are self registering to both LiteLLM and the playground. )","title":"3. Deploy models with compatible frameworks"},{"location":"use_cases/self_hosted_llm_pool/#4-access-your-models","text":"Once they are donwloaded and loaded into memory, your models will be readily available both via the LiteLLM API as well as through the UI Playground.","title":"4. Access your models"},{"location":"use_cases/self_hosted_llm_pool/#ui-playground","text":"The pool comes with an OpenWebUI deployment ( playground job) to make it easy to test model inference with LLMs via the browser. Within the UI you can select the model you wish to test and have a chat. Note: the playground is a shared instance to help users test models without code and should not be used in production. You need to create a playground account to access it. This can be different to your Kalavai account details. The creation of a new user is necessary to keep things like user chat history and preferences.","title":"UI Playground"},{"location":"use_cases/self_hosted_llm_pool/#single-api-endpoint","text":"All interactions to models in the pool are brokered by a LiteLLM endpoint that is installed in the system. To interact with it you need the following: The LITELLM_URL is the endpoint displayed in the Jobs page for the litellm job. The LITELLM_KEY is the one you have generated above. The MODEL_NAME you want to use (the job name displayed in the Jobs page) In this example: LITELLM_URL=http://192.168.68.67:30535 LITELLM_KEY=sk-qoQC5lijoaBwXoyi_YP1xA MODEL_NAME=qwen3_qwen3_4b_gguf_qwen3_4b_q4_k_m_gguf","title":"Single API endpoint"},{"location":"use_cases/self_hosted_llm_pool/#check-available-llms","text":"Using cURL: curl -X GET \"<LITELLM_URL>/v1/models\" \\ -H 'Authorization: Bearer <LITELLM_KEY>' \\ -H \"accept: application/json\" \\ -H \"Content-Type: application/json\" Using python: import requests LITELLM_URL = \"http://192.168.68.67:30535\" LITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\" def list_models (): response = requests . get ( f \" { LITELLM_URL } /v1/models\" , headers = { \"Authorization\" : f \"Bearer { LITELLM_KEY } \" } ) return response . json () if __name__ == \"__main__\" : print ( list_models () )","title":"Check available LLMs"},{"location":"use_cases/self_hosted_llm_pool/#use-models","text":"Using cURL: curl --location '<LITELLM_URL>/chat/completions' \\ --header 'Authorization: Bearer <LITELLM_KEY>' \\ --header 'Content-Type: application/json' \\ --data '{ \"model\": \"<MODEL_NAME>\", \"messages\": [ { \"role\": \"user\", \"content\": \"what llm are you\" } ] }' Using python: import requests LITELLM_URL = \"http://192.168.68.67:30535\" LITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\" def model_inference (): response = requests . post ( f \" { LITELLM_URL } /chat/completions\" , headers = { \"Authorization\" : f \"Bearer { LITELLM_KEY } \" }, json = { \"model\" : \"<MODEL_NAME>\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"what llm are you\" }] } ) return response . json () if __name__ == \"__main__\" : print ( model_inference () ) For more details on the endpoint(s) parameters, check out LiteLLM documentation and the Swagger API","title":"Use models"},{"location":"use_cases/self_hosted_llm_pool/#5-clean-up","text":"To remove any model deployment, navigate to the Jobs page, select the job (checkbox next to its name) and click the bin icon on top of the table. This will remove the deployment from any worker involved and free its resources.","title":"5. Clean up"},{"location":"use_cases/self_hosted_llm_pool/#6-whats-next","text":"Enjoy your new supercomputer, check out our templates and examples for more model engines and keep us posted on what you achieve!","title":"6. What's next?"}]}