{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kalavai","text":"<p>Kalavai is an open source platform that turns everyday devices into your very own AI supercomputer. We help you aggregate resources from multiple machines: home desktops, gaming laptops, work computers, cloud VMs... When you need to go beyond, Kalavai facilitates matchmaking of resources so anyone in our community can tap into a larger pool of devices by inspiring others to join your cause.</p>"},{"location":"#what-can-you-do-with-kalavai","title":"What can you do with Kalavai","text":"<p>Kalavai helps manage the complexity of bringing Large Language Models into practical applications. With Kalavai you can easily:</p> <ul> <li>Deploy Large Language Models in a single machine, or seamlessly across multiple nodes</li> <li>Run a Ray cluster for your AI training, fine tuning and serving needs</li> </ul>"},{"location":"#want-to-be-notified-of-the-latest-features","title":"Want to be notified of the latest features?","text":"<p>Subscribe to our substack channel, where we regularly publish news, articles and updates.</p> <p>Join our discord community</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>The best way to getting started is joining our public shared pool, where you can share your computing resources with peers and deploy shared LLMs.</p> <p>If you want to deploy kalavai yourself, check out our start guide for a step-by-step guide on how to get started. This is the recommended entry point for those that wish to explore the basics of Kalavai.</p> <p>For those that are more shoot first, ask later, we've prepared a quick start tutorial that goes straight to the point, from downloading Kalavai to running your own LLM across community machines.</p>"},{"location":"boinc/","title":"BOINC: Volunteer scientific computing","text":"<p>BOINC is an open source platform for volunteer computing, organised in scientific projects.</p> <p>Kalavai makes it easy to share your computing resources with Science United, a coordinated model for scientific computing where volunteers share their machines with a multitude of projects.</p> <p>What you get by sharing:</p> <ul> <li>Eternal kudos from the community!</li> <li>Kalavai credits that can be used in any of our public pools</li> </ul>","tags":["boinc","volunteer computing"]},{"location":"boinc/#requirements","title":"Requirements","text":"<ul> <li>A free Kalavai account. Create one here.</li> <li>A computer with the minimum requirements (see below)</li> </ul> <p>Hardware requirements</p> <ul> <li>4+ CPUs</li> <li>4GB+ RAM</li> <li>(optional) 1+ NVIDIA GPU</li> </ul>","tags":["boinc","volunteer computing"]},{"location":"boinc/#how-to-join","title":"How to join","text":"<ol> <li> <p>Create a free account with Kalavai.</p> </li> <li> <p>Install the kalavai client following the instructions here. Currently we support Linux distros and Windows.</p> </li> <li> <p>Get the joining token. Visit our platform and go to <code>Community pools</code>. Then click <code>Join</code> on the <code>BOINC</code> Pool to reveal the joining details. Copy the command (including the token).</p> </li> </ol> <p></p> <ol> <li>Authenticate the computer you want to use as worker:</li> </ol> <pre><code>$ kalavai login\n\n[10:33:16] Kalavai account details. If you don't have an account, create one at https://platform.kalavai.net                                                                 \nUser email: &lt;your email&gt;\nPassword: &lt;your password&gt;\n\n[10:33:25] &lt;email&gt; logged in successfully\n</code></pre> <ol> <li>Join the pool with the following command:</li> </ol> <pre><code>$ kalavai pool join &lt;token&gt;\n\n[16:28:14] Token format is correct\n           Joining private network\n\n[16:28:24] Scanning for valid IPs...\n           Using 100.10.0.8 address for worker\n           Connecting to BOINC @ 100.10.0.9 (this may take a few minutes)...\n[16:29:41] Worskpace created\n           You are connected to BOINC\n</code></pre> <p>That's it, your machine is now contributing to scientific discovery!</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#stop-sharing","title":"Stop sharing","text":"<p>You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). </p> <p>To pause sharing (but remain on the pool), run the following command:</p> <pre><code>kalavai pool pause\n</code></pre> <p>When you are ready to resume sharing, run:</p> <pre><code>kalavai pool resume\n</code></pre> <p>To stop and leave the pool, run the following:</p> <pre><code>kalavai pool stop\n</code></pre>","tags":["boinc","volunteer computing"]},{"location":"boinc/#faqs","title":"FAQs","text":"","tags":["boinc","volunteer computing"]},{"location":"boinc/#something-isnt-right","title":"Something isn't right","text":"<p>Growing pains! Please report any issues in our github repository.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#can-i-join-and-leave-whenever-i-want","title":"Can I join (and leave) whenever I want?","text":"<p>Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#what-is-in-it-for-me","title":"What is in it for me?","text":"<p>If you decide to share your compute with BOINC, you will gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon).</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#is-my-gpu-constantly-being-used","title":"Is my GPU constantly being used?","text":"<p>No. BOINC projects upload tasks to be completed to a queue, which volunteers computers poll for work. If there is no suitable work to be done by the worker, the machine will remain idle and no resources are spent.</p> <p>If at any point you need your machine back, pause or stop sharing and come back when you are free.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#how-do-i-check-how-much-have-i-contributed","title":"How do I check how much have I contributed?","text":"<p>Kalavai pools all the machines together and contributes to BOINC as a single entity. You can check how much the pool has shared overtime through the Science United leaderboard page -look out for <code>Kalavai.net</code> entry.</p> <p></p> <p>Individual users can also check how much compute have they contributed via their home page in our platform. Once you are logged in, click on the button displaying your user name on the left panel. This view will show how much of each key resource you have contributed thus far (CPUs, RAM, GPU).</p> <p></p>","tags":["boinc","volunteer computing"]},{"location":"choose_job_resources/","title":"Choosing job resources","text":"<p>Work in progress.</p>","tags":["job","resources","estimate resources"]},{"location":"choose_job_resources/#choosing-pool-resources-for-your-jobs","title":"Choosing pool resources for your jobs","text":"<p>Jobs describe the required resources for workers. All of these parameters have default values which are generally good for most cases, but more demanding LLMs will require extra resources. Here are the general resource parameters that are common to all jobs. For engine-specific information, check out the documentation for the job (vLLM, llama.cpp)</p> <ul> <li>working_memory </li> <li>cpus (per worker)</li> <li>memory (per worker RAM)</li> </ul> <p>If you want help on how much a model may require, you can use our internal estimation tool:</p> <p>kalavai job estimate  --precision  <p>For example, to deploy a 1B model at 16 floating point precision:</p> <pre><code>$ kalavai job estimate 1 --precision 16\n\nThere are 3 GPUs available (24.576GBs) \nA 1B model requires ~1.67GB vRAM at 16bits precision\nLooking at current capacity, use 1 GPU workers for a total 4.10 GB vRAM\n</code></pre>","tags":["job","resources","estimate resources"]},{"location":"cli/","title":"Kalavai from the command line (CLI)","text":"<p>The full functionality set of Kalavai LLM Pools can be accessed via the command line. This is ideal when working with Virtual Machines in the cloud or in automating workflows where GUI access is not possible or not required.</p> <pre><code>$ kalavai --help\n\nusage: kalavai [-h] command ...\n\npositional arguments:\n  command\n    login     [AUTH] (For public clusters only) Log in to Kalavai server.\n    logout    [AUTH] (For public clusters only) Log out of Kalavai server.\n    gui\n    location\n    pool\n    storage\n    node\n    job\n    ray\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre> <p>For help on a specific command, or group of commands, you can use the --help flag:</p> <pre><code>$ kalavai pool --help\n\nusage: kalavai pool [-h] command ...\n\npositional arguments:\n  command\n    publish      [AUTH] Publish pool to Kalavai platform, where other users may be able to join\n    unpublish    [AUTH] Unpublish pool to Kalavai platform. Cluster and all its workers will still work\n    list         [AUTH] List public pools in to Kalavai platform.\n    start        Start Kalavai pool and start/resume sharing resources.\n    token        Generate a join token for others to connect to your pool\n    check-token  Utility to check the validity of a join token\n    join         Join Kalavai pool and start/resume sharing resources.\n    stop         Stop sharing your device and clean up. DO THIS ONLY IF YOU WANT TO REMOVE KALAVAI-CLIENT from your\n                 device.\n    pause        Pause sharing your device and make your device unavailable for kalavai scheduling.\n    resume       Resume sharing your device and make device available for kalavai scheduling.\n    gpus         Display GPU information from all connected nodes\n    resources    Display information about resources on the pool\n    update       Update kalavai pool\n    status       Run diagnostics on a local installation of kalavai\n    attach       Set creds in token on the local instance\n\noptions:\n  -h, --help     show this help message and exit\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#examples","title":"Examples","text":"","tags":["cli","command line"]},{"location":"cli/#start-a-seed-node-and-get-token","title":"Start a seed node and get token","text":"<pre><code>kalavai pool start &lt;pool-name&gt;\n</code></pre> <p>Now you are ready to add worker nodes to this seed. To do so, generate a joining token:</p> <pre><code>$ kalavai pool token --user\n\nJoin token: &lt;token&gt;\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#add-worker-nodes","title":"Add worker nodes","text":"<pre><code>kalavai pool join &lt;token&gt;\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#attach-more-clients","title":"Attach more clients","text":"<p>You can now connect to an existing pool from any computer -not just from worker nodes. To connect to a pool, run:</p> <pre><code>kalavai pool attach &lt;token&gt;\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#check-resources-in-the-pool","title":"Check resources in the pool","text":"<p>List resources are available:</p> <pre><code>$ kalavai pool resources\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503           \u2503 n_nodes \u2503 cpu   \u2503 memory      \u2503 nvidia.com/gpu \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Available \u2502 4       \u2502 38.08 \u2502 70096719872 \u2502 3              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Total     \u2502 4       \u2502 42    \u2502 70895734784 \u2502 3              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n$ kalavai pool gpus\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Node               \u2503 Ready \u2503 GPU(s)                                               \u2503 Available \u2503 Total \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 carlosfm-desktop-1 \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 2070 (8 GBs)               \u2502 1         \u2502 1     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 carlosfm-desktop-2 \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 3060 (12 GBs)              \u2502 1         \u2502 1     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 pop-os             \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4 GBs) \u2502 1         \u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#deploy-jobs","title":"Deploy jobs","text":"<p>Deploy a job using a template:</p> <pre><code>$ kalavai job run aphrodite --values qwen2.5-1.5B.yaml\n\n[01:42:07] SELECT Target GPUs for the job          \n[KalavaiAuthClient]Logged in as carlosfm\n\n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\n[01:42:40] AVOID Target GPUs for the job\n\n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\n[01:43:13] Template /home/carlosfm/.cache/kalavai/templates/aphrodite/template.yaml successfully deployed!\n[01:43:15] Service deployed   \n</code></pre> <p>List available jobs:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Deployment \u2503 Status                         \u2503 Workers    \u2503 Endpoint                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 qwen-1     \u2502 [2024-11-27T02:17:35Z] Pending \u2502 Pending: 1 \u2502 http://100.10.0.2:30271 \u2502\n\u2502            \u2502                                \u2502 Ready: 1   \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[01:48:23] Check detailed status with kalavai job status &lt;name of deployment&gt; \n           Get logs with kalavai job logs &lt;name of deployment&gt; (note it only works when the deployment is complete) \n</code></pre>","tags":["cli","command line"]},{"location":"compatibility/","title":"Compatibility matrix","text":"<p>If your system is not currently supported, open an issue and request it. We are expanding this list constantly.</p>","tags":["requirements","compatibility"]},{"location":"compatibility/#os-compatibility","title":"OS compatibility","text":"<p>Since worker nodes run inside docker, any machine that can run docker should be compatible with Kalavai. Here are instructions for linux, Windows and MacOS.</p> <p>The kalavai client, which controls and access pools, can be installed on any machine that has python 3.6+.</p> <p>Support for Windows and MacOS workers is experimental: kalavai workers run on docker containers that require access to the host network interfaces, thus systems that do not support containers natively (Windows and MacOS) may have difficulties finding each other.</p> <p>Any system that runs python 3.6+ is able to run the <code>kalavai-client</code> and therefore connect and operate an LLM pool, without sharing with the pool. Your computer won't be adding its capacity to the pool, but it wil be able to deploy jobs and interact with models.</p>","tags":["requirements","compatibility"]},{"location":"compatibility/#hardware-compatibility","title":"Hardware compatibility:","text":"<ul> <li><code>amd64</code> or <code>x86_64</code> CPU architecture</li> <li>NVIDIA GPU</li> <li>AMD and Intel GPUs are currently not supported (yet!)</li> </ul>","tags":["requirements","compatibility"]},{"location":"compatibility/#help-testing-new-systems","title":"Help testing new systems","text":"<p>If you want to help testing Kalavai in new Windows / Linux based systems (thank you!), follow the next steps:</p> <ol> <li> <p>Follow the instructions to install the kalavai client.</p> </li> <li> <p>Save the entire install logs (printed out in the console) to a file (install.log)</p> </li> <li> <p>If the installation went through, run kalavai commands to test the output:</p> </li> </ol> <pre><code>kalavai pool status &gt; status.log\nkalavai pool start test &gt; test_pool.log\nkalavai pool resources &gt; resources.log\n</code></pre> <ol> <li> <p>Create an issue on our repo and share the results. Include the four log files (status.log, test_pool.log, resources.log and install.log) as well as a description of the system you are testing.</p> </li> <li> <p>If the system ends up being supported, you'll be invited to create a PR to add support to the compatibility matrix.</p> </li> </ol>","tags":["requirements","compatibility"]},{"location":"compatibility/#help-testing-amd-gpus","title":"Help testing AMD GPUs","text":"<p>If you are interested in testing AMD support within Kalavai and own an AMD GPU card, please contact us or create an issue!</p>","tags":["requirements","compatibility"]},{"location":"concepts/","title":"Concepts","text":"<p>Work in progress</p>","tags":["concepts","architecture","pool"]},{"location":"concepts/#core-components","title":"Core components","text":"<p>Kalavai turns devices into a scalable LLM platform. It connects multiple machines together and manages the distribution LLM workloads on them.</p> <p>There are three core components:</p> <ul> <li>Kalavai client: python CLI program that lets users create and interact with LLM pools distributed across multiple machines.</li> <li>Seed node: master / server machine that initialises and manages an LLM pool. This is the node where the client runs the start command (<code>kalavai pool start &lt;pool&gt;</code>)</li> <li>Worker node: any machine that joins an LLM pool, where the LLM workload will be deployed to. This are nodes that run the join command (<code>kalavai pool join &lt;token&gt;</code>)</li> </ul> <p>Typically a client will be installed in both the seed and worker nodes, but since v0.5.0, clients can also be installed on external machines. This is useful to be able to connect and send work to your pool from any machine.</p>","tags":["concepts","architecture","pool"]},{"location":"concepts/#how-it-works","title":"How it works?","text":"<p>To create an LLM pool, you need a seed node which acts as a control plane. It handles bookkeeping for the pool. With a seed node, you can generate join tokens, which you can share with other machines --worker nodes.</p> <p>The more worker nodes you have in a pool, the bigger workloads it can run. Note that the only requirement for a fully functioning pool is a single seed node.</p> <p>Once you have a pool running, you can easily deploy workloads using template jobs. These are community integrations that let users deploy LLMs using multiple model engines. A template makes using Kalavai really easy for end users, with a parameterised interface, and it also makes the platform infinitely expandable.</p>","tags":["concepts","architecture","pool"]},{"location":"faqs/","title":"FAQs","text":"<p>Work in progress</p>","tags":["FAQs"]},{"location":"faqs/#general","title":"General","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-ai-pools","title":"What are AI pools?","text":"<p>In Kalavai parlor, a pool refers to a group of resources. We go beyond machine procurement and include everything a team needs to work on AI; from the hardware devices (GPUs, CPUs and memory) to the setup of a distributed environment and the tech stack needed to make it useful.</p> <p>Kalavai aims to manage it all (procurement of additional cloud resources, installing and configuring open source and industry standard frameworks, configuration management, facilitate distributed computing) so teams can focus on AI innovation.</p>","tags":["FAQs"]},{"location":"faqs/#isnt-the-performance-of-distributed-training-much-slower","title":"Isn\u2019t the performance of distributed training much slower?","text":"<p>Distributed computing is not an option: due to the skyrocketing demand in computation from AI models, we are going to need to use multiple devices to do training and inference. </p> <p>NVIDIA cannot get devices larger fast enough, and cloud providers are busy counting the money they are going to make from all that computing to care.</p> <p>Kalavai has considerable tailwinds that will work to minimise the impact of distributed computing in the future:  - Consumer-grade GPU performance per dollar is improving at a faster rate than cloud GPUs - By 2030: Internet broadband speed will reach Gbit/s and 6G will reduce latency &lt; 1 microsecond</p>","tags":["FAQs"]},{"location":"faqs/#host-nodes","title":"Host nodes","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-the-minimum-specs-for-sharing","title":"What are the minimum specs for sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#is-my-device-safe","title":"Is my device safe?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-use-my-device-whilst-sharing","title":"Can I use my device whilst sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-limit-what-i-share-with-kalavai","title":"Can I limit what I share with Kalavai?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-run-the-kalavai-app-within-a-vm","title":"Can I run the kalavai app within a VM?","text":"","tags":["FAQs"]},{"location":"faqs/#why-does-it-require-sudo-privileges","title":"Why does it require sudo privileges?","text":"","tags":["FAQs"]},{"location":"faqs/#developers","title":"Developers","text":"","tags":["FAQs"]},{"location":"faqs/#there-are-plenty-of-mlops-platforms-out-there-why-would-organisations-turn-to-you-instead","title":"There are plenty of MLOps platforms out there, why would organisations turn to you instead?","text":"<p>MLOps solutions out there are great, and they continue to develop. But they all need hardware to run on; whether it is on premise servers, public cloud resources or managed services. </p> <p>We think of MLOps platforms as complementors, that\u2019s why we are building a marketplace for third parties to bring their solutions to our users. Since we manage the computing layer, we abstract away the complexity of integration for them, so they can also bring their tools without having to build multiple integrations.</p>","tags":["FAQs"]},{"location":"faqs/#enterprises","title":"Enterprises","text":"","tags":["FAQs"]},{"location":"faqs/#you-are-leveraging-the-organisations-existing-hardware-but-this-is-unlikely-to-meet-ai-demands-are-we-not-back-to-square-one-since-they-need-to-anyways-go-to-the-cloud","title":"You are leveraging the organisation's existing hardware, but this is unlikely to meet AI demands. Are we not back to square one since they need to anyways go to the cloud?","text":"<p>Our goal is not to narrow companies' choices but to manage the complexity of hybrid clouds. Organisations can bring hardware from anywhere (their own premises, their company devices, all the way to multi cloud on-demand resources) and Kalavai manages them equally. Developers then see a pool of resources that they treat the same.</p>","tags":["FAQs"]},{"location":"faqs/#organisations-with-on-premise-servers-already-have-systems-to-use-them-why-would-they-trust-you-to-manage-that-for-all-their-needs","title":"Organisations with on premise servers already have systems to use them. Why would they trust you to manage that for all their needs?","text":"<p>Kalavai works as an integration system, it does not force organisations to switch every workflow they have over to benefit from it. They can install the kalavai client in their existing on premise servers, which will automatically then connect them to the pool and make them able to run workflows. The kalavai client is designed to co-exist with any application and can be limited to use only a portion of resources, so organisations can easily continue to use their on premise deployments.</p>","tags":["FAQs"]},{"location":"faqs/#ive-heard-of-a-bunch-of-service-providers-for-rent-a-gpu-on-demand-isnt-the-market-saturated-already","title":"I\u2019ve heard of a bunch of service providers for rent-a-GPU on demand. Isn\u2019t the market saturated already?","text":"<p>Kalavai does not have any hardware to lease. We believe there are enough providers out there to cover that. Where there\u2019s a gap is in managing the complexity of use cases that require distributed computing. When workflows require more than one computing device to run, organisations need to manage the orchestration, maintenance and coordination of devices.</p> <p>We have designed Kalavai to integrate nicely with almost any computing resource out there, from public cloud, serverless GPU providers and on premise devices.</p> <p>Got another question?</p>","tags":["FAQs"]},{"location":"getting_started/","title":"Getting started","text":"<p>The <code>kalavai</code> client is the main tool to interact with the Kalavai platform, to create and manage both local and public pools and also to interact with them (e.g. deploy models). Let's go over its installation. </p> <p>From release v0.5.0, you can now install <code>kalavai</code> client in non-worker computers. You can run a pool on a set of machines and have the client on a remote computer from which you access the LLM pool. Because the client only requires having python installed, this means more computers are now supported to run it.</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#requirements-to-run-the-client","title":"Requirements to run the client","text":"<ul> <li>Python 3.6+</li> <li>For seed and workers: Docker engine installed (for linux, Windows and MacOS) with privilege access.</li> </ul>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#install-the-client","title":"Install the client","text":"<p>The client is a python package and can be installed with one command:</p> <pre><code>pip install kalavai-client\n</code></pre>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#createa-a-local-private-llm-pool","title":"Createa a local, private LLM pool","text":"<p>Kalavai is free to use, no caps, for both commercial and non-commercial purposes. All you need to get started is one or more computers that can see each other (i.e. within the same network), and you are good to go. If you are interested in join computers in different locations / networks, contact us or book a demo with the founders.</p> <p>You can create and manage your pools with the new kalavai GUI, which can be started with:</p> <pre><code>$ kalavai gui start\n\n[+] Running 1/1\n \u2714 Container kalavai_gui  Started0.1s  \nLoading GUI, may take a few minutes. It will be available at http://localhost:3000\n</code></pre> <p>This will expose the GUI and the backend services in localhost. By default, the GUI is accessible via http://localhost:3000</p> <p>Note that to use the GUI you will need a free account in the platform. Create one here.</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#1-create-an-llm-pool","title":"1. Create an LLM pool","text":"<p>After you have logged in with your account, you can create your LLM pool by clicking on the <code>circle-plus</code> button. Give the pool a name, and select an IP to use as the pool address. Note that this address will need to be visible by worker machines that want to join in.</p> <p></p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#2-add-worker-nodes","title":"2. Add worker nodes","text":"<p>Important: only nodes within the same network as the seed node (the one that created the pool) can be added successfully. If you are interested in join computers in different locations / networks, contact us or book a demo with the founders.</p> <p>Increase the power of your AI pool by inviting others to join. For that, you need to generate a joining token. Use the navigation panel to go to <code>Devices</code>, and then click the <code>circle-plus</code> button to add new devices. You can select the <code>Access mode</code>, which determine the level of access new nodes will have over the pool: - <code>admin</code>: Same level of access than the seed node, including generating new joining tokens and deleting nodes. - <code>user</code>: Can deploy jobs, but lacks admin access over nodes. - <code>worker</code>: Workers carry on jobs, but cannot deploy their own jobs.</p> <p></p> <p>Copy the joining token and share it with others. On the machines you want to add to the pool, after logging in to kalavai GUI, paste the joining token in the text field under <code>Access with token</code>, and click join</p> <p></p> <p>Kalavai asks you if you want to join (run workloads in the local machine) or attach (use the node to access and control the pool, without running workloads) to the pool. </p> <p></p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#3-explore-resources","title":"3. Explore resources","text":"<p>For both seed and worker nodes, the dashboard shows a high level view of the LLM pool: resources available, current utilisation and active devices and deployments.</p> <p></p> <p>Use the navigation bar to see more details on key resources:</p> <ul> <li>Devices: every machine connected to the pool and its current status</li> </ul> <p></p> <ul> <li>GPUs: list of all available and utilised GPUs</li> </ul> <p></p> <ul> <li>Jobs: all models and deployments active in the pool</li> </ul> <p></p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#4-leave-the-pool","title":"4. Leave the pool","text":"<p>Any device can leave the pool at any point and its workload will get reassigned. To leave the pool, click the <code>circle-stop</code> button on the dashboard, under <code>Local status</code> card. Nodes can rejoin at any point following the above procedure.</p> <p></p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#whats-next","title":"What's next","text":"<p>Now that you know how to get a pool up and running, check our end to end tutorial on how to self-host an LLM Pool, or go full on easy-mode by joining a public pool.</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"local_pool/","title":"Local pool","text":"","tags":["create local pool","bootstrap","seed node"]},{"location":"local_pool/#createa-a-local-pool","title":"Createa a local pool","text":"<p>Kalavai is free to use, no caps, for both commercial and non-commercial purposes. All you need to get started is one or more computers that can see each other (i.e. within the same network), and you are good to go. If you wish to join computers in different locations / networks, check managed kalavai.</p>","tags":["create local pool","bootstrap","seed node"]},{"location":"local_pool/#1-start-a-seed-node","title":"1. Start a seed node","text":"<p>Simply use the CLI to start your seed node:</p> <pre><code>kalavai pool start &lt;pool-name&gt;\n</code></pre> <p>Now you are ready to add worker nodes to this seed. To do so, generate a joining token:</p> <pre><code>$ kalavai pool token\n\nJoin token: &lt;token&gt;\n</code></pre>","tags":["create local pool","bootstrap","seed node"]},{"location":"local_pool/#2-add-worker-nodes","title":"2. Add worker nodes","text":"<p>Increase the power of your AI pool by inviting others to join.</p> <p>Copy the joining token. On the worker node, run:</p> <pre><code>kalavai pool join &lt;token&gt;\n</code></pre>","tags":["create local pool","bootstrap","seed node"]},{"location":"petals/","title":"Public Petals Swarm: BitTorrent-style LLMs","text":"<p>Contribute to the public Petals swarm and help deploy and fine tune Large Language Models across consumer-grade devices. See more about the Petals project here. You'll get:</p> <ul> <li>Eternal kudos from the community!</li> <li>Access to all the models in the server</li> <li>Easy access for inference (via Petals SDK and installation-free Kalavai endpoint).</li> </ul>","tags":["petals","share"]},{"location":"petals/#requirements","title":"Requirements","text":"<ul> <li>A free Kalavai account. Create one here.</li> <li>A computer with the minimum requirements (see below)</li> </ul> <p>Hardware requirements</p> <ul> <li>1+ NVIDIA GPU</li> <li>2+ CPUs</li> <li>4GB+ RAM</li> <li>Free space 4x available VRAM (for an 8GB VRAM GPU, you'll need ~32GB free space in your disk)</li> </ul>","tags":["petals","share"]},{"location":"petals/#how-to-join","title":"How to join","text":"<ol> <li> <p>Create a free account with Kalavai.</p> </li> <li> <p>Install the kalavai client following the instructions here. Currently we support Linux distros and Windows.</p> </li> <li> <p>Get the joining token. Visit our platform and go to <code>Community pools</code>. Then click <code>Join</code> on the <code>Petals</code> Pool to reveal the joining details. Copy the command (including the token).</p> </li> </ol> <p></p> <ol> <li>Authenticate the computer you want to use as worker:</li> </ol> <pre><code>$ kalavai login\n\n[10:33:16] Kalavai account details. If you don't have an account, create one at https://platform.kalavai.net                                                                 \nUser email: &lt;your email&gt;\nPassword: &lt;your password&gt;\n\n[10:33:25] &lt;email&gt; logged in successfully\n</code></pre> <ol> <li>Join the pool with the following command:</li> </ol> <pre><code>$ kalavai pool join &lt;token&gt;\n\n[16:28:14] Token format is correct\n           Joining private network\n\n[16:28:24] Scanning for valid IPs...\n           Using 100.10.0.8 address for worker\n           Connecting to PETALS @ 100.10.0.9 (this may take a few minutes)...\n[16:29:41] Worskpace created\n           You are connected to PETALS\n</code></pre>","tags":["petals","share"]},{"location":"petals/#check-petals-health","title":"Check Petals health","text":"<p>Kalavai's pool connects directly to the public swarm on Petals, which means we can use their public health check UI to see how much we are contributing and what models are ready to use.</p> <p></p> <p>Models with at least one copy of each shard (a green dot in each column) are ready to be used. If not, wait for more workers to join in.</p> <p>Using the kalavai client you can monitor the state of the pool and all of the connected nodes:</p> <pre><code>$ kalavai pool status\n\n# Displays the status of the pool\n\n$ kalavai node list\n\n# Displays the list of connected nodes, and their current status\n</code></pre> <p>The command <code>kalavai node list</code> is useful to see if our node has any issues and whether it's currently online.</p>","tags":["petals","share"]},{"location":"petals/#how-to-use-the-models","title":"How to use the models","text":"<p>For all public swarms you can use the Petals SDK in the usual way. Here is an example:</p> <pre><code>from transformers import AutoTokenizer\nfrom petals import AutoDistributedModelForCausalLM\n\n# Choose any model available at https://health.petals.dev\nmodel_name = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n\n# Connect to a distributed network hosting model layers\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n\n# Run the model as if it were on your computer\ninputs = tokenizer(\"A cat sat\", return_tensors=\"pt\")[\"input_ids\"]\noutputs = model.generate(inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))  # A cat sat on a mat...\n</code></pre> <p>This path is great if you are a dev with python installed, and don't mind installing the Petals SDK. If you want an install-free path, Kalavai deploys a single endpoint for models, which allows you to do inference via gRPC and HTTP requests. Substitute KALAVAI_ENDPOINT with the endpoint displayed under the <code>Community Pools</code> page. Here is a request example:</p> <pre><code>\"\"\"\nMore info: https://github.com/petals-infra/chat.petals.dev\n\nRequired: pip install websockets\n\"\"\"\nimport time\nimport json\nimport websockets\nimport asyncio\n\n\nKALAVAI_ENDPOINT = \"192.168.68.67:31220\" # &lt;-- change for the kalavai endpoint\nMODEL_NAME = \"mistralai/Mixtral-8x22B-Instruct-v0.1\" # &lt;-- change for the models available in Kalavai PETALS pool.\n\n\nasync def ws_generate(text, max_length=100, temperature=0.1):\n    async with websockets.connect(f\"ws://{KALAVAI_ENDPOINT}/api/v2/generate\") as websocket:\n        try:\n            await websocket.send(\n                json.dumps({\"model\": MODEL_NAME, \"type\": \"open_inference_session\", \"max_length\": max_length})\n            )\n            response = await websocket.recv()\n            result = json.loads(response)\n\n            if result[\"ok\"]:\n                await websocket.send(\n                    json.dumps({\n                        \"type\": \"generate\",\n                        \"model\": MODEL_NAME,\n                        \"inputs\": text,\n                        \"max_length\": max_length,\n                        \"temperature\": temperature\n                    })\n                )\n                response = await websocket.recv()\n                return json.loads(response)\n            else:\n                return response\n        except Exception as e:\n            return {\"error\": str(e)}\n\n\nif __name__ == \"__main__\":\n    t = time.time()\n    output = asyncio.get_event_loop().run_until_complete(\n        ws_generate(text=\"Tell me a story: \")\n    )\n    final_time = time.time() - t\n    print(f\"[{final_time:.2f} secs]\", output)\n    print(f\"{output['token_count'] / final_time:.2f}\", \"tokens/s\")\n</code></pre> <p>NOTE: the endpoints are only available within worker nodes, not from any other computer.</p>","tags":["petals","share"]},{"location":"petals/#stop-sharing","title":"Stop sharing","text":"<p>You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). </p> <p>To pause sharing (but remain on the pool), run the following command:</p> <pre><code>kalavai pool pause\n</code></pre> <p>When you are ready to resume sharing, run:</p> <pre><code>kalavai pool resume\n</code></pre> <p>To stop and leave the pool, run the following:</p> <pre><code>kalavai pool stop\n</code></pre>","tags":["petals","share"]},{"location":"petals/#faqs","title":"FAQs","text":"","tags":["petals","share"]},{"location":"petals/#something-isnt-right","title":"Something isn't right","text":"<p>Growing pains! Please report any issues in our github repository.</p>","tags":["petals","share"]},{"location":"petals/#can-i-join-and-leave-whenever-i-want","title":"Can I join (and leave) whenever I want?","text":"<p>Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here.</p>","tags":["petals","share"]},{"location":"petals/#what-is-in-it-for-me","title":"What is in it for me?","text":"<p>If you decide to share your compute with the community, not only you'll get access to all the models we deploy in it, but you will also gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon).</p>","tags":["petals","share"]},{"location":"petals/#is-my-data-secured-private","title":"Is my data secured / private?","text":"<p>The public pool in Kalavai has the same level of privacy and security than the general Petals public swarm. See their privacy details here. In the future we will improve support for private swarms; at the moment private swarms are a beta feature for all kalavai pools that can be used via the petals template.</p>","tags":["petals","share"]},{"location":"petals/#is-my-gpu-constantly-being-used","title":"Is my GPU constantly being used?","text":"<p>Yes and no. The model weights for the shard you are responsible for are loaded in GPU memory for as long as your machine is sharing. However, this does not mean the GPU is active (doing computing) constantly; computation (and hence the vast majority of energy comsumption) only happens when your shard is summoned to process inference requests.</p> <p>If at any point you need your GPU memory back, pause or stop sharing and come back when you are free.</p>","tags":["petals","share"]},{"location":"public_llm_pool/","title":"LLM Pools: deploy and orchestrate Large Language Models","text":"<p>\u2b50\u2b50\u2b50 Kalavai and our LLM pools are open source and free to use in both commercial and non-commercial purposes. If you find it useful, consider supporting us by giving a star to our GitHub project, joining our discord channel, follow our Substack and give us a review on Product Hunt.</p> <p>Beta feature: we are trialing shared pools. If you encounter any issues, please submit a ticket in our GitHub repo.</p> <p>\ud83d\udd25\ud83d\udd25\ud83d\udd25 We have deployed <code>DeepSeek R1</code> onto our public LLM pool. Join in to access the most demanded open source model for free. The more people that joins, the bigger models we can deploy</p> <p>LLM pools in <code>Kalavai</code> are an easy way to expand your computing power beyond a single machine, with zero-devops knowledge. Kalavai aggregates the GPUs, CPUs and RAM memory from any compatible machine and makes it ready for LLM workflows. All you need is three steps to get your supercomputing cluster going:</p> <ol> <li>Start a pool with the kalavai client</li> <li>Use the joining token to connect other machines to the pool</li> <li>Deploy LLMs with ready-made templates!</li> </ol> <p>In this guide, we'll show how to join our first public LLM pool, ideal for AI developers that want to go beyond the hardware they have access to. This will manage step 1, so you can jump ahead to step 2 and 3. If you are interested in hosting your own private shared pool, check out this one.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Install the <code>kalavai</code> client in your computer. Make sure it is version v0.5.12 or above.</li> </ul> <pre><code>$ pip show kalavai-client\n\nName: kalavai-client\nVersion: 0.5.12\nSummary: Client app for kalavai platform\n...\n</code></pre> <ul> <li>Create a free account on our platform.</li> <li>For public pools, only linux systems are currently supported. You can also use Windows under WSL (Windows Linux Subsystem). This is a temporary limitation of the public VPN we use.</li> </ul>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#join-in","title":"Join in","text":"<p>Authenticate your computer using the kalavai client:</p> <pre><code>$ kalavai login\n\nKalavai account details. If you don't have an account, create one at https://platform.kalavai.net                                                                  \nUser email: &lt;your.account@email.com&gt;\nPassword: \n[10:56:05] &lt;your.account@email.com&gt; logged in successfully  \n</code></pre> <p>Get the joining token from our platform, under Community Pools page. Find the <code>Public-LLMs</code> pool and click JOIN to reveal the joining token. Copy and paste the command on your computer:</p> <pre><code>kalavai pool join &lt;TOKEN&gt;\n</code></pre> <p>That's it! Not only you are sharing your computing time with the community, but now you can tap into a large pool of resources (GPUs, CPUs, RAM...), and any LLM deployed on them.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#what-can-you-do-in-the-pool","title":"What can you do in the pool?","text":"<p>Public shared pools are both public (anyone can join in) and shared (raw resources and deployments are accessible by all). This means any user that is part of the pool can check out the models already present in it, run inference on them, an deploy new ones with the resources available.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#a-use-existing-models","title":"A. Use existing models","text":"<p>All users can interact with models within the pool in two ways:</p> <ol> <li>Single API endpoint for HTTP requests</li> <li>Unified ChatGPT-like UI playground</li> </ol>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#ui-playground","title":"UI Playground","text":"<p>The pool comes with an OpenWebUI deployment to make it easy to test model inference with LLMs via the browser. To check the endpoint, use the kalavai client to locate the playground deployment:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Owner   \u2503 Deployment \u2503 Workers  \u2503 Endpoint                                 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 playground \u2502 Ready: 1 \u2502 http://100.10.0.5:31912 (mapped to 8080) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Within the UI you can select the model you wish to test and have a chat.</p> <p></p> <p>Note: the playground is a shared instance to help users test models without code and should not be used in production. You need to create a playground account to access it. This can be different to your Kalavai account details. The creation of a new user is necessary to keep things like user chat history and preferences.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#single-api-endpoint","title":"Single API endpoint","text":"<p>All interactions to models in the pool are brokered by a LiteLLM endpoint that is installed in the system. To interact with it you need a LITELLM_URL and a LITELLM_KEY.</p> <p>The <code>LITELLM_URL</code> can be found using the kalavai client, as the endpoint of the <code>litellm-1</code> deployment:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Owner   \u2503 Deployment \u2503 Workers  \u2503 Endpoint                                 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 litellm    \u2502 Ready: 3 \u2502 http://100.10.0.5:30916 (mapped to 4000) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The <code>LITELLM_KEY</code> is shown on the Community Pool page of our platform.</p> <p></p> <p>In this example:</p> <ul> <li><code>LITELLM_URL=http://100.10.0.5:30916</code></li> <li><code>LITELLM_KEY=sk-qoQC5lijoaBwXoyi_YP1xA</code></li> </ul>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#check-available-llms","title":"Check available LLMs","text":"<p>Using cURL:</p> <pre><code>curl -X GET \"&lt;LITELLM_URL&gt;/v1/models\" \\\n  -H 'Authorization: Bearer &lt;LITELLM_KEY&gt;' \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\"\n</code></pre> <p>Using python:</p> <pre><code>import requests\n\nLITELLM_URL = \"http://100.10.0.5:30916\"\nLITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\"\n\n\ndef list_models():\n    response = requests.get(\n        f\"{LITELLM_URL}/v1/models\",\n        headers={\"Authorization\": f\"Bearer {LITELLM_KEY}\"}\n    )\n    return response.json()\n\n\nif __name__ == \"__main__\":\n    print(\n        list_models()\n    )\n</code></pre>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#use-models","title":"Use models","text":"<p>Using cURL:</p> <pre><code>curl --location '&lt;LITELLM_URL&gt;/chat/completions' \\\n  --header 'Authorization: Bearer &lt;LITELLM_KEY&gt;' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n      \"model\": \"&lt;MODEL_NAME&gt;\",\n      \"messages\": [\n          {\n          \"role\": \"user\",\n          \"content\": \"what llm are you\"\n          }\n      ]\n  }'\n</code></pre> <p>Using python:</p> <pre><code>import requests\n\nLITELLM_URL = \"http://206.189.19.245:30916\"\nLITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\"\n\ndef model_inference():\n    response = requests.post(\n        f\"{LITELLM_URL}/chat/completions\",\n        headers={\"Authorization\": f\"Bearer {LITELLM_KEY}\"},\n        json={\n            \"model\": \"&lt;MODEL_NAME&gt;\",\n            \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"what llm are you\"\n            }]\n        }\n    )\n    return response.json()\n\n\nif __name__ == \"__main__\":\n    print(\n        model_inference()\n    )\n</code></pre> <p>For more details on the endpoint(s) parameters, check out LiteLLM documentation and the Swagger API</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#b-deploy-new-models","title":"B. Deploy new models","text":"<p>Kalavai makes deployment of new models easy with the use of templates (no-code recipes for model engines). At the moment we support the following engines:</p> <ul> <li>vLLM</li> <li>llama.cpp</li> <li>Aphrodite Engine</li> </ul> <p>Kalavai supports any of the models each of the engines does. For more information, check out vLLM, llama.cpp and Aphrodite Engine support lists. </p> <p>We are constantly adding new templates, so if your favourite one is not yet available, request it in our issues page.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#new-vllm-model","title":"New vLLM model","text":"<p>Here we'll deploy an LLM across 2 machines using vLLM. Check our template documentation or our multi-node deployment guide for more details and parameters with vLLM. Deploying with llama.cpp is similar too.</p> <p>You need the LiteLLM KEY of the pool. See here for details on how to get it.</p> <p>Create a <code>values.yaml</code> file that will include the parameters to pass to the vLLM engine:</p> <pre><code>- name: litellm_key\n  value: \"&lt;LITELLM_KEY&gt;\"\n  default: \"\"\n  description: \"Master key of the LiteLLM service (central registry)\"\n\n- name: workers\n  value: 2\n  default: 1\n  description: \"Number of remote workers (for tensor and pipeline parallelism). This is in addition to the main node\"\n\n- name: model_id\n  value: Qwen/Qwen2.5-1.5B-Instruct\n  default: null\n  description: \"Huggingface model id to load\"\n\n- name: pipeline_parallel_size\n  value: 2\n  default: 1\n  description: \"Pipeline parallelism (use the number of nodes)\"\n</code></pre> <p>Use the kalavai client to deploy your model. Choose option <code>0</code> to let kalavai select any available GPU device in the pool:</p> <pre><code>$ kalavai job run vllm --values values.yaml\n\nChecking current GPU stock...\n\nSELECT Target GPUs for the job (loading models)   \n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\nAVOID Target GPUs for the job (loading models) \n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\nTemplate templates/vllm/template.yaml successfully deployed!  \n</code></pre> <p>Check progress of your deployment:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Owner   \u2503 Deployment                 \u2503 Workers    \u2503 Endpoint                                 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 litellm-1                  \u2502 Ready: 3   \u2502 http://100.10.0.5:30916 (mapped to 4000) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 &lt;user&gt;  \u2502 qwen-qwen2-5-1-5b-instruct \u2502 Ready: 1   \u2502                                          \u2502\n\u2502         \u2502                            \u2502 Pending: 2 \u2502                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Your model will appear listed under your user () ownership. Note once all workers are listed as Ready, the model still needs to be loaded onto the machines and thus it may take some time for it to be available through the LiteLLM API. You can check the workers progress: <pre><code>kalavai job logs qwen-qwen2-5-1-5b-instruct\n</code></pre> <p>Note: models may take several minutes to be ready, particularly if the model weights are large. This is due to the time the system takes to 1) download the models from source and 2) distribute them to the memory of each device. This is an overhead that only happens once per deployment.</p> <p>Once the model is loaded, you can interact with it as you would with any other model via the LiteLLM API.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#whats-next","title":"What's next?","text":"<p>Enjoy your new supercomputer, check out our templates and examples for more model engines and keep us posted on what you achieve!</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#how-many-resources-to-request","title":"How many resources to request?","text":"<p>For this job example we don't need to tweak resource parameters (the model fits in a single worker), but for other models you may need to up the number of workers, desired RAM, etc. For more details on how to choose resources for jobs, check out this guide.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#delete-deployment","title":"Delete deployment","text":"<p>You are welcome to leave the model running for others, but if you wish to remove your deployment, you can do so at any time with the kalavai client:</p> <pre><code>kalavai job delete &lt;deployment name&gt;\n</code></pre> <p>where the  is the name listed under <code>kalavai job list</code>.","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#faqs","title":"FAQs","text":"<p>Work in Progress!</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_llm_pool/#why-is-it-free","title":"Why is it free?","text":"<p>We thought the community may be interested in a public version, so we have created a public shared pool that anyone can join. It is truly public (anyone can join) and shared (resources are pooled together, and anyone can see and use each other's LLMs).</p> <p>We are committed to advancing community computing, and that's why not only we are showing how anyone can create their own pool with their devices, but access to the public instance is free.</p>","tags":["crowdsource","public","LLM pool","shared llm"]},{"location":"public_pool/","title":"Public pools: crowdsource community resources","text":"<p>Coming soon!</p> <p>Our public platform expands local pools in two key aspects: - Worker nodes no longer have to be in the same local network - Users can tap into community resources: inspire others in the community to join their projects with their resources</p> <p>To get started, you need is a free account on our platform.</p>","tags":["crowdsource","public pool"]},{"location":"public_pool/#a-tap-into-community-resources","title":"A) Tap into community resources","text":"<p>Create a new pool, using a public location provided by Kalavai:</p> <pre><code># Authenticate with your kalavai account\nkalavai login\n\n# Get available public locations\nkalavai location list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513  \n\u2503 VPN \u2503 location    \u2503 subnet        \u2503          \n\u2521\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0   \u2502 uk_london_1 \u2502 100.10.0.0/16 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Create and publish your pool\nkalavai pool start &lt;pool-name&gt; --location uk_london_1\n</code></pre> <p>If all goes well, your pool will be created and published on the <code>Public Seeds</code> section of our platform</p> <p></p> <p>Note: to be able to publish pools your account needs to have sufficient karma points. Earn karma by sharing your resources with others.</p>","tags":["crowdsource","public pool"]},{"location":"public_pool/#b-share-resources-with-inspiring-community-projects","title":"B) Share resources with inspiring community projects","text":"<p>Have idle computing resources? Wish to be part of exciting public projects? Want to give back to the community? Earn social credit (both literally and metaphorically) by sharing your computer with others within the community.</p> <p>All you need is a public joining key. Get them in our platform, on the list of published pools. Press <code>Join</code> and follow the instructions</p> <p></p>","tags":["crowdsource","public pool"]},{"location":"ray/","title":"Ray","text":"<p>Work in progress</p>","tags":["ray"]},{"location":"ray/#ray-clusters-for-distributed-computing","title":"Ray clusters for distributed computing","text":"<p>From Ray's documentation:</p> <p>Ray is an open-source unified framework for scaling AI and Python applications like machine learning.</p> <p>Kalavai and Ray work perfectly together. Ray is a great framework to deal with distributed computation on top of an existing hardware pool. Kalavai acts as a unifying layer that brings that required hardware together for Ray to do its magic.</p> <p>To get started, check out our example to get a Ray cluster going. </p>","tags":["ray"]},{"location":"ray/#create-a-cluster","title":"Create a cluster","text":"<ul> <li>Specs how to define specs: kalavai pool resources (cpu, memory and nvidia.com/gpu)</li> </ul> <pre><code>$ kalavai pool resources\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \n\u2503           \u2503 n_nodes \u2503 cpu                \u2503 memory      \u2503 nvidia.com/gpu \u2503 \n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \n\u2502 Available \u2502 2       \u2502 10.684999999999999 \u2502 16537780224 \u2502 1              \u2502 \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \n\u2502 Total     \u2502 4       \u2502 42                 \u2502 70895030272 \u2502 3              \u2502 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n</code></pre> <pre><code>spec:\n  ...\n  headGroupSpec:\n    ...\n    template:\n      spec:\n        ...\n        containers:\n        ...\n          resources:\n            limits:\n              cpu: 2\n              memory: 4Gi\n            requests:\n              cpu: 2\n              memory: 4Gi\n  workerGroupSpecs:\n  ...\n    template:\n      spec:\n        containers:\n        ...\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n              cpu: 2\n              memory: 4Gi\n            requests:\n              nvidia.com/gpu: 1\n              cpu: 2\n              memory: 4Gi\n</code></pre> <p>Interact with Ray - Interactive mode - Endpoint - RayJobs</p>","tags":["ray"]},{"location":"ray/#advanced-topics","title":"Advanced topics","text":"<p>Autoscaling</p> <p>Node hardware requirements (limits vs requests)</p>","tags":["ray"]},{"location":"self_hosted_llm_pool/","title":"Self-hosted LLM pools","text":"<p>\u2b50\u2b50\u2b50 Kalavai and our LLM pools are open source and free to use in both commercial and non-commercial purposes. If you find it useful, consider supporting us by giving a star to our GitHub project, joining our discord channel, follow our Substack and give us a review on Product Hunt.</p> <p>Ideal for AI teams that want to supercharge their resources without opening it to the public.</p> <p>For easy-mode LLM pools, check out our Public LLM pool, which comes with managed LiteLLM API and OpenWebUI playground for all models.</p> <p>This guide will show you how to start a self-hosted LLM pool with your own hardware, configure it with a single API and UI Playground for all your models and deploy and access a Llama 3.1 8B instance.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#what-youll-achieve","title":"What you'll achieve","text":"<ol> <li>Configure unified LLM interface</li> <li>Deploy a llamacpp model</li> <li>Access model via code and UI</li> </ol>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#1-pre-requisites","title":"1. Pre-requisites","text":"<ul> <li>Install kalavai CLI on each machine</li> <li>Set up a 2 machine LLM pool, i.e. a seed node and one worker</li> </ul> <p>Note: the following commands can be executed on any machine that is part of the pool, provided you have used <code>admin</code> or <code>user</code> access modes to generate the token. If you have used <code>worker</code>, deployments are only allowed in the seed node.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#2-configure-unified-llm-interface","title":"2. Configure unified LLM interface","text":"<p>This is an optional but highly recommended step that will help automatically register any model deployment centrally, so you can interact with any model through a single OpenAI-like API endpoint, or if you prefer UI testing, a single ChatGPT-like UI playground.</p> <p>We'll use our own template jobs for the task, so no code is required. Both jobs will require a permanent storage, which can be created easily in an LLM pool using <code>kalavai storage create &lt;db name&gt; &lt;size in GB&gt;</code>. Using the <code>kalavai</code> client, create two storage spaces:</p> <pre><code>$ kalavai storage create litellm-db 1\n\nStorage litellm-db (1Gi) created\n\n$ kalavai storage create webui-db 2\n\nStorage webui-db (2Gi) created\n</code></pre>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#unified-openai-like-api","title":"Unified OpenAI-like API","text":"<p>Model templates deployed in LLM pools have an optional key parameter to register themselves with a LiteLLM instance. LiteLLM is a powerful API that unifies all of your models into a single API, making developing apps with LLMs easier and more flexible.</p> <p>Our LiteLLM template automates the deployment of the API across a pool, database included. To deploy it using the Kalavai GUI, navigate to <code>Jobs</code>, then click on the <code>circle-plus</code> button, in which you can select a <code>litellm</code> template. Set the values of <code>db_storage</code> to <code>litellm-db</code> (or the one you used above).</p> <p></p> <p>Once the deployment is complete, you can check the LiteLLM endpoint by navigating to <code>Jobs</code> and seeing the corresponding endpoint for the <code>litellm</code> job.</p> <p></p> <p>You will need a virtual key to register models with LiteLLM. For testing you can use the master key defined in your values.yaml under <code>master_key</code>, but it is recommended to generate a virtual one that does not have privilege access. The easiest way of doing so is via the admin UI, under http://192.168.68.67:30535/ui (see more details here).</p> <pre><code>Example virtual key: sk-rDCm0Vd5hDOigaNbQSSsEQ\n</code></pre> <p></p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#unified-ui-playground","title":"Unified UI Playground","text":"<p>OpenWebUI is a great ChatGPT-like app that helps testing LLMs. Our WebUI template manages the deployment of an OpenWebUI instance in your LLM pool, and links it to your LiteLLM instance, so any models deployed and registered with LiteLLM automatically appear in the playground.</p> <p>To deploy, navigate back to <code>Jobs</code> and click the <code>circle-plus</code> button, this time selecting the playground template. Set the <code>litellm_key</code> to match your virtual key, and <code>data_storage</code> to <code>webui-db</code> (or the one created above).</p> <p>Once it's ready, you can access the UI via its advertised endpoint (under <code>Jobs</code>), directly on your browser. The first time you login you'll be able to create an admin user. Check the official documentation for more details on the app.</p> <p></p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#check-deployment-progress","title":"Check deployment progress","text":"<p>Jobs may take a while to deploy. Check the progress with:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Owner   \u2503 Deployment \u2503 Workers    \u2503 Endpoint                                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 litellm    \u2502 Ready: 2   \u2502 http://192.168.68.67:30535 (mapped to 4000) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 default \u2502 webui-1    \u2502 Pending: 1 \u2502 http://192.168.68.67:31141 (mapped to 8080) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this case, <code>litellm</code> has been deployed but <code>webui-1</code> is still pending schedule. If a job cannot be scheduled due to lack of resources, consider adding more nodes or reducing the requested resources via the <code>values.yaml</code> files.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#3-deploy-models-with-compatible-frameworks","title":"3. Deploy models with compatible frameworks","text":"<p>Using a self-hosted LLM pool is the same as using a public one, with the only difference being access. Your self-hosted LLM pool is private and only those you give a joining token access to can see and use it. </p> <p>To deploy a multi-node, multi-GPU vLLM model, check out the section under Public LLM pools.</p> <p>In this section, we'll look into how to deploy a model with another of our supported model engines: llama.cpp</p> <p>We provide an example of template values to deploy Llama 3.1 8B model. Copy its content in your machine into a <code>values.yaml</code> file. Feel free to modify its values. If you use the default values, the deployment will use the following parameters:</p> <ul> <li><code>litellm_key</code>: set it to your virtual key to automatically register it with both LiteLLM and OpenWebUI instances.</li> <li><code>cpu_workers</code>: the workload will be split across this many workers. Note that a worker is not necessarily a single node, but a set of <code>cpus</code> and <code>memory</code> RAM (if a node has enough memory and cpus, it will accommodate multiple workers).</li> <li><code>repo_id</code>: huggingface model id to deploy</li> <li><code>model_filename</code>: for gguf models, often repositories have multiple quantized versions. This parameter indicates the name of the file / version you wish to deploy.</li> </ul> <p>When you are ready, deploy:</p> <pre><code>$ kalavai job run llamacpp --values values.yaml\n\nTemplate /home/carlosfm/.cache/kalavai/templates/llamacpp/template.yaml successfully deployed!                                                                      \nService deployed\n</code></pre> <p>Once it has been scheduled, check the progress with:</p> <pre><code>kalavai job logs meta-llama-3-1-8b-instruct-q4-k-m-gguf\n\nPod meta-llama-3-1-8b-instruct-q4-k-m-gguf-cpu-0                                 cli.py:1640\n\n           -- The C compiler identification is GNU 12.2.0                                   cli.py:1641\n           -- The CXX compiler identification is GNU 12.2.0                                            \n           -- Detecting C compiler ABI info                                                            \n           -- Detecting C compiler ABI info - done                                                     \n           -- Check for working C compiler: /usr/bin/cc - skipped                                      \n\n           ...                                            \n\n           Pod meta-llama-3-1-8b-instruct-q4-k-m-gguf-cpu-1                                 cli.py:1640\n           -- The C compiler identification is GNU 12.2.0                                   cli.py:1641\n           -- The CXX compiler identification is GNU 12.2.0                                            \n           -- Detecting C compiler ABI info                                                            \n           -- Detecting C compiler ABI info - done                                                     \n           -- Check for working C compiler: /usr/bin/cc - skipped                                      \n           ...                                                      \n\n           Pod meta-llama-3-1-8b-instruct-q4-k-m-gguf-cpu-2                                 cli.py:1640\n           -- The C compiler identification is GNU 12.2.0                                   cli.py:1641\n           -- The CXX compiler identification is GNU 12.2.0                                            \n           -- Detecting C compiler ABI info                                                            \n           -- Detecting C compiler ABI info - done                                                     \n           -- Check for working C compiler: /usr/bin/cc - skipped                                      \n           ...                                                     \n\n           Pod meta-llama-3-1-8b-instruct-q4-k-m-gguf-registrar-0                           cli.py:1640\n           Waiting for model service...                                                     cli.py:1641\n           Waiting for                                                                                 \n           meta-llama-3-1-8b-instruct-q4-k-m-gguf-server-0.meta-llama-3-1-8b-instruct-q4-k-            \n           m-gguf:8080...                                                                              \n           ...Not ready, backoff                                                                       \n           ...Not ready, backoff                                                                       \n\n           Pod meta-llama-3-1-8b-instruct-q4-k-m-gguf-server-0                              cli.py:1640\n           Collecting llama-cpp-python==0.3.2                                               cli.py:1641\n             Downloading llama_cpp_python-0.3.2.tar.gz (65.0 MB)                                       \n                \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 65.0/65.0 MB 24.0 MB/s eta 0:00:00            \n             Installing build dependencies: started                                                    \n             ...\n</code></pre> <p>The logs include individual logs for each worker.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#4-access-your-models","title":"4. Access your models","text":"<p>Once they are donwloaded and loaded into memory, your models will be readily available both via the LiteLLM API as well as through the UI Playground. Check out our full guide on how to access deployed models via API and the playground.</p> <p></p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#5-clean-up","title":"5. Clean up","text":"<p>Remove models:</p> <pre><code>kalavai job delete &lt;name of the model&gt;\n</code></pre> <p>You can identify the name of the model by listing them with:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Owner   \u2503 Deployment                           \u2503 Workers    \u2503 Endpoint                              \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 litellm                              \u2502 Ready: 2   \u2502 http://192.168.68.67:30535 (mapped to \u2502\n\u2502         \u2502                                      \u2502            \u2502 4000)                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 default \u2502 meta-llama-3-1-8b-instruct-q4-k-m-gg \u2502 Pending: 5 \u2502 http://192.168.68.67:31645 (mapped to \u2502\n\u2502         \u2502 uf                                   \u2502            \u2502 8080)                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 default \u2502 webui-1                              \u2502 Ready: 1   \u2502 http://192.168.68.67:31141 (mapped to \u2502\n\u2502         \u2502                                      \u2502            \u2502 8080)                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Disconnect a worker node and remove the pool:</p> <pre><code># from a worker node\nkalavai pool stop\n\n# from the seed node\nkalavai pool stop\n</code></pre>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#6-whats-next","title":"6. What's next?","text":"<p>Enjoy your new supercomputer, check out our templates and examples for more model engines and keep us posted on what you achieve!</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"templates/","title":"Develop with Kalavai","text":"<p>Work in progress</p> <p>Template jobs built by Kalavai and the community make deploying distributed LLMs easy for end users.</p> <p>Templates are like recipes, where developers describe what worker nodes should run, and users customise the behaviour via pre-defined parameters. Kalavai handles the heavy lifting: workload distribution, communication between nodes and monitoring the state of the deployment to restart the job if required.</p> <p>Using the client, you can list what templates your LLM pool supports:</p> <pre><code>$ kalavai job templates\n\n[10:51:29] Templates available in the pool\n           ['vllm', 'aphrodite', 'llamacpp', 'petals', 'litellm', 'playground', 'boinc', 'gpustack']\n</code></pre> <p>Deploying a template is easy:</p> <pre><code>kalavai job run &lt;template name&gt; --values &lt;template values&gt;\n</code></pre> <p>Where <code>&lt;template name&gt;</code> refers to one of the supported templates above, and <code>&lt;template values&gt;</code> is a local yaml file containing the parameters of the job. See examples for more information.</p>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"templates/#list-of-available-templates","title":"List of available templates","text":"<ul> <li>vLLM: deploy your favourite LLMs in distributed machines.</li> <li>llama.cpp: deploy llama.cpp models (CPU and GPU) in distributed machines.</li> <li>Aphrodite: deploy your favourite LLMs in distributed machines.</li> <li>Petals: bit-torrent style deployment of LLMs</li> <li>litellm: Unified LLM API.</li> <li>playground: Unified UI playground.</li> </ul>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"templates/#how-to-contribute","title":"How to contribute","text":"<p>Do you want to develop your own template and share it with the community? We are working on a path to make it easier for developers to do so. Hang on tight! But for now, head over to our repository and check the examples in there.</p>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"templates/#why-is-insert-preferred-application-not-supported","title":"Why is [insert preferred application] not supported?","text":"<p>If your preferred distributed ML application is not yet supported, let us know! Or better yet, add it and contribute to community integrations.</p>","tags":["integrations","jobs","LLM engines","ray"]}]}