{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Kalavai","text":"<p>Kalavai is an open source platform that turns any device into a self-hosted AI platform. </p>"},{"location":"#how-kalavai-can-help-you","title":"How Kalavai can help you","text":"<ul> <li>We help you aggregate resources from multiple machines: home desktops, work computers, cloud VMs, raspberry pi's, Mac, etc.</li> <li>Our platform has ready-made templates for users to deploy common AI building blocks: model inference (vLLM, llama.cpp, SGLang and more), automation workflows (n8n and Flowise), evaluation and monitoring tools (Langfuse), production tools (LiteLLM, OpenWebUI)</li> <li>Support for multimodal GenAI: Text-only, Text-to-Speech, Speech-to-Text, Image generation, Image / Video understanding.</li> <li>Allocation of resources is managed automatically. If your deployment requires multiple machines, we handle the workload distribution.</li> </ul>"},{"location":"#want-to-be-notified-of-the-latest-features","title":"Want to be notified of the latest features?","text":"<p>Subscribe to our substack channel, where we regularly publish news, articles and updates.</p> <p>Join our discord community</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>The best way to getting started is to check out our start guide for a step-by-step guide on how to get started. This is the recommended entry point for those that wish to explore the basics of Kalavai.</p>"},{"location":"boinc/","title":"BOINC: Volunteer scientific computing","text":"<p>BOINC is an open source platform for volunteer computing, organised in scientific projects.</p> <p>Kalavai makes it easy to share your computing resources with Science United, a coordinated model for scientific computing where volunteers share their machines with a multitude of projects.</p> <p>What you get by sharing:</p> <ul> <li>Eternal kudos from the community!</li> <li>Kalavai credits that can be used in any of our public pools</li> </ul>","tags":["boinc","volunteer computing"]},{"location":"boinc/#requirements","title":"Requirements","text":"<ul> <li>A free Kalavai account. Create one here.</li> <li>A computer with the minimum requirements (see below)</li> </ul> <p>Hardware requirements</p> <ul> <li>4+ CPUs</li> <li>4GB+ RAM</li> <li>(optional) 1+ NVIDIA GPU</li> </ul>","tags":["boinc","volunteer computing"]},{"location":"boinc/#how-to-join","title":"How to join","text":"<ol> <li> <p>Create a free account with Kalavai.</p> </li> <li> <p>Install the kalavai client following the instructions here. Currently we support Linux distros and Windows.</p> </li> <li> <p>Get the joining token. Visit our platform and go to <code>Community pools</code>. Then click <code>Join</code> on the <code>BOINC</code> Pool to reveal the joining details. Copy the command (including the token).</p> </li> </ol> <p></p> <ol> <li>Authenticate the computer you want to use as worker:</li> </ol> <pre><code>$ kalavai login\n\n[10:33:16] Kalavai account details. If you don't have an account, create one at https://platform.kalavai.net                                                                 \nUser email: &lt;your email&gt;\nPassword: &lt;your password&gt;\n\n[10:33:25] &lt;email&gt; logged in successfully\n</code></pre> <ol> <li>Join the pool with the following command:</li> </ol> <pre><code>$ kalavai pool join &lt;token&gt;\n\n[16:28:14] Token format is correct\n           Joining private network\n\n[16:28:24] Scanning for valid IPs...\n           Using 100.10.0.8 address for worker\n           Connecting to BOINC @ 100.10.0.9 (this may take a few minutes)...\n[16:29:41] Worskpace created\n           You are connected to BOINC\n</code></pre> <p>That's it, your machine is now contributing to scientific discovery!</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#stop-sharing","title":"Stop sharing","text":"<p>You can either pause sharing, or stop and leave the pool altogether (don't worry, you can rejoin using the same steps above anytime). </p> <p>To pause sharing (but remain on the pool), run the following command:</p> <pre><code>kalavai pool pause\n</code></pre> <p>When you are ready to resume sharing, run:</p> <pre><code>kalavai pool resume\n</code></pre> <p>To stop and leave the pool, run the following:</p> <pre><code>kalavai pool stop\n</code></pre>","tags":["boinc","volunteer computing"]},{"location":"boinc/#faqs","title":"FAQs","text":"","tags":["boinc","volunteer computing"]},{"location":"boinc/#something-isnt-right","title":"Something isn't right","text":"<p>Growing pains! Please report any issues in our github repository.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#can-i-join-and-leave-whenever-i-want","title":"Can I join (and leave) whenever I want?","text":"<p>Yes, you can, and we won't hold a grudge if you need to use your computer. You can pause or quit altogether as indicated here.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#what-is-in-it-for-me","title":"What is in it for me?","text":"<p>If you decide to share your compute with BOINC, you will gather credits in Kalavai, which will be redeemable for computing in any other public pool (this feature is coming really soon).</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#is-my-gpu-constantly-being-used","title":"Is my GPU constantly being used?","text":"<p>No. BOINC projects upload tasks to be completed to a queue, which volunteers computers poll for work. If there is no suitable work to be done by the worker, the machine will remain idle and no resources are spent.</p> <p>If at any point you need your machine back, pause or stop sharing and come back when you are free.</p>","tags":["boinc","volunteer computing"]},{"location":"boinc/#how-do-i-check-how-much-have-i-contributed","title":"How do I check how much have I contributed?","text":"<p>Kalavai pools all the machines together and contributes to BOINC as a single entity. You can check how much the pool has shared overtime through the Science United leaderboard page -look out for <code>Kalavai.net</code> entry.</p> <p></p> <p>Individual users can also check how much compute have they contributed via their home page in our platform. Once you are logged in, click on the button displaying your user name on the left panel. This view will show how much of each key resource you have contributed thus far (CPUs, RAM, GPU).</p> <p></p>","tags":["boinc","volunteer computing"]},{"location":"choose_job_resources/","title":"Choosing job resources","text":"<p>Work in progress.</p>","tags":["job","resources","estimate resources"]},{"location":"choose_job_resources/#choosing-pool-resources-for-your-jobs","title":"Choosing pool resources for your jobs","text":"<p>Jobs describe the required resources for workers. All of these parameters have default values which are generally good for most cases, but more demanding LLMs will require extra resources. Here are the general resource parameters that are common to all jobs. For engine-specific information, check out the documentation for the job (vLLM, llama.cpp)</p> <ul> <li>working_memory </li> <li>cpus (per worker)</li> <li>memory (per worker RAM)</li> </ul> <p>If you want help on how much a model may require, you can use our internal estimation tool:</p> <p>kalavai job estimate  --precision  <p>For example, to deploy a 1B model at 16 floating point precision:</p> <pre><code>$ kalavai job estimate 1 --precision 16\n\nThere are 3 GPUs available (24.576GBs) \nA 1B model requires ~1.67GB vRAM at 16bits precision\nLooking at current capacity, use 1 GPU workers for a total 4.10 GB vRAM\n</code></pre>","tags":["job","resources","estimate resources"]},{"location":"cli/","title":"Kalavai from the command line (CLI)","text":"<p>The full functionality set of Kalavai LLM Pools can be accessed via the command line. This is ideal when working with Virtual Machines in the cloud or in automating workflows where GUI access is not possible or not required.</p> <pre><code>$ kalavai --help\n\nusage: kalavai [-h] command ...\n\npositional arguments:\n  command\n    login     [AUTH] (For public clusters only) Log in to Kalavai server.\n    logout    [AUTH] (For public clusters only) Log out of Kalavai server.\n    gui\n    location\n    pool\n    storage\n    node\n    job\n    ray\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre> <p>For help on a specific command, or group of commands, you can use the --help flag:</p> <pre><code>$ kalavai pool --help\n\nusage: kalavai pool [-h] command ...\n\npositional arguments:\n  command\n    publish      [AUTH] Publish pool to Kalavai platform, where other users may be able to join\n    unpublish    [AUTH] Unpublish pool to Kalavai platform. Cluster and all its workers will still work\n    list         [AUTH] List public pools in to Kalavai platform.\n    start        Start Kalavai pool and start/resume sharing resources.\n    token        Generate a join token for others to connect to your pool\n    check-token  Utility to check the validity of a join token\n    join         Join Kalavai pool and start/resume sharing resources.\n    stop         Stop sharing your device and clean up. DO THIS ONLY IF YOU WANT TO REMOVE KALAVAI-CLIENT from your\n                 device.\n    pause        Pause sharing your device and make your device unavailable for kalavai scheduling.\n    resume       Resume sharing your device and make device available for kalavai scheduling.\n    gpus         Display GPU information from all connected nodes\n    resources    Display information about resources on the pool\n    update       Update kalavai pool\n    status       Run diagnostics on a local installation of kalavai\n    attach       Set creds in token on the local instance\n\noptions:\n  -h, --help     show this help message and exit\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#examples","title":"Examples","text":"","tags":["cli","command line"]},{"location":"cli/#start-a-seed-node-and-get-token","title":"Start a seed node and get token","text":"<pre><code>kalavai pool start &lt;pool-name&gt;\n</code></pre> <p>Now you are ready to add worker nodes to this seed. To do so, generate a joining token:</p> <pre><code>$ kalavai pool token --user\n\nJoin token: &lt;token&gt;\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#add-worker-nodes","title":"Add worker nodes","text":"<pre><code>kalavai pool join &lt;token&gt;\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#attach-more-clients","title":"Attach more clients","text":"<p>You can now connect to an existing pool from any computer -not just from worker nodes. To connect to a pool, run:</p> <pre><code>kalavai pool attach &lt;token&gt;\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#check-resources-in-the-pool","title":"Check resources in the pool","text":"<p>List resources are available:</p> <pre><code>$ kalavai pool resources\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503           \u2503 n_nodes \u2503 cpu   \u2503 memory      \u2503 nvidia.com/gpu \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Available \u2502 4       \u2502 38.08 \u2502 70096719872 \u2502 3              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Total     \u2502 4       \u2502 42    \u2502 70895734784 \u2502 3              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n$ kalavai pool gpus\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Node               \u2503 Ready \u2503 GPU(s)                                               \u2503 Available \u2503 Total \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 carlosfm-desktop-1 \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 2070 (8 GBs)               \u2502 1         \u2502 1     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 carlosfm-desktop-2 \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 3060 (12 GBs)              \u2502 1         \u2502 1     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 pop-os             \u2502 True  \u2502 NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4 GBs) \u2502 1         \u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>","tags":["cli","command line"]},{"location":"cli/#deploy-jobs","title":"Deploy jobs","text":"<p>Deploy a job using a template:</p> <pre><code>$ kalavai job run aphrodite --values qwen2.5-1.5B.yaml\n\n[01:42:07] SELECT Target GPUs for the job          \n[KalavaiAuthClient]Logged in as carlosfm\n\n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\n[01:42:40] AVOID Target GPUs for the job\n\n0) Any/None\n1) NVIDIA-NVIDIA GeForce RTX 2070 (8GB) (in use: False)\n2) NVIDIA-NVIDIA GeForce RTX 3060 (12GB) (in use: False)\n3) NVIDIA-NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB) (in use: False)\n--&gt;  : 0\n\n[01:43:13] Template /home/carlosfm/.cache/kalavai/templates/aphrodite/template.yaml successfully deployed!\n[01:43:15] Service deployed   \n</code></pre> <p>List available jobs:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Deployment \u2503 Status                         \u2503 Workers    \u2503 Endpoint                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 qwen-1     \u2502 [2024-11-27T02:17:35Z] Pending \u2502 Pending: 1 \u2502 http://100.10.0.2:30271 \u2502\n\u2502            \u2502                                \u2502 Ready: 1   \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[01:48:23] Check detailed status with kalavai job status &lt;name of deployment&gt; \n           Get logs with kalavai job logs &lt;name of deployment&gt; (note it only works when the deployment is complete) \n</code></pre>","tags":["cli","command line"]},{"location":"compatibility/","title":"Compatibility matrix","text":"<p>If your system is not currently supported, open an issue and request it. We are expanding this list constantly.</p>","tags":["requirements","compatibility"]},{"location":"compatibility/#os-compatibility","title":"OS compatibility","text":"<p>Currently seed nodes are supported exclusively on linux machines (x86_64 platform). However Kalavai supports mix-pools, i.e. having Windows and MacOS computers as workers.</p> <p>Since worker nodes run inside docker, any machine that can run docker should be compatible with Kalavai. Here are instructions for linux, Windows and MacOS.</p> <p>The kalavai client, which controls and access pools, can be installed on any machine that has python 3.10+.</p>","tags":["requirements","compatibility"]},{"location":"compatibility/#hardware-compatibility","title":"Hardware compatibility:","text":"<ul> <li><code>amd64</code> or <code>x86_64</code> CPU architecture for seed and worker nodes.</li> <li><code>arm64</code> CPU architecture for worker nodes.</li> <li>NVIDIA GPU</li> <li>Mac M series, AMD and Intel GPUs are currently not supported (interested in helping us test it?)</li> </ul>","tags":["requirements","compatibility"]},{"location":"compatibility/#help-testing-new-systems","title":"Help testing new systems","text":"<p>If you want to help testing Kalavai in new Windows / Linux based systems (thank you!), follow the next steps:</p> <ol> <li> <p>Follow the instructions to install the kalavai client.</p> </li> <li> <p>Save the entire install logs (printed out in the console) to a file (install.log)</p> </li> <li> <p>If the installation went through, run kalavai commands to test the output:</p> </li> </ol> <pre><code>kalavai pool status &gt; status.log\nkalavai pool start test &gt; test_pool.log\nkalavai pool resources &gt; resources.log\n</code></pre> <ol> <li> <p>Create an issue on our repo and share the results. Include the four log files (status.log, test_pool.log, resources.log and install.log) as well as a description of the system you are testing.</p> </li> <li> <p>If the system ends up being supported, you'll be invited to create a PR to add support to the compatibility matrix.</p> </li> </ol>","tags":["requirements","compatibility"]},{"location":"compatibility/#help-testing-amd-gpus","title":"Help testing AMD GPUs","text":"<p>If you are interested in testing AMD support within Kalavai and own an AMD GPU card, please contact us or create an issue!</p>","tags":["requirements","compatibility"]},{"location":"concepts/","title":"Concepts","text":"<p>Work in progress</p>","tags":["concepts","architecture","pool"]},{"location":"concepts/#core-components","title":"Core components","text":"<p>Kalavai turns devices into a scalable LLM platform. It connects multiple machines together and manages the distribution LLM workloads on them.</p> <p>There are three core components:</p> <ul> <li>Kalavai client: python CLI program that lets users create and interact with LLM pools distributed across multiple machines.</li> <li>Seed node: master / server machine that initialises and manages an LLM pool. This is the node where the client runs the start command (<code>kalavai pool start &lt;pool&gt;</code>). Note that seed nodes must remain on and available for the platform to remain operational.</li> <li>Worker node: any machine that joins an LLM pool, where the LLM workload will be deployed to. This are nodes that run the join command (<code>kalavai pool join &lt;token&gt;</code>)</li> </ul> <p>Typically a client will be installed in both the seed and worker nodes, but since v0.5.0, clients can also be installed on external machines. This is useful to be able to connect and send work to your pool from any machine.</p>","tags":["concepts","architecture","pool"]},{"location":"concepts/#how-it-works","title":"How it works?","text":"<p>To create an LLM pool, you need a seed node which acts as a control plane. It handles bookkeeping for the pool. With a seed node, you can generate join tokens, which you can share with other machines --worker nodes.</p> <p>The more worker nodes you have in a pool, the bigger workloads it can run. Note that the only requirement for a fully functioning pool is a single seed node.</p> <p>Once you have a pool running, you can easily deploy workloads using template jobs. These are community integrations that let users deploy LLMs using multiple model engines. A template makes using Kalavai really easy for end users, with a parameterised interface, and it also makes the platform infinitely expandable.</p>","tags":["concepts","architecture","pool"]},{"location":"faqs/","title":"FAQs","text":"<p>Work in progress</p>","tags":["FAQs"]},{"location":"faqs/#general","title":"General","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-ai-pools","title":"What are AI pools?","text":"<p>In Kalavai parlor, a pool refers to a group of resources. We go beyond machine procurement and include everything a team needs to work on AI; from the hardware devices (GPUs, CPUs and memory) to the setup of a distributed environment and the tech stack needed to make it useful.</p> <p>Kalavai aims to manage it all (procurement of additional cloud resources, installing and configuring open source and industry standard frameworks, configuration management, facilitate distributed computing) so teams can focus on AI innovation.</p>","tags":["FAQs"]},{"location":"faqs/#isnt-the-performance-of-distributed-training-much-slower","title":"Isn\u2019t the performance of distributed training much slower?","text":"<p>Distributed computing is not an option: due to the skyrocketing demand in computation from AI models, we are going to need to use multiple devices to do training and inference. </p> <p>NVIDIA cannot get devices larger fast enough, and cloud providers are busy counting the money they are going to make from all that computing to care.</p> <p>Kalavai has considerable tailwinds that will work to minimise the impact of distributed computing in the future:  - Consumer-grade GPU performance per dollar is improving at a faster rate than cloud GPUs - By 2030: Internet broadband speed will reach Gbit/s and 6G will reduce latency &lt; 1 microsecond</p>","tags":["FAQs"]},{"location":"faqs/#host-nodes","title":"Host nodes","text":"","tags":["FAQs"]},{"location":"faqs/#what-are-the-minimum-specs-for-sharing","title":"What are the minimum specs for sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#is-my-device-safe","title":"Is my device safe?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-use-my-device-whilst-sharing","title":"Can I use my device whilst sharing?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-limit-what-i-share-with-kalavai","title":"Can I limit what I share with Kalavai?","text":"","tags":["FAQs"]},{"location":"faqs/#can-i-run-the-kalavai-app-within-a-vm","title":"Can I run the kalavai app within a VM?","text":"","tags":["FAQs"]},{"location":"faqs/#why-does-it-require-sudo-privileges","title":"Why does it require sudo privileges?","text":"","tags":["FAQs"]},{"location":"faqs/#developers","title":"Developers","text":"","tags":["FAQs"]},{"location":"faqs/#there-are-plenty-of-mlops-platforms-out-there-why-would-organisations-turn-to-you-instead","title":"There are plenty of MLOps platforms out there, why would organisations turn to you instead?","text":"<p>MLOps solutions out there are great, and they continue to develop. But they all need hardware to run on; whether it is on premise servers, public cloud resources or managed services. </p> <p>We think of MLOps platforms as complementors, that\u2019s why we are building a marketplace for third parties to bring their solutions to our users. Since we manage the computing layer, we abstract away the complexity of integration for them, so they can also bring their tools without having to build multiple integrations.</p>","tags":["FAQs"]},{"location":"faqs/#enterprises","title":"Enterprises","text":"","tags":["FAQs"]},{"location":"faqs/#you-are-leveraging-the-organisations-existing-hardware-but-this-is-unlikely-to-meet-ai-demands-are-we-not-back-to-square-one-since-they-need-to-anyways-go-to-the-cloud","title":"You are leveraging the organisation's existing hardware, but this is unlikely to meet AI demands. Are we not back to square one since they need to anyways go to the cloud?","text":"<p>Our goal is not to narrow companies' choices but to manage the complexity of hybrid clouds. Organisations can bring hardware from anywhere (their own premises, their company devices, all the way to multi cloud on-demand resources) and Kalavai manages them equally. Developers then see a pool of resources that they treat the same.</p>","tags":["FAQs"]},{"location":"faqs/#organisations-with-on-premise-servers-already-have-systems-to-use-them-why-would-they-trust-you-to-manage-that-for-all-their-needs","title":"Organisations with on premise servers already have systems to use them. Why would they trust you to manage that for all their needs?","text":"<p>Kalavai works as an integration system, it does not force organisations to switch every workflow they have over to benefit from it. They can install the kalavai client in their existing on premise servers, which will automatically then connect them to the pool and make them able to run workflows. The kalavai client is designed to co-exist with any application and can be limited to use only a portion of resources, so organisations can easily continue to use their on premise deployments.</p>","tags":["FAQs"]},{"location":"faqs/#ive-heard-of-a-bunch-of-service-providers-for-rent-a-gpu-on-demand-isnt-the-market-saturated-already","title":"I\u2019ve heard of a bunch of service providers for rent-a-GPU on demand. Isn\u2019t the market saturated already?","text":"<p>Kalavai does not have any hardware to lease. We believe there are enough providers out there to cover that. Where there\u2019s a gap is in managing the complexity of use cases that require distributed computing. When workflows require more than one computing device to run, organisations need to manage the orchestration, maintenance and coordination of devices.</p> <p>We have designed Kalavai to integrate nicely with almost any computing resource out there, from public cloud, serverless GPU providers and on premise devices.</p> <p>Got another question?</p>","tags":["FAQs"]},{"location":"getting_started/","title":"Getting started","text":"<p>The <code>kalavai</code> client is the main tool to interact with the Kalavai platform, to create and manage pools and also to interact with them (e.g. deploy models). Let's go over its installation. </p> <p>From release v0.5.0, you can now install <code>kalavai</code> client in non-worker computers. You can run a pool on a set of machines and have the client on a remote computer from which you access the LLM pool. Because the client only requires having python installed, this means more computers are now supported to run it.</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#requirements-to-run-the-client","title":"Requirements to run the client","text":"<p>For seed nodes: - A 64 bits x86 based Linux machine (laptop, desktop or VM) - Docker engine installed with privilege access. - Python 3.10+</p> <p>For workers sharing resources with the pool:</p> <ul> <li>A laptop, desktop or Virtual Machine (MacOS, Linux or Windows; ARM or x86)</li> <li>If self-hosting, workers should be on the same network as the seed node. Looking for over-the-internet connectivity? Check out our managed seeds</li> <li>Docker engine installed (for linux, Windows and MacOS) with privilege access.</li> <li>Python 3.10+</li> </ul>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#ports","title":"Ports","text":"<p>Once a machine is part of a pool, the following ports must be enabled and open to accept and process workloads:</p> <p>Seed nodes: - 2379-2380 TCP inbound/outbound  - 6443 TCP inbound - 8472 UDP inbound/outbound - 10250 TCP inbound/outbound - 51820-51821 UDP inbound/outbound</p> <p>Worker nodes: - 6443 TCP outbound - 8472 UDP inbound/outbound - 10250 TCP inbound/outbound - 51820-51821 UDP inbound/outbound - 5121 TCP inbound/outbound</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#install-the-client","title":"Install the client","text":"<p>The client is a python package and can be installed with one command:</p> <pre><code>pip install kalavai-client\n</code></pre>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#create-a-local-private-ai-pool","title":"Create a local, private AI pool","text":"<p>Kalavai is free to use, no caps, for both commercial and non-commercial purposes. All you need to get started is one or more computers that can see each other (i.e. within the same network), and you are good to go. For over-the-internet pools, see our managed pools.</p> <p>To create your own AI pool, you will need at least one machine (the seed) and (optionally) one or more workers. See our concepts page for an overview of AI pool architecture. Note that seed machines should always be available for the platform to remain operational.</p> <p>You can create a seed by self-hosting the open source platform (limited to same network machines only) or using our managed pools service (pre-configured, hosted seed with over-the-internet workers from everywhere).</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#1a-self-hosted-create-a-seed","title":"1a. [Self hosted] Create a seed","text":"<p>Note: Currently seed nodes are only supported in Linux x86_64 machines. </p> <p>In any machine with the <code>kalavai</code> client installed, execute the following to start a seed node:</p> <pre><code>kalavai pool start &lt;name&gt;\n</code></pre> <p>Where  is the name of the pool. This will deploy a series of docker containers to manage and interact with the platform. Once the seed is up and running, you can start the GUI manually to manage devices and workloads: <pre><code>$ kalavai gui start\n\n[10:11:13] Using ports: [49152, 49153, 49154]                                      cli.py:236\n[+] Running 2/2\n \u2714 Network kalavai_kalavai-net  Created0.1s  \n \u2714 Container kalavai_gui        Started0.4s  \n           Loading GUI, may take a few minutes. It will be available at            cli.py:258\n           http://localhost:49153\n</code></pre> <p>By default, the GUI is available via your browser at http://localhost:49153 (but note the port may change depending on port availability).</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#1b-managed-pools-create-a-seed","title":"1b. [Managed pools] Create a seed","text":"<p>We offer a service to host and manage seed nodes with the following advantages: - Connect worker nodes from anywhere (over-the-internet) - Always on to keep your AI pool operational. - Great if you don't have a linux x86_64 machine to use as a seed.</p> <p>Create a free account on our platform. Then, navigate to <code>My Pools</code> to manage and create seed nodes for your pools:</p> <p></p> <p>Once your seed is up and running and the status is <code>Healthy</code>, you can access the GUI by clicking on the GUI action.</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#2-add-worker-nodes","title":"2. Add worker nodes","text":"<p>Important: if you are self hosting seed nodes, only nodes within the same network as the seed node can be added successfully. This limitation does not apply to our managed seeds</p> <p>Increase the power of your AI pool by adding resources from other devices. For that, you need to generate a joining token. You can do this by using the seed GUI or the CLI.</p> <p>[On the seed node] Using the GUI</p> <p>Use the navigation panel to go to <code>Devices</code>, and then click the <code>circle-plus</code> button to add new devices. You can select the <code>Access mode</code>, which determine the level of access new nodes will have over the pool: - <code>admin</code>: Same level of access than the seed node, including generating new joining tokens and deleting nodes. - <code>user</code>: Can deploy jobs, but lacks admin access over nodes. - <code>worker</code>: Workers carry on jobs, but cannot deploy their own jobs or manage devices.</p> <p></p> <p>[On the seed node] Using the CLI</p> <p>Alternatively, if you do not want to use the GUI, you can join from the command-line. Run the following to obtain your joining token:</p> <pre><code>kalavai pool token --worker\n</code></pre> <p>[On the worker node] Join the pool</p> <p>Once you have the joining token, use it on the machines you want to add to the pool. Workers can use the GUI interface to make this step easier too:</p> <pre><code>kalavai gui start\n</code></pre> <p>Then paste the joining token in the text field under <code>Access with token</code>, and click join</p> <p></p> <p>Kalavai asks you if you want to join (run workloads in the local machine) or attach (use the node to access and control the pool, without running workloads) to the pool. </p> <p></p> <p>Alternatively, for command-liners, join with the CLI:</p> <pre><code>kalavai pool join &lt;TOKEN&gt;\n</code></pre> <p>Or attach with the CLI:</p> <pre><code>kalavai pool attach &lt;TOKEN&gt;\n</code></pre>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#3-explore-resources","title":"3. Explore resources","text":"<p>For both seed and worker nodes, the dashboard shows a high level view of the LLM pool: resources available, current utilisation and active devices and deployments.</p> <p></p> <p>Use the navigation bar to see more details on key resources:</p> <ul> <li>Devices: every machine connected to the pool and its current status</li> </ul> <p></p> <ul> <li>GPUs: list of all available and utilised GPUs</li> </ul> <p></p> <ul> <li>Jobs: all models and deployments active in the pool</li> </ul> <p></p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#4-leave-the-pool","title":"4. Leave the pool","text":"<p>Any device can leave the pool at any point and its workload will get reassigned. To leave the pool, click the <code>circle-stop</code> button on the dashboard, under <code>Local status</code> card. Nodes can rejoin at any point following the above procedure.</p> <p></p> <p>Or do so with the CLI (from the worker machine):</p> <pre><code>kalavai pool stop\n</code></pre>","tags":["kalavai-client","cli","install","requirements"]},{"location":"getting_started/#whats-next","title":"What's next","text":"<p>Now that you know how to get a pool up and running, check our end to end tutorial on how to self-host an LLM Pool with OpenAI compatible API and a ChatGPT-like interface for all your LLM models.</p>","tags":["kalavai-client","cli","install","requirements"]},{"location":"ray/","title":"Ray","text":"<p>Work in progress</p>","tags":["ray"]},{"location":"ray/#ray-clusters-for-distributed-computing","title":"Ray clusters for distributed computing","text":"<p>From Ray's documentation:</p> <p>Ray is an open-source unified framework for scaling AI and Python applications like machine learning.</p> <p>Kalavai and Ray work perfectly together. Ray is a great framework to deal with distributed computation on top of an existing hardware pool. Kalavai acts as a unifying layer that brings that required hardware together for Ray to do its magic.</p> <p>To get started, check out our example to get a Ray cluster going. </p>","tags":["ray"]},{"location":"ray/#create-a-cluster","title":"Create a cluster","text":"<ul> <li>Specs how to define specs: kalavai pool resources (cpu, memory and nvidia.com/gpu)</li> </ul> <pre><code>$ kalavai pool resources\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \n\u2503           \u2503 n_nodes \u2503 cpu                \u2503 memory      \u2503 nvidia.com/gpu \u2503 \n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \n\u2502 Available \u2502 2       \u2502 10.684999999999999 \u2502 16537780224 \u2502 1              \u2502 \n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \n\u2502 Total     \u2502 4       \u2502 42                 \u2502 70895030272 \u2502 3              \u2502 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n</code></pre> <pre><code>spec:\n  ...\n  headGroupSpec:\n    ...\n    template:\n      spec:\n        ...\n        containers:\n        ...\n          resources:\n            limits:\n              cpu: 2\n              memory: 4Gi\n            requests:\n              cpu: 2\n              memory: 4Gi\n  workerGroupSpecs:\n  ...\n    template:\n      spec:\n        containers:\n        ...\n          resources:\n            limits:\n              nvidia.com/gpu: 1\n              cpu: 2\n              memory: 4Gi\n            requests:\n              nvidia.com/gpu: 1\n              cpu: 2\n              memory: 4Gi\n</code></pre> <p>Interact with Ray - Interactive mode - Endpoint - RayJobs</p>","tags":["ray"]},{"location":"ray/#advanced-topics","title":"Advanced topics","text":"<p>Autoscaling</p> <p>Node hardware requirements (limits vs requests)</p>","tags":["ray"]},{"location":"self_hosted_llm_pool/","title":"Self-hosted LLM pools","text":"<p>\u2b50\u2b50\u2b50 Kalavai and our LLM pools are open source and free to use in both commercial and non-commercial purposes. If you find it useful, consider supporting us by giving a star to our GitHub project, joining our discord channel and follow our Substack.</p> <p>This guide will show you how to start a self-hosted LLM pool with your own hardware, configure it with a single API and UI Playground for all your models and deploy and access a Qwen3 4B instance.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#what-youll-achieve","title":"What you'll achieve","text":"<ol> <li>Configure unified LLM interface</li> <li>Deploy a llamacpp model</li> <li>Access model via code and UI</li> </ol>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#1-pre-requisites","title":"1. Pre-requisites","text":"<ul> <li>Install kalavai CLI on each machine</li> <li>Set up a 2 machine LLM pool, i.e. a seed node and one worker</li> </ul> <p>Note: the following commands can be executed on any machine that is part of the pool, provided you have used <code>admin</code> or <code>user</code> access modes to generate the token. If you have used <code>worker</code>, deployments are only allowed in the seed node.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#unified-openai-like-api","title":"Unified OpenAI-like API","text":"<p>Model templates deployed in LLM pools have an optional key parameter to register themselves with a LiteLLM instance. LiteLLM is a powerful API that unifies all of your models into a single API, making developing apps with LLMs easier and more flexible.</p> <p>Our LiteLLM template automates the deployment of the API across a pool, database included. To deploy it using the Kalavai GUI, navigate to <code>Jobs</code>, then click on the <code>circle-plus</code> button, in which you can select a <code>litellm</code> template.</p> <p></p> <p>Once the deployment is complete, you can check the LiteLLM endpoint by navigating to <code>Jobs</code> and seeing the corresponding endpoint for the <code>litellm</code> job.</p> <p></p> <p>You will need a virtual key to register models with LiteLLM. For testing you can use the master key defined in your values.yaml under <code>master_key</code>, but it is recommended to generate a virtual one that does not have privilege access. The easiest way of doing so is via the admin UI, under http://192.168.68.67:30535/ui (see more details here).</p> <pre><code>Example virtual key: sk-rDCm0Vd5hDOigaNbQSSsEQ\n</code></pre> <p></p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#unified-ui-playground","title":"Unified UI Playground","text":"<p>OpenWebUI is a great ChatGPT-like app that helps testing LLMs. Our WebUI template manages the deployment of an OpenWebUI instance in your LLM pool, and links it to your LiteLLM instance, so any models deployed and registered with LiteLLM automatically appear in the playground.</p> <p>To deploy, navigate back to <code>Jobs</code> and click the <code>circle-plus</code> button, this time selecting the playground template. Set the <code>litellm_key</code> to match your virtual key.</p> <p>Once it's ready, you can access the UI via its advertised endpoint (under <code>Jobs</code>), directly on your browser. The first time you login you'll be able to create an admin user. Check the official documentation for more details on the app.</p> <p></p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#check-deployment-progress","title":"Check deployment progress","text":"<p>Jobs may take a while to deploy. Check the progress in the <code>Jobs</code> page, or using the CLI:</p> <pre><code>$ kalavai job list\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Owner   \u2503 Deployment \u2503 Workers    \u2503 Endpoint                                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 default \u2502 litellm    \u2502 Ready: 2   \u2502 http://192.168.68.67:30535 (mapped to 4000) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 default \u2502 webui-1    \u2502 Pending: 1 \u2502 http://192.168.68.67:31141 (mapped to 8080) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this case, <code>litellm</code> has been deployed but <code>webui-1</code> is still pending schedule. If a job cannot be scheduled due to lack of resources, consider adding more nodes or reducing the requested resources via the <code>values.yaml</code> files.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#3-deploy-models-with-compatible-frameworks","title":"3. Deploy models with compatible frameworks","text":"<p>In this section, we'll look into how to deploy a model with another of our supported model engines: llama.cpp. You can use the kalavai CLI to deploy jobs (via kalavai job deploy) but here we'll use the much simpler GUI route.</p> <p>Just like we did for LiteLLM and Playground, you can deploy a model by navigating to the Jobs page and clicking the <code>circle-plus</code> button. Select <code>llamacpp</code> as model template, and populate the following values:</p> <ul> <li><code>working_memory</code>: 10 (enough free space GBs to fit the model weights)</li> <li><code>workers</code>: 2 (this will distribute the model onto our 2 machines)</li> <li><code>repo_id</code>: Qwen/Qwen3-4B-GGUF (the repo id from Huggingface)</li> <li><code>model_filename</code>: Qwen3-4B-Q4_K_M.gguf (the filename of the quantized version we want)</li> <li><code>hf_token</code>:  if using a gated model (in this case it's not needed) <li><code>litellm_key</code>: sk-qoQC5lijoaBwXoyi_YP1xA (Advanced parameter; the virtual key generated above for LiteLLM. This is key to make sure models are self registering to both LiteLLM and the playground.)</li> <p></p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#4-access-your-models","title":"4. Access your models","text":"<p>Once they are donwloaded and loaded into memory, your models will be readily available both via the LiteLLM API as well as through the UI Playground. </p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#ui-playground","title":"UI Playground","text":"<p>The pool comes with an OpenWebUI deployment (<code>playground</code> job) to make it easy to test model inference with LLMs via the browser. Within the UI you can select the model you wish to test and have a chat.</p> <p></p> <p>Note: the playground is a shared instance to help users test models without code and should not be used in production. You need to create a playground account to access it. This can be different to your Kalavai account details. The creation of a new user is necessary to keep things like user chat history and preferences.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#single-api-endpoint","title":"Single API endpoint","text":"<p>All interactions to models in the pool are brokered by a LiteLLM endpoint that is installed in the system. To interact with it you need the following:</p> <ul> <li>The <code>LITELLM_URL</code> is the endpoint displayed in the <code>Jobs</code> page for the <code>litellm</code> job.</li> <li>The <code>LITELLM_KEY</code> is the one you have generated above.</li> <li>The <code>MODEL_NAME</code> you want to use (the job name displayed in the <code>Jobs</code> page)</li> </ul> <p>In this example:</p> <ul> <li><code>LITELLM_URL=http://192.168.68.67:30535</code></li> <li><code>LITELLM_KEY=sk-qoQC5lijoaBwXoyi_YP1xA</code></li> <li><code>MODEL_NAME=qwen3_qwen3_4b_gguf_qwen3_4b_q4_k_m_gguf</code></li> </ul>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#check-available-llms","title":"Check available LLMs","text":"<p>Using cURL:</p> <pre><code>curl -X GET \"&lt;LITELLM_URL&gt;/v1/models\" \\\n  -H 'Authorization: Bearer &lt;LITELLM_KEY&gt;' \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\"\n</code></pre> <p>Using python:</p> <pre><code>import requests\n\nLITELLM_URL = \"http://192.168.68.67:30535\"\nLITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\"\n\n\ndef list_models():\n    response = requests.get(\n        f\"{LITELLM_URL}/v1/models\",\n        headers={\"Authorization\": f\"Bearer {LITELLM_KEY}\"}\n    )\n    return response.json()\n\n\nif __name__ == \"__main__\":\n    print(\n        list_models()\n    )\n</code></pre>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#use-models","title":"Use models","text":"<p>Using cURL:</p> <pre><code>curl --location '&lt;LITELLM_URL&gt;/chat/completions' \\\n  --header 'Authorization: Bearer &lt;LITELLM_KEY&gt;' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n      \"model\": \"&lt;MODEL_NAME&gt;\",\n      \"messages\": [\n          {\n          \"role\": \"user\",\n          \"content\": \"what llm are you\"\n          }\n      ]\n  }'\n</code></pre> <p>Using python:</p> <pre><code>import requests\n\nLITELLM_URL = \"http://192.168.68.67:30535\"\nLITELLM_KEY = \"sk-qoQC5lijoaBwXoyi_YP1xA\"\n\ndef model_inference():\n    response = requests.post(\n        f\"{LITELLM_URL}/chat/completions\",\n        headers={\"Authorization\": f\"Bearer {LITELLM_KEY}\"},\n        json={\n            \"model\": \"&lt;MODEL_NAME&gt;\",\n            \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"what llm are you\"\n            }]\n        }\n    )\n    return response.json()\n\n\nif __name__ == \"__main__\":\n    print(\n        model_inference()\n    )\n</code></pre> <p>For more details on the endpoint(s) parameters, check out LiteLLM documentation and the Swagger API</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#5-clean-up","title":"5. Clean up","text":"<p>To remove any model deployment, navigate to the <code>Jobs</code> page, select the job (checkbox next to its name) and click the <code>bin</code> icon on top of the table. This will remove the deployment from any worker involved and free its resources.</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"self_hosted_llm_pool/#6-whats-next","title":"6. What's next?","text":"<p>Enjoy your new supercomputer, check out our templates and examples for more model engines and keep us posted on what you achieve!</p>","tags":["private","self-hosted","LLM pool","llamacpp","openai-like api","chatgpt-like ui"]},{"location":"templates/","title":"Develop with Kalavai","text":"<p>Work in progress</p> <p>Template jobs built by Kalavai and the community make deploying distributed LLMs easy for end users.</p> <p>Templates are like recipes, where developers describe what worker nodes should run, and users customise the behaviour via pre-defined parameters. Kalavai handles the heavy lifting: workload distribution, communication between nodes and monitoring the state of the deployment to restart the job if required.</p> <p>Using the client, you can list what templates your LLM pool supports:</p> <pre><code>$ kalavai job templates\n\n[10:51:29] Templates available in the pool\n           ['vllm', 'aphrodite', 'llamacpp', 'petals', 'litellm', 'playground', 'boinc', 'gpustack']\n</code></pre> <p>Deploying a template is easy:</p> <pre><code>kalavai job run &lt;template name&gt; --values &lt;template values&gt;\n</code></pre> <p>Where <code>&lt;template name&gt;</code> refers to one of the supported templates above, and <code>&lt;template values&gt;</code> is a local yaml file containing the parameters of the job. See examples for more information.</p>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"templates/#list-of-available-templates","title":"List of available templates","text":"<ul> <li>vLLM: deploy your favourite LLMs in distributed machines.</li> <li>llama.cpp: deploy llama.cpp models (CPU and GPU) in distributed machines.</li> <li>Aphrodite: deploy your favourite LLMs in distributed machines.</li> <li>Petals: bit-torrent style deployment of LLMs</li> <li>litellm: Unified LLM API.</li> <li>playground: Unified UI playground.</li> </ul>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"templates/#how-to-contribute","title":"How to contribute","text":"<p>Do you want to develop your own template and share it with the community? We are working on a path to make it easier for developers to do so. Hang on tight! But for now, head over to our repository and check the examples in there.</p>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"templates/#why-is-insert-preferred-application-not-supported","title":"Why is [insert preferred application] not supported?","text":"<p>If your preferred distributed ML application is not yet supported, let us know! Or better yet, add it and contribute to community integrations.</p>","tags":["integrations","jobs","LLM engines","ray"]},{"location":"managed/cogenai/","title":"Model self-hosting made easier: CoGenAI","text":"<p>CoGenAI is the easiest way to deploy and use LLMs.</p> <p>Production ready Single endpoint for all models Scalable and resilient No config, just works Supported models growing Explain params + example</p> <p>Model Hosting with CoGenAI Overview</p> <p>CoGenAI  is Kalavai\u2019s managed model hosting platform \u2014 designed for self-hosting open-source LLMs (like Llama, Mistral, Falcon, etc.) with zero infrastructure setup.</p> <p>It lets you deploy models directly into Kalavai\u2019s managed environment \u2014 production-ready from day one.</p> <p>Why CoGenAI?</p> <p>Fully managed: No infrastructure or DevOps required</p> <p>Scalable: Auto-scale inference pods based on load</p> <p>Affordable: Pay only for the compute you use</p> <p>Secure: Isolated environments with private model endpoints</p> <p>Getting Started</p> <p>Sign in to CoGenAI</p> <p>Visit https://cogenai.kalavai.net</p> <p>Log in or create your Kalavai account</p> <p>Select a Base Model Choose from supported open models (e.g., Llama 3, Mistral, Falcon).</p> <p>Deploy Your Model</p> <p>kalavai model deploy --model llama3 --name my-llm --gpus 2</p> <p>Access Your Endpoint Once deployed, CoGenAI provides an inference API endpoint:</p> <p>curl -X POST https://api.cogenai.kalavai.net/v1/my-llm \\   -H \"Authorization: Bearer \" \\   -d '{\"prompt\": \"Hello Kalavai!\"}' <p>Monitor and Scale</p> <p>View logs and metrics via CoGenAI Dashboard</p> <p>Adjust compute resources with one command:</p> <p>kalavai model scale my-llm --gpus 4</p>","tags":["LLM hosting","LLM inference","CoGenAI"]},{"location":"managed/gpustack/","title":"GPUStack","text":"<p>Beta access only. Register your interest here</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#gpustack-in-kalavai","title":"GPUStack in Kalavai","text":"<p>Create, manage and deploy LLMs across multiple devices with GPUStack in Kalavai.</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#what-is-gpustack","title":"What is GPUStack?","text":"<p>GPUStack is an open-source system for managing GPU scheduling and sharing across workloads. Kalavai offers managed GPUStack clusters that automatically allocate resources to the GPUStack pool.</p> <p>Platform features:</p> <ul> <li>Create GPUStack clusters with mix hardware (NVIDIA, AMD) without dealing with infrastructure</li> <li>Highly configurable (number of GPUs, node capabilities)</li> <li>Flexible and affordable access to thousands of data centre-level GPUs</li> </ul>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#getting-started","title":"Getting Started","text":"<p>Log in to your Kalavai account and navigate to the <code>Clusters</code> page. This section allows you to create, manage and connect to your GPU clusters. </p> <p></p> <p>As long as you are within your resource quota (as indicated under <code>Available Resources</code>) you can create as many clusters as you need --even multiple of the same type. You can create a cluster by selecting any of the supported templates (growing!) under the <code>Create new Cluster</code> section.</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#create-a-gpustack-cluster","title":"Create a GPUStack cluster","text":"<p>Select <code>GPUStack Cluster</code> on the list of cluster templates to configure your GPUStack cluster.</p> <p></p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#configuring-your-gpustack-cluster","title":"Configuring your GPUStack cluster","text":"<p>The GPUStack template allows you to configure your cluster instance to your needs. </p> <p></p> <p>Here's a list of key parameters:</p> <ul> <li><code>admin_password</code> (default: <code>\"password\"</code>, required): Default password for the <code>admin</code> user (required to login to the UI)</li> <li><code>working_memory</code> (default: <code>5</code>, optional): Temporary storage (in GB) to cache model weights.</li> <li><code>cuda_workers</code> (default: <code>1</code>, required): Number of remote NVIDIA workers (for tensor and pipeline parallelism).</li> <li><code>rocm_workers</code> (default: <code>1</code>, required): Number of remote AMD ROCm workers (for tensor and pipeline parallelism).</li> <li><code>token</code> (default: <code>\"sometoken\"</code>, required): Token used to load the cluster or authenticate access.</li> <li><code>hf_token</code> (default: <code>null</code>, required): Huggingface token, required to load model weights.</li> <li><code>cpus</code> (default: <code>2</code>, optional): CPUs per single worker (final count = cpus * num_workers).</li> <li><code>gpus</code> (default: <code>1</code>, optional): GPUs per single worker (final count = gpus * num_workers).</li> <li><code>memory</code> (default: <code>12</code>, optional): RAM memory per single worker (final count = memory * num_workers)</li> </ul> <p>When you are ready, click on <code>Deploy Cluster</code>. The GPUStack instance may take a few minutes to spin up. Check the status of the pool under <code>Your clusters</code>. Note that the cluster will be ready for access as soon as the head node is ready. Workers may be queued up to join based on the number and types requested.</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#example-hybrid-gpu-cluster","title":"Example: Hybrid GPU cluster","text":"<p>GPUStack lets you connect heterogeneous GPU devices to a single instance. For instance, you may connect 4 NVIDIA GPUs and 8 AMD GPUs with the following settings:</p> <ul> <li><code>cuda_workers</code>: 4</li> <li><code>rocm_workers</code>: 8</li> </ul> <p>When you deploy models in the GPUStack interface you can target either of the GPUs or do distributed deployments across multiple devices.</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#connecting-to-your-cluster","title":"Connecting to your cluster","text":"<p>Once the status of the cluster is <code>Ready</code> you are ready to put the instance to work. Each GPUStack cluster exposes a single endpoint for you to connect to the UI interface. </p> <p>Click on the UI endpoint of your GPUStack cluster. You will be presented with the login screen. Use the following default credentials:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>&lt;password set during cluster config&gt;</code></li> </ul> <p></p> <p>You can monitor the status of the workers and GPUs under the Resources section of your GPUStack cluster.</p> <p></p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#deploy-a-model","title":"Deploy a model","text":"<p>Follow the official examples from GPUStack to deploy models and more.</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/gpustack/#what-next","title":"What next","text":"<p>GPUStack documentation</p>","tags":["GPUstack","GPU cluster","LLM deployment"]},{"location":"managed/overview/","title":"Kalavai Developer Documentation","text":"<p>Welcome to the Kalavai Developer Docs \u2014 your guide to building, training, and deploying AI workloads on Kalavai\u2019s distributed compute platform.</p> <p>Kalavai leverages spare data center capacity to deliver flexible, cost-effective compute for machine learning, AI inference, and large model hosting.</p>","tags":["managed platform","hosted clusters"]},{"location":"managed/overview/#what-is-the-kalavai-platform","title":"What is the Kalavai Platform","text":"<p>Kalavai Platform is a managed computing platform that simplifies access to GPU compute and LLM hosting. It builds on our open-source orchestration library, integrating directly with tools you already use \u2014 like Ray and GPUStack \u2014 to provide on-demand distributed compute for AI workloads.</p>","tags":["managed platform","hosted clusters"]},{"location":"managed/overview/#efficient-cost-and-low-infrastructure-overhead","title":"Efficient cost and low infrastructure overhead","text":"<p>Our platform abstracts the complexity of provisioning and managing GPU clusters, while optimizing performance and cost through dynamic utilization of spare capacity. When using the Kalavai Platform, users have direct access to a large fleet of data centre level GPUs at the lowest price in the market.</p> Product Description Managed Ray Clusters Spin up distributed Ray and GPUStack clusters for training, hyperparameter tuning, reinforcement learning and custom workloads. Model Hosting Simple, scalable, and affordable managed hosting for open-source LLMs \u2014 from experimentation to production with CoGenAI.","tags":["managed platform","hosted clusters"]},{"location":"managed/overview/#beta-tester-program","title":"Beta Tester Program \ud83d\ude80","text":"<p>We\u2019re currently in Beta, and inviting developers and research teams to get early access to Kalavai. We're seeking developers who have hands-on experience with one or more of the following frameworks to participate in our beta testing program: Ray, Unsloth, Axolotl or GPUStack.</p> <p>\ud83d\udc49 Join the Beta Tester Program to get started.</p> <p>Join our exclusive Discord community for beta testers.</p>","tags":["managed platform","hosted clusters"]},{"location":"managed/overview/#get-started-for-free","title":"Get started for free","text":"<p>Get a free account here to access the Kalavai Platform. All accounts come with access to free resources, like CPUs, GPUs and memory.</p> <p>During the Beta Testing phase, you will be asked to join the Beta Program the first time you login. This will grant you free resources to test the platform.</p> <p></p> <p>If you need more resources, you can request more in the Clusters page, but note that capacity is limited.</p>","tags":["managed platform","hosted clusters"]},{"location":"managed/ray/","title":"Ray","text":"<p>Beta access only. Register your interest here</p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#ray-in-kalavai","title":"Ray in Kalavai","text":"<p>Create, manage and deploy Ray workloads in Kalavai without touching infrastructure.</p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#what-is-ray","title":"What is Ray?","text":"<p>Ray is a distributed framework for scaling Python and ML workloads. Kalavai\u2019s managed Ray clusters let you launch distributed training or inference tasks without setting up or managing nodes.</p> <p>Platform features:</p> <ul> <li>Create Ray clusters that autoscale to your needs without dealing with infrastructure</li> <li>Highly configurable (python version, CUDA kernels, node capabilities)</li> <li>Flexible and affordable access to thousands of data centre-level GPUs</li> </ul>","tags":["ray","distributed ML"]},{"location":"managed/ray/#getting-started","title":"Getting Started","text":"<p>Log in to your Kalavai account and navigate to the <code>Clusters</code> page. This section allows you to create, manage and connect to your GPU clusters. </p> <p></p> <p>As long as you are within your resource quota (as indicated under <code>Available Resources</code>) you can create as many clusters as you need --even multiple of the same type. You can create a cluster by selecting any of the supported templates (growing!) under the <code>Create new Cluster</code> section.</p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#create-a-ray-cluster","title":"Create a Ray cluster","text":"<p>Select <code>Ray Cluster</code> on the list of cluster templates to configure your Ray cluster.</p> <p></p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#configuring-your-ray-cluster","title":"Configuring your Ray cluster","text":"<p>The Ray template allows you to configure your cluster instance to your needs. </p> <p></p> <p>Here's a list of key parameters:</p> <ul> <li><code>workers</code> (default: <code>1</code>, required): Number of desired starting workers</li> <li><code>min_workers</code> (default: <code>1</code>, required): Minimum desired workers for autoscaling</li> <li><code>max_workers</code> (default: <code>1</code>, required): Maximum workers for autoscaling</li> <li><code>cpus</code> (default: <code>2</code>, optional): CPUs to be used per single worker (final one = cpus * workers). Workers should have these many CPUs available.</li> <li><code>gpus</code> (default: <code>1</code>, optional): GPUs to be used per single worker (final one = gpus * workers). Workers should have these many GPUs available.</li> <li><code>memory</code> (default: <code>8</code>, optional): RAM memory to be used per single worker (final one = memory * workers). Workers should have this much RAM available.</li> <li><code>cuda_gpu_mem_percentage</code> (default: <code>100</code>, optional): Maximum memory fraction allowed to be used from the GPU vRAM.</li> <li><code>ray_version</code> (default: <code>\"2.49.0\"</code>, optional): Ray version to use in the cluster</li> <li><code>python_version</code> (default: <code>\"312\"</code>, optional): Python version to use in the cluster (39, 310, 311, 312)</li> <li><code>cuda_version</code> (default: <code>\"cu124\"</code>, optional): CUDA version to use in the cluster (cu117 to cu128)</li> <li><code>upscaling_mode</code> (default: <code>\"Default\"</code>, optional): Defines autoscale mode. One of: Conservative, Default or Aggressive. More info</li> <li><code>idle_timeout_seconds</code> (default: <code>60</code>, optional): Defines the waiting time in seconds before scaling down an idle worker pod. More info</li> </ul> <p>When you are ready, click on <code>Deploy Cluster</code>. The Ray instance may take a few minutes to spin up. Check the status of the pool under <code>Your clusters</code>.</p> <p></p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#example-autoscalable-cluster","title":"Example: Autoscalable cluster","text":"<p>Autoscalable clusters are ideal to keep cost under control. You can set up a no-GPU cluster that autoscales on demand to up to 10 GPUs based on your workloads. To do so, here are the parameters you can use:</p> <ul> <li><code>workers</code>: 0</li> <li><code>min_workers</code>: 0</li> <li><code>max_workers</code>: 10</li> <li><code>idle_time_out</code>: 120</li> </ul> <p>With this configuration, you get a 0 GPUs cluster that scales up to 10 GPUs when you send demand to it. Once the demand ceases, each idle worker scales down itself after 120 seconds.</p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#connecting-to-your-cluster","title":"Connecting to your cluster","text":"<p>Once the status of the cluster is <code>Ready</code> you are ready to put the instance to work. Each Ray cluster exposes a list of endpoints:</p> <ul> <li><code>GCS</code>: Global Control Store, Coordinates communication between the head node and worker nodes.</li> <li><code>Dashboard</code>: Provides a web-based monitoring interface for the Ray cluster.</li> <li><code>Client</code>: Allows external Python clients (e.g., from your laptop) to connect remotely to a running Ray cluster. Used to connect remotely to your cluster with <code>ray.init(address=ray://&lt;address&gt;)</code>. Make sure you do not use <code>http://</code> within the address and use the <code>ray://</code> protocol.</li> <li><code>Serve</code>: The HTTP entrypoint for Ray Serve, which is Ray\u2019s model serving layer.</li> </ul>","tags":["ray","distributed ML"]},{"location":"managed/ray/#python-example","title":"Python example","text":"<p>To run Ray in python locally and connect to your cluster, first install a matching version of ray library:</p> <pre><code>pip install ray[default]==2.49.0 # &lt;-- should match the version on your cluster\n</code></pre>","tags":["ray","distributed ML"]},{"location":"managed/ray/#connect-directly-from-python","title":"Connect directly from python","text":"<pre><code># Name: test.py\nimport ray\nray.init(\"&lt;client endpoint&gt;\")\n\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(2)]\nprint(ray.get(futures)) # [0, 1]\n</code></pre> <p>And execute it locally:</p> <pre><code>python test.py\n</code></pre> <p>Note that your local python version must match that of the cluster. If you want to wave this restriction, use the submission route below.</p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#submit-script-to-your-cluster","title":"Submit script to your cluster","text":"<p>We need to create a python script, put it in its own working folder, then submit it to our cluster.</p> <p>Create a <code>raytest.py</code> script and place it under <code>raytest/</code> folder:</p> <pre><code>import ray\n\nray.init()\n\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(2)]\nprint(ray.get(futures)) # [0, 1]\n</code></pre> <p>The folder structure should look as follows:</p> <pre><code>raytest/\n|\n|---raytest.py\n</code></pre> <p>Now submit your job using the dashboard endpoint in your Ray cluster as <code>address</code>.</p> <pre><code>ray job submit --working-dir ./raytest --address &lt;dashboard endpoint&gt; -- python raytest.py\n</code></pre> <p>You should see the output in the console, and can also inspect the job progress by visiting the dashboard endpoint in your browser, under <code>Jobs</code></p> <p></p>","tags":["ray","distributed ML"]},{"location":"managed/ray/#what-next","title":"What next","text":"<p>Ray official documentation and examples.</p>","tags":["ray","distributed ML"]}]}