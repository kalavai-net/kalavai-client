helmDefaults:
  timeout: 1200

repositories:
  {% if deploy_rocm %}
  # amd gpu operator: https://github.com/ROCm/gpu-operator
  - name: rocm
    url: https://rocm.github.io/gpu-operator
  {% endif %}
  - name: kuberay
    url: https://ray-project.github.io/kuberay-helm/
  - name: kalavai
    url: https://kalavai-net.github.io/helm-charts/
  - name: kalavai-templates
    url: https://kalavai-net.github.io/kalavai-templates/
  - name: kalavai-job-operator
    url: https://kalavai-net.github.io/kalavai-job-operator/ 
  {% if deploy_longhorn %}
  - name: longhorn
    url: https://charts.longhorn.io
  {% endif %}
  - name: volcano-sh
    url: https://volcano-sh.github.io/helm-charts
  {% if deploy_monitoring %}
  - name: prometheus-community # prometheus
    url: https://prometheus-community.github.io/helm-charts
  - name: bitnami
    url: https://charts.bitnami.com/bitnami
  {% endif %}
  {% if deploy_opencost %}
  - name: opencost-charts
    url: https://opencost.github.io/opencost-helm-chart
  {% endif %}
  {% if deploy_minio %}
  - name: minio
    url: https://charts.min.io/
  {% endif %}
  - name: hami-charts
    url: https://project-hami.github.io/HAMi

releases:
  - name: job-operator
    namespace: kalavai
    chart: kalavai-job-operator/kalavai-job-operator
    version: 0.0.15
    installed: true
    needs:
    - flux-system/flux
    - volcano-system/volcano-sh
  - name: flux
    namespace: flux-system
    chart: oci://ghcr.io/fluxcd-community/charts/flux2
    version: 2.16.2 # latest version broken (2.17.1), needs --no-hooks
    installed: true
    set:
    - name: helmController.create
      value: true
    - name: server.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: sourceController.create
      value: true
    - name: server.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: imageAutomationController.create
      value: false
    - name: imageReflectionController.create
      value: false
    - name: kustomizeController.create
      value: false
    - name: notificationController.create
      value: false
  # HACK: to avoid racing conditions with traefik not being ready, the CRDs are installed
  # in the base kalavai-runner image, then here we apply artificial wait to give them time
  - name: cert-manager
    namespace: cert-manager
    chart: oci://quay.io/jetstack/charts/cert-manager
    version: "v1.19.1"
    installed: {{deploy_rocm|default("False", true)}}
    set:
      - name: crds.enabled
        value: false  # We'll handle CRDs manually in the kalavai-runner
    hooks:
      # --- Step 1: Wait to ensure CRDs and traefik are ready (presync)
      - events: ["presync"]
        showlogs: true
        command: "bash"
        args:
          - "-c"
          - |
            echo ">>> Waiting for CRDs to establish and traefik to be ready....$(date)"
            sleep 240
            echo ">>> PRESYNC completed: $(date)"
  - name: rocm
    needs: 
    - cert-manager/cert-manager
    namespace: kalavai
    chart: rocm/gpu-operator-charts
    installed: {{deploy_rocm|default("false", true)}}
    version: "1.4.0"
  - name: lago
    namespace: kalavai
    chart: kalavai/lago
    installed: {{deploy_lago|default("false", true)}}
    set:
    - name: external.api.nodePort
      value: 32000
    - name: external.front.nodePort
      value: 30080
    - name: apiUrl
      value: http://{{cluster_ip}}:32000
    - name: frontUrl
      value: http://{{cluster_ip}}:30080
  - name: minio
    needs: 
    - kalavai/longhorn
    namespace: minio
    chart: minio/minio
    installed: {{deploy_minio|default("false", true)}}
    set:
    - name: replicas
      value: {{minio_replicas}}
    - name: resources.requests.memory
      value: "{{minio_resources_memory}}"
    - name: persistence.enabled
      value: true
    - name: persistence.storageClass
      value: {{minio_persistence_storageClass}}
    - name: persistence.size
      value: "{{minio_persistence_size}}"
    - name: persistence.accessMode
      value: "ReadWriteMany"
    - name: rootUser
      value: {{minio_rootUser}}
    - name: rootPassword
      value: {{minio_rootPassword}}
    - name: service.type
      value: "NodePort"
    - name: service.nodePort
      value: {{minio_service_port}}
    - name: consoleService.type
      value: "NodePort"
    - name: consoleService.nodePort
      value: {{minio_console_port}}
    - name: buckets[0].name
      value: "llm-storage"
    - name: buckets[0].policy
      value: "public"
    - name: buckets[0].purge
      value: false
  - name: opencost
    namespace: opencost
    chart: opencost-charts/opencost
    installed: {{deploy_opencost|default("false", true)}}
    set:
    - name: service.type
      value: NodePort
    - name: opencost.nodeSelector.{{kalavai_role_label}}
      value: server
    # point at prometheus instance (theres an opencost.prometheus.external too)
    - name: opencost.prometheus.internal.enabled
      value: true
    - name: opencost.prometheus.internal.serviceName
      value: {{prometheus_service_name}}
    - name: opencost.prometheus.internal.namespaceName
      value: {{prometheus_namespace}}
    - name: opencost.prometheus.internal.port
      value: {{prometheus_port}}
  - name: prometheus
    namespace: {{prometheus_namespace}}
    chart: prometheus-community/kube-prometheus-stack #prometheus/prometheus
    installed: {{deploy_monitoring|default("false", true)}}
    version: "80.5.0"
    set:
    - name: alertmanager.alertmanagerSpec.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: prometheus.prometheusSpec.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: grafana.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: prometheusOperator.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: kube-state-metrics.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: server.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: prometheus-pushgateway.enabled
      value: false
    - name: alertmanager.enabled
      value: false
    - name: server.retention
      value: {{prometheus_server_retention}}
    - name: server.persistentVolume.size
      value: {{prometheus_disk_size}}
    - name: prometheus-node-exporter.hostRootFsMount.enabled
      value: false
    # thanos sidecar
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#thanosspec
    - name: prometheus.prometheusSpec.thanos.image
      value: quay.io/thanos/thanos:v0.41.0
    - name: prometheus.prometheusSpec.thanos.objectStorageConfig.secret.type
      value: s3
    - name: prometheus.prometheusSpec.thanos.objectStorageConfig.secret.config.bucket
      value: {{thanos_s3_bucket}}
    - name: prometheus.prometheusSpec.thanos.objectStorageConfig.secret.config.endpoint
      value: {{thanos_s3_endpoint}}
    - name: prometheus.prometheusSpec.thanos.objectStorageConfig.secret.config.region
      value: {{thanos_s3_region}}
    - name: prometheus.prometheusSpec.thanos.objectStorageConfig.secret.config.access_key
      value: {{thanos_s3_access_key}}
    - name: prometheus.prometheusSpec.thanos.objectStorageConfig.secret.config.secret_key
      value: {{thanos_s3_secret_key}}
    - name: prometheus.prometheusSpec.thanosService.enabled
      value: true
    - name: prometheus.prometheusSpec.externalLabels.cluster
      value: "{{prometheus_externalLabel_cluster}}"
    - name: prometheus.prometheusSpec.disableCompaction
      value: true
  # thanos (compactor, query, bucket, store gateway) components with custom image
  - name: thanos
    needs:
    - monitoring/prometheus
    namespace: monitoring
    chart: bitnami/thanos
    version: "17.3.1"
    installed: {{deploy_monitoring|default("false", true)}}
    set:
    # Custom Docker images
    - name: image.registry
      value: quay.io
    - name: image.repository
      value: thanos/thanos
    - name: image.tag
      value: v0.41.0
    - name: global.security.allowInsecureImages
      value: true
    # Enable all Thanos components
    - name: query.nodeSelector.{{kalavai_role_label}}
      value: "server"
    - name: queryFrontend.nodeSelector.{{kalavai_role_label}}
      value: "server"
    - name: bucketweb.nodeSelector.{{kalavai_role_label}}
      value: "server"
    - name: compactor.nodeSelector.{{kalavai_role_label}}
      value: "server"
    - name: storegateway.nodeSelector.{{kalavai_role_label}}
      value: "server"
    - name: query.enabled
      value: true
    - name: query.replicaCount
      value: {{thanos_query_replicas|default("1")}} # more concurrent users and complex queries
    - name: query.resources.requests.memory
      value: "1Gi"
    - name: query.resources.requests.cpu
      value: "1000m"
    - name: storegateway.resources.requests.memory  
      value: "2Gi"
    - name: queryFrontend.enabled
      value: true
    - name: queryFrontend.service.type
      value: NodePort
    - name: queryFrontend.replicaCount  
      value: {{thanos_query_frontend_replicas|default("1")}} # better query caching
    - name: compactor.enabled
      value: true
    - name: compactor.retentionResolutionRaw
      value: {{thanos_retention_raw|default("30d")}}
    - name: compactor.retentionResolution5m
      value: {{thanos_retention_5m|default("60d")}}
    - name: compactor.retentionResolution1h
      value: {{thanos_retention_1h|default("90d")}}
    - name: storegateway.enabled
      value: true
    - name: storegateway.replicaCount
      value: {{thanos_store_replicas|default("1")}} # better s3 data serving
    - name: bucketweb.enabled
      value: true
    - name: bucketweb.replicaCount
      value: {{thanos_bucket_replicas|default("1")}}
    # - name: ruler.enabled
    #   value: true
    # - name: ruler.replicaCount
    #   value: {{thanos_ruler_replicas|default("1")}}
    # S3 object storage configuration
    - name: objstoreConfig
      value: |
        type: s3
        config:
          bucket: {{thanos_s3_bucket}}
          endpoint: {{thanos_s3_endpoint}}
          region: {{thanos_s3_region}}
          access_key: {{thanos_s3_access_key}}
          secret_key: {{thanos_s3_secret_key}}
          insecure: true
  - name: volcano-sh
    namespace: volcano-system
    chart: volcano-sh/volcano
    installed: true
    version: "1.13.0"
  - name: kuberay
    namespace: kuberay
    chart: kuberay/kuberay-operator
    installed: true
    version: "1.5.0"
  - name: kuberay-apiserver
    namespace: kuberay
    chart: kuberay/kuberay-apiserver
    installed: true
  - name: longhorn
    namespace: kalavai
    chart: longhorn/longhorn
    installed: {{deploy_longhorn|default("false", true)}}
    set:
    # security issue! enable for testing only
    - name: service.ui.type
      value: NodePort
    - name: service.ui.nodePort
      value: {{longhorn_ui_port}}
    - name: service.manager.type
      value: NodePort
    - name: service.manager.nodePort
      value: {{longhorn_manager_port}}
    - name: persistence.defaultClassReplicaCount
      value: {{longhorn_replicas}}
    - name: global.nodeSelector.{{longhorn_label_selector_key}}
      value: "{{longhorn_label_selector_value}}"
    - name: defaultSettings.storageMinimalAvailablePercentage
      value: {{longhorn_minimal_available_percentage}}
  - name: kalavai-watcher
    namespace: kalavai
    chart: kalavai/kalavai-watcher
    version: "0.4.22"
    installed: true
    set:
    - name: namespace
      value: kalavai
    - name: deployment.replicas
      value: {{watcher_replicas}}
    - name: image_tag
      value: {{watcher_image_tag}}
    - name: deployment.in_cluster
      value: "True"
    - name: deployment.kalavai_username_key
      value: "{{kalavai_username_key}}"
    - name: deployment.use_auth_key
      value: "True"
    - name: deployment.admin_key
      value: "{{watcher_admin_key}}"
    - name: deployment.write_key
      value: "{{watcher_write_key}}"
    - name: deployment.readonly_key
      value: "{{watcher_readonly_key}}"
    - name: deployment.is_shared_pool
      value: {{watcher_is_shared_pool}}
    - name: deployment.prometheus_endpoint
      value: "http://thanos-query-frontend.{{prometheus_namespace}}:{{prometheus_port}}"
    - name: deployment.opencost_endpoint
      value: {{opencost_endpoint}}
    - name: deployment.longhorn_manager_endpoint
      value: {{longhorn_manager_endpoint}}
    - name: service.nodePort
      value: {{watcher_port}}
    - name: resources.limits.memory
      value: {{watcher_resources_memory}}
    - name: resources.limits.cpu
      value: {{watcher_resources_cpu}}
    - name: deployment.nodeSelector.{{kalavai_role_label}}
      value: "server"
  - name: hami-vgpu
    namespace: kalavai
    chart: hami-charts/hami
    installed: true
    version: "2.6.1"
    set:
    - name: resourceCores
      value: "nvidia.com/gpucores"
    - name: resourceMem
      value: "nvidia.com/gpumem"
    - name: resourceMemPercentage
      value: "nvidia.com/gpumem-percentage"
    - name: devicePlugin.runtimeClassName
      value: "nvidia"
    - name: scheduler.defaultSchedulerPolicy.nodeSchedulerPolicy
      value: "{{gpu_nodeSchedulerPolicy}}"
    - name: scheduler.defaultSchedulerPolicy.gpuSchedulerPolicy
      value: "{{gpu_gpuSchedulerPolicy}}"
    - name: scheduler.defaultCores
      value: "100"
    - name: scheduler.kubeScheduler.imageTag
      value: v1.31.1
    - name: devicePlugin.deviceMemoryScaling
      value: "1"
    - name: devicePlugin.deviceSplitCount
      value: "{{gpu_deviceSplitCount}}"
    

