helmDefaults:
  timeout: 1200

repositories:
  {% if deploy_rocm %}
  # amd gpu operator: https://github.com/ROCm/gpu-operator
  - name: rocm
    url: https://rocm.github.io/gpu-operator
  {% endif %}
  - name: kuberay
    url: https://ray-project.github.io/kuberay-helm/
  - name: kalavai
    url: https://kalavai-net.github.io/helm-charts/
  - name: kalavai-templates
    url: https://kalavai-net.github.io/kalavai-templates/
  - name: kalavai-job-operator
    url: https://kalavai-net.github.io/kalavai-job-operator/ 
  {% if deploy_longhorn %}
  - name: longhorn
    url: https://charts.longhorn.io
  {% endif %}
  - name: volcano-sh
    url: https://volcano-sh.github.io/helm-charts
  {% if deploy_prometheus %}
  - name: prometheus-community # prometheus
    url: https://prometheus-community.github.io/helm-charts
  {% endif %}
  {% if deploy_opencost %}
  - name: opencost-charts
    url: https://opencost.github.io/opencost-helm-chart
  {% endif %}
  {% if deploy_minio %}
  - name: minio
    url: https://charts.min.io/
  {% endif %}
  - name: hami-charts
    url: https://project-hami.github.io/HAMi

releases:
  - name: job-operator
    namespace: kalavai
    chart: kalavai-job-operator/kalavai-job-operator
    version: 0.0.12
    installed: true
    needs:
    - flux-system/flux
    - volcano-system/volcano-sh
  - name: flux
    namespace: flux-system
    chart: oci://ghcr.io/fluxcd-community/charts/flux2
    version: 2.16.2 # latest version broken (2.17.1), needs --no-hooks
    installed: true
    set:
    - name: helmController.create
      value: true
    - name: helmController.nodeSelector.kalavai/role
      value: server
    - name: sourceController.create
      value: true
    - name: sourceController.nodeSelector.kalavai/role
      value: server
    - name: imageAutomationController.create
      value: false
    - name: imageReflectionController.create
      value: false
    - name: kustomizeController.create
      value: false
    - name: notificationController.create
      value: false
  # HACK: to avoid racing conditions with traefik not being ready, the CRDs are installed
  # in the base kalavai-runner image, then here we apply artificial wait to give them time
  - name: cert-manager
    namespace: cert-manager
    chart: oci://quay.io/jetstack/charts/cert-manager
    version: "v1.19.1"
    installed: {{deploy_rocm|default("False", true)}}
    set:
      - name: crds.enabled
        value: false  # We'll handle CRDs manually in the kalavai-runner
    hooks:
      # --- Step 1: Wait to ensure CRDs and traefik are ready (presync)
      - events: ["presync"]
        showlogs: true
        command: "bash"
        args:
          - "-c"
          - |
            echo ">>> Waiting for CRDs to establish and traefik to be ready....$(date)"
            sleep 240
            echo ">>> PRESYNC completed: $(date)"
  - name: rocm
    needs: 
    - cert-manager/cert-manager
    namespace: kalavai
    chart: rocm/gpu-operator-charts
    installed: {{deploy_rocm|default("false", true)}}
    version: "1.4.0"
  - name: lago
    namespace: kalavai
    chart: kalavai/lago
    installed: {{deploy_lago|default("false", true)}}
    set:
    - name: external.api.nodePort
      value: 32000
    - name: external.front.nodePort
      value: 30080
    - name: apiUrl
      value: http://{{cluster_ip}}:32000
    - name: frontUrl
      value: http://{{cluster_ip}}:30080
  - name: minio
    needs: 
    - kalavai/longhorn
    namespace: minio
    chart: minio/minio
    installed: {{deploy_minio|default("false", true)}}
    set:
    - name: replicas
      value: {{minio_replicas}}
    - name: resources.requests.memory
      value: "{{minio_resources_memory}}"
    - name: persistence.enabled
      value: true
    - name: persistence.storageClass
      value: {{minio_persistence_storageClass}}
    - name: persistence.size
      value: "{{minio_persistence_size}}"
    - name: persistence.accessMode
      value: "ReadWriteMany"
    - name: rootUser
      value: {{minio_rootUser}}
    - name: rootPassword
      value: {{minio_rootPassword}}
    - name: service.type
      value: "NodePort"
    - name: service.nodePort
      value: {{minio_service_port}}
    - name: consoleService.type
      value: "NodePort"
    - name: consoleService.nodePort
      value: {{minio_console_port}}
    - name: buckets[0].name
      value: "llm-storage"
    - name: buckets[0].policy
      value: "public"
    - name: buckets[0].purge
      value: false
  - name: opencost
    namespace: opencost
    chart: opencost-charts/opencost
    installed: {{deploy_opencost|default("false", true)}}
    set:
    - name: service.type
      value: NodePort
    - name: opencost.nodeSelector.{{kalavai_role_label}}
      value: server
    # point at prometheus instance (theres an opencost.prometheus.external too)
    - name: opencost.prometheus.internal.enabled
      value: true
    - name: opencost.prometheus.internal.serviceName
      value: {{prometheus_service_name}}
    - name: opencost.prometheus.internal.namespaceName
      value: {{prometheus_namespace}}
    - name: opencost.prometheus.internal.port
      value: {{prometheus_port}}
  - name: prometheus
    namespace: {{prometheus_namespace}}
    chart: prometheus-community/kube-prometheus-stack #prometheus/prometheus
    installed: {{deploy_prometheus|default("false", true)}}
    version: "80.5.0"
    set:
    - name: server.nodeSelector.{{kalavai_role_label}}
      value: server
    - name: prometheus-pushgateway.enabled
      value: false
    - name: alertmanager.enabled
      value: false
    - name: server.retention
      value: {{prometheus_server_retention}}
    - name: server.persistentVolume.size
      value: {{prometheus_disk_size}}
    - name: prometheus-node-exporter.hostRootFsMount.enabled
      value: false
  - name: volcano-sh
    namespace: volcano-system
    chart: volcano-sh/volcano
    installed: true
    version: "1.13.0"
  - name: kuberay
    namespace: kuberay
    chart: kuberay/kuberay-operator
    installed: true
    version: "1.5.0"
  - name: kuberay-apiserver
    namespace: kuberay
    chart: kuberay/kuberay-apiserver
    installed: true
  - name: longhorn
    namespace: kalavai
    chart: longhorn/longhorn
    installed: {{deploy_longhorn|default("false", true)}}
    set:
    # security issue! enable for testing only
    - name: service.ui.type
      value: NodePort
    - name: service.ui.nodePort
      value: {{longhorn_ui_port}}
    - name: service.manager.type
      value: NodePort
    - name: service.manager.nodePort
      value: {{longhorn_manager_port}}
    - name: persistence.defaultClassReplicaCount
      value: {{longhorn_replicas}}
    - name: global.nodeSelector.{{longhorn_label_selector_key}}
      value: "{{longhorn_label_selector_value}}"
    - name: defaultSettings.storageMinimalAvailablePercentage
      value: {{longhorn_minimal_available_percentage}}
  - name: kalavai-watcher
    namespace: kalavai
    chart: kalavai/kalavai-watcher
    version: "0.4.22"
    installed: true
    set:
    - name: namespace
      value: kalavai
    - name: deployment.replicas
      value: {{watcher_replicas}}
    - name: image_tag
      value: "{{watcher_image_tag}}" #"v2025.07.34"
    - name: deployment.in_cluster
      value: "True"
    - name: deployment.kalavai_username_key
      value: "{{kalavai_username_key}}"
    - name: deployment.use_auth_key
      value: "True"
    - name: deployment.admin_key
      value: "{{watcher_admin_key}}"
    - name: deployment.write_key
      value: "{{watcher_write_key}}"
    - name: deployment.readonly_key
      value: "{{watcher_readonly_key}}"
    - name: deployment.is_shared_pool
      value: {{watcher_is_shared_pool}}
    - name: deployment.prometheus_endpoint
      value: "http://{{prometheus_service_name}}.{{prometheus_namespace}}.svc.cluster.local:{{prometheus_port}}"
    - name: deployment.opencost_endpoint
      value: {{opencost_endpoint}}
    - name: deployment.longhorn_manager_endpoint
      value: {{longhorn_manager_endpoint}}
    - name: service.nodePort
      value: {{watcher_port}}
    - name: resources.limits.memory
      value: {{watcher_resources_memory}}
    - name: resources.limits.cpu
      value: {{watcher_resources_cpu}}
    - name: deployment.nodeSelector.{{kalavai_role_label}}
      value: "server"
  - name: hami-vgpu
    namespace: kalavai
    chart: hami-charts/hami
    installed: true
    version: "2.6.1"
    set:
    - name: resourceCores
      value: "nvidia.com/gpucores"
    - name: resourceMem
      value: "nvidia.com/gpumem"
    - name: resourceMemPercentage
      value: "nvidia.com/gpumem-percentage"
    - name: devicePlugin.runtimeClassName
      value: "nvidia"
    - name: scheduler.defaultSchedulerPolicy.nodeSchedulerPolicy
      value: "{{gpu_nodeSchedulerPolicy}}"
    - name: scheduler.defaultSchedulerPolicy.gpuSchedulerPolicy
      value: "{{gpu_gpuSchedulerPolicy}}"
    - name: scheduler.defaultCores
      value: "100"
    - name: scheduler.kubeScheduler.imageTag
      value: v1.31.1
    - name: devicePlugin.deviceMemoryScaling
      value: "1"
    - name: devicePlugin.deviceSplitCount
      value: "{{gpu_deviceSplitCount}}"
    

