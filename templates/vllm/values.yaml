############################
### MUST HAVE THESE FIELDS ##
- name: id_field
  value: model_id
  default: model_id
  description: "Field that contains the ID of the job"

# must contain the list of ports to enable in the leader node
# - name: endpoint_ports
#   value: "8080"
#   default: "8080"
#   description: "[DO NOT CHANGE] Ports opened on master node (will be shown as endpoints for the job)"
############################

- name: litellm_base_url
  value: "http://litellm-1.default.svc.cluster.local:4000" #"http://192.168.68.67:30219"
  default: "http://litellm-1.default.svc.cluster.local:4000"
  description: "Base URL of the LiteLLM service (central registry)"

- name: litellm_key
  value: ""
  default: "sk-1234"
  description: "Master key of the LiteLLM service (central registry)"

- name: storage
  value: "models-hub"
  default: "models-hub"
  description: "Pool storage to use to cache model weights"

- name: workers
  value: "1"
  default: "1"
  description: "Number of workers (for tensor and pipeline parallelism)"

- name: model_id
  value: null
  default: null
  description: "Huggingface model id to load"

- name: lora_modules
  value: ""
  default: ""
  description: "LoRa adaptor(s) to load; semi-colon separated list of huggingface repos of the lora adaptors to load"

- name: hf_token
  value: <yout token>
  default: null
  description: "Huggingface token, required to load model weights"

- name: cpus
  value: "2"
  default: "2"
  description: "CPUs per single worker (final one = cpus * num_workers)"

- name: gpus
  value: "1"
  default: "1"
  description: "GPUs per single worker (final one = gpus * num_workers)"

- name: memory
  value: "4"
  default: "4"
  description: "RAM memory per single worker (final one = memory * num_workers)"

- name: tensor_parallel_size
  value: "1"
  default: "1"
  description: "Tensor parallelism (use the number of GPUs per node)"

- name: pipeline_parallel_size
  value: "1"
  default: "1"
  description: "Pipeline parallelism (use the number of nodes)"

- name: shmem_size
  value: "4"
  default: "4"
  description: "Size of the shared memory volume (in GBs)"

- name: extra
  value: "--dtype float16 --enforce-eager"
  default: --dtype float16 --enforce-eager
  description: "Extra parameters to pass to the vLLM server. See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server"
